{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of the spark_sklearn fit for the NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Collecting spark_sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/3f/34b8dec7d2cfcfe0ba99d637b4f2d306c1ca0b404107c07c829e085f6b38/spark-sklearn-0.3.0.tar.gz\n",
      "Collecting scikit-learn<0.20,>=0.18.1 (from spark_sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/67/370aa248f54769a56216707ad7b9af19745e85a603fafa47bde353f327fb/scikit_learn-0.19.2-cp27-cp27mu-manylinux1_x86_64.whl (5.0MB)\n",
      "\u001b[K     |████████████████████████████████| 5.0MB 10.8MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: spark-sklearn\n",
      "  Building wheel for spark-sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/llayer/.cache/pip/wheels/64/28/e8/cb0250888675c630786f932dcc63ed96ac1aca299bcfb7235f\n",
      "Successfully built spark-sklearn\n",
      "Installing collected packages: scikit-learn, spark-sklearn\n",
      "Successfully installed scikit-learn-0.19.2 spark-sklearn-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spark_sklearn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(sites, codes):\n",
    "\n",
    "    sites_index = {k: v for v, k in enumerate(sites)}\n",
    "    codes_index = {k: v for v, k in enumerate(codes)}\n",
    "    return sites_index, codes_index\n",
    "\n",
    "\n",
    "def prune_to_index(codes, sites, only_unknown = False, counts = False, error_threshold = 0, site_threshold = 0):\n",
    "    \n",
    "    all_sites = list(sites['site'])\n",
    "    all_codes = list(codes['error'])\n",
    "    good_sites = list(sites['site'])\n",
    "    good_codes = list(codes['error'])\n",
    "    \n",
    "    if only_unknown == True:\n",
    "        informative_sites = list(sites[sites['only_unknown'] == False]['site'])\n",
    "        good_sites = list(set(informative_sites) & set(good_sites))  \n",
    "\n",
    "    if site_threshold > 0:\n",
    "        if counts == False:\n",
    "            frequent_sites = list(sites[sites['frequency'] > site_threshold]['site'])\n",
    "        else:\n",
    "            frequent_sites = list(sites[sites['counts'] > site_threshold]['site'])\n",
    "        good_sites = list(set(frequent_sites) & set(good_sites))  \n",
    "            \n",
    "    if error_threshold > 0:\n",
    "        if counts == False:\n",
    "            frequent_errors = list(codes[codes['frequency'] > error_threshold]['error'])\n",
    "        else:\n",
    "            frequent_errors = list(codes[codes['counts'] > error_threshold]['error'])    \n",
    "        good_codes = list(set(frequent_errors) & set(good_codes)) \n",
    "        \n",
    "    # Get the pruned sites and codes\n",
    "    pruned_sites = list(set(all_sites) - set(good_sites))\n",
    "    pruned_codes = list(set(all_codes) - set(good_codes))\n",
    "    \n",
    "    # Index the results\n",
    "    good_sites_index = {k: v for v, k in enumerate(good_sites)}\n",
    "    pruned_sites_index = {k: len(good_sites) for k in pruned_sites}\n",
    "    good_codes_index = {k: v for v, k in enumerate(good_codes)}\n",
    "    pruned_codes_index = {k: len(good_codes) for k in pruned_codes}    \n",
    "        \n",
    "    def merge_dicts(x, y):\n",
    "        z = x.copy()   \n",
    "        z.update(y) \n",
    "        return z\n",
    "    \n",
    "    codes_index = merge_dicts(good_codes_index, pruned_codes_index)\n",
    "    sites_index = merge_dicts(good_sites_index, pruned_sites_index)\n",
    "    \n",
    "    return sites_index, codes_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, msg_only = False, sample = False, sample_fact = 3):\n",
    "\n",
    "    actionshist = pd.read_hdf(path, 'frame')\n",
    "    \n",
    "    print( actionshist['label'].value_counts() )\n",
    "    \n",
    "    if sample == True:\n",
    "        minority_class = actionshist[actionshist['label'] == 1]\n",
    "        n_samples = int(sample_fact*len(minority_class))\n",
    "        majority_class_sampled = actionshist[actionshist['label'] == 0].sample(n_samples , random_state=42)\n",
    "        print('After sampling:', 'Minority class', len(minority_class), 'Majority class', len(majority_class_sampled) )\n",
    "        actionshist = pd.concat([minority_class, majority_class_sampled])\n",
    "    \n",
    "    if msg_only == False:\n",
    "        sites = pd.read_hdf(path, 'frame2')\n",
    "        codes = pd.read_hdf(path, 'frame3')\n",
    "    else:\n",
    "        codes = pd.read_hdf(path, 'frame4')\n",
    "        sites = pd.read_hdf(path, 'frame5')\n",
    "        codes.rename({'errors_msg': 'error'}, axis=1, inplace=True)\n",
    "        sites.rename({'sites_msg': 'site'}, axis=1, inplace=True)\n",
    "        \n",
    "    return actionshist, codes, sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    31839\n",
      "1     1747\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "\n",
    "# Include counts\n",
    "MSG_ONLY = False\n",
    "PRUNING = 'Neg'\n",
    "\n",
    "# sample\n",
    "SAMPLE = False\n",
    "SAMPLE_FACT = 5\n",
    "\n",
    "# batch generator param\n",
    "AVG_W2V = False\n",
    "MAX_WORDS = 400\n",
    "GEN_PARAM = {}\n",
    "GEN_PARAM['averaged'] = AVG_W2V\n",
    "GEN_PARAM['only_msg'] = MSG_ONLY \n",
    "GEN_PARAM['sequence'] = False\n",
    "GEN_PARAM['max_msg'] = 1\n",
    "GEN_PARAM['cut_front'] = True\n",
    "TRAIN_ON_BATCH = True\n",
    "\n",
    "# Model\n",
    "MODEL = 'nlp_msg'\n",
    "\n",
    "# Defines the input experiments for the machine learning\n",
    "EXPERIMENTS = [\n",
    "    \n",
    "    # 1st experiment initial parameter\n",
    "    {'NAME': 'NOMINAL_t', 'DIM':50, 'VOCAB': -1, 'ALGO': 'sg',\n",
    "     'NLP_PARAM': {'cudnn': False, 'batch_norm': False, 'word_encoder': 'LSTM', \n",
    "                   'attention': False, 'include_counts': True, 'avg_w2v': False},\n",
    "     'CALLBACK': { 'es': True, 'patience': 3, 'kill_slowstarts': True, 'kill_threshold': 0.51 }\n",
    "    }\n",
    "]\n",
    "    \n",
    "e = EXPERIMENTS[ 0 ]\n",
    "\n",
    "# Load the data\n",
    "path = '/eos/user/l/llayer/AIErrorLogAnalysis/data/input/' + 'input_' + 'NOMINAL' + '.h5'\n",
    "actionshist, codes, sites = load_data(path, msg_only=MSG_ONLY,\n",
    "                                                  sample=SAMPLE, sample_fact = SAMPLE_FACT)\n",
    "e['NLP_PARAM']['embedding_matrix_path'] = '/eos/user/l/llayer/AIErrorLogAnalysis/data/word2vec/' + 'embedding_matrix_' + 'NOMINAL' + '.npy'\n",
    "\n",
    "sites_index, codes_index = sites_index, codes_index = prune_to_index(codes, sites, only_unknown = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>label</th>\n",
       "      <th>error</th>\n",
       "      <th>site</th>\n",
       "      <th>site_state</th>\n",
       "      <th>count</th>\n",
       "      <th>msg_encoded</th>\n",
       "      <th>exit_code</th>\n",
       "      <th>error_type</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T1_US_FNAL_Disk, T3_US_FNALLPC]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, 85]</td>\n",
       "      <td>[T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...</td>\n",
       "      <td>[bad, bad, bad, bad, good]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...</td>\n",
       "      <td>[nan, nan, nan, nan, 8021.0]</td>\n",
       "      <td>[nan, nan, nan, nan, Fatal Exception]</td>\n",
       "      <td>[nan, nan, nan, nan, [-1.26796770096, 1.129392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...</td>\n",
       "      <td>0</td>\n",
       "      <td>[50664, -1, -1]</td>\n",
       "      <td>[T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]</td>\n",
       "      <td>[good, good, good]</td>\n",
       "      <td>[2, 1, 1]</td>\n",
       "      <td>[[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...</td>\n",
       "      <td>[50664.0, nan, nan]</td>\n",
       "      <td>[PerformanceKill, nan, nan]</td>\n",
       "      <td>[[-0.292789727449, 1.32788562775, 0.7245721220...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           task_name  label  \\\n",
       "0  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...      0   \n",
       "1  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...      0   \n",
       "2  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...      0   \n",
       "3  /amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...      0   \n",
       "4  /amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...      0   \n",
       "\n",
       "                  error                                               site  \\\n",
       "0              [-1, -1]                   [T1_US_FNAL_Disk, T3_US_FNALLPC]   \n",
       "1              [-1, -1]                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "2              [-1, -1]                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "3  [-1, -1, -1, -1, 85]  [T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...   \n",
       "4       [50664, -1, -1]               [T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]   \n",
       "\n",
       "                   site_state            count  \\\n",
       "0                  [bad, bad]           [1, 1]   \n",
       "1                  [bad, bad]           [1, 1]   \n",
       "2                  [bad, bad]           [1, 1]   \n",
       "3  [bad, bad, bad, bad, good]  [1, 1, 1, 1, 1]   \n",
       "4          [good, good, good]        [2, 1, 1]   \n",
       "\n",
       "                                         msg_encoded  \\\n",
       "0                                         [nan, nan]   \n",
       "1                                         [nan, nan]   \n",
       "2                                         [nan, nan]   \n",
       "3  [nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...   \n",
       "4  [[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...   \n",
       "\n",
       "                      exit_code                             error_type  \\\n",
       "0                    [nan, nan]                             [nan, nan]   \n",
       "1                    [nan, nan]                             [nan, nan]   \n",
       "2                    [nan, nan]                             [nan, nan]   \n",
       "3  [nan, nan, nan, nan, 8021.0]  [nan, nan, nan, nan, Fatal Exception]   \n",
       "4           [50664.0, nan, nan]            [PerformanceKill, nan, nan]   \n",
       "\n",
       "                                                 avg  \n",
       "0                                         [nan, nan]  \n",
       "1                                         [nan, nan]  \n",
       "2                                         [nan, nan]  \n",
       "3  [nan, nan, nan, nan, [-1.26796770096, 1.129392...  \n",
       "4  [[-0.292789727449, 1.32788562775, 0.7245721220...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionshist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_list(msgs):\n",
    "    new_array = []\n",
    "    for msg in msgs:\n",
    "        if not isinstance(msg, list):\n",
    "            new_array.append([])\n",
    "        else:\n",
    "            new_array.append(msg)\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionshist['msg'] = actionshist['msg_encoded'].apply(insert_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>label</th>\n",
       "      <th>error</th>\n",
       "      <th>site</th>\n",
       "      <th>site_state</th>\n",
       "      <th>count</th>\n",
       "      <th>msg_encoded</th>\n",
       "      <th>exit_code</th>\n",
       "      <th>error_type</th>\n",
       "      <th>avg</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T1_US_FNAL_Disk, T3_US_FNALLPC]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, 85]</td>\n",
       "      <td>[T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...</td>\n",
       "      <td>[bad, bad, bad, bad, good]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...</td>\n",
       "      <td>[nan, nan, nan, nan, 8021.0]</td>\n",
       "      <td>[nan, nan, nan, nan, Fatal Exception]</td>\n",
       "      <td>[nan, nan, nan, nan, [-1.26796770096, 1.129392...</td>\n",
       "      <td>[[], [], [], [], [35, 12, 10, 37, 186, 34, 25,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...</td>\n",
       "      <td>0</td>\n",
       "      <td>[50664, -1, -1]</td>\n",
       "      <td>[T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]</td>\n",
       "      <td>[good, good, good]</td>\n",
       "      <td>[2, 1, 1]</td>\n",
       "      <td>[[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...</td>\n",
       "      <td>[50664.0, nan, nan]</td>\n",
       "      <td>[PerformanceKill, nan, nan]</td>\n",
       "      <td>[[-0.292789727449, 1.32788562775, 0.7245721220...</td>\n",
       "      <td>[[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           task_name  label  \\\n",
       "0  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...      0   \n",
       "1  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...      0   \n",
       "2  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...      0   \n",
       "3  /amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...      0   \n",
       "4  /amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...      0   \n",
       "\n",
       "                  error                                               site  \\\n",
       "0              [-1, -1]                   [T1_US_FNAL_Disk, T3_US_FNALLPC]   \n",
       "1              [-1, -1]                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "2              [-1, -1]                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "3  [-1, -1, -1, -1, 85]  [T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...   \n",
       "4       [50664, -1, -1]               [T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]   \n",
       "\n",
       "                   site_state            count  \\\n",
       "0                  [bad, bad]           [1, 1]   \n",
       "1                  [bad, bad]           [1, 1]   \n",
       "2                  [bad, bad]           [1, 1]   \n",
       "3  [bad, bad, bad, bad, good]  [1, 1, 1, 1, 1]   \n",
       "4          [good, good, good]        [2, 1, 1]   \n",
       "\n",
       "                                         msg_encoded  \\\n",
       "0                                         [nan, nan]   \n",
       "1                                         [nan, nan]   \n",
       "2                                         [nan, nan]   \n",
       "3  [nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...   \n",
       "4  [[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...   \n",
       "\n",
       "                      exit_code                             error_type  \\\n",
       "0                    [nan, nan]                             [nan, nan]   \n",
       "1                    [nan, nan]                             [nan, nan]   \n",
       "2                    [nan, nan]                             [nan, nan]   \n",
       "3  [nan, nan, nan, nan, 8021.0]  [nan, nan, nan, nan, Fatal Exception]   \n",
       "4           [50664.0, nan, nan]            [PerformanceKill, nan, nan]   \n",
       "\n",
       "                                                 avg  \\\n",
       "0                                         [nan, nan]   \n",
       "1                                         [nan, nan]   \n",
       "2                                         [nan, nan]   \n",
       "3  [nan, nan, nan, nan, [-1.26796770096, 1.129392...   \n",
       "4  [[-0.292789727449, 1.32788562775, 0.7245721220...   \n",
       "\n",
       "                                                 msg  \n",
       "0                                           [[], []]  \n",
       "1                                           [[], []]  \n",
       "2                                           [[], []]  \n",
       "3  [[], [], [], [], [35, 12, 10, 37, 186, 34, 25,...  \n",
       "4  [[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionshist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = actionshist[['label', 'error', 'site', 'msg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = [[[[2,1], []], [1,0]]]\n",
    "tf = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[2, 1], []]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0       1\n",
       "0  [[2, 1], []]  [1, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, error=[u'-1', u'-1'], site=[u'T1_US_FNAL_Disk', u'T3_US_FNALLPC'], msg=[[], []])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed, Dropout, Flatten, Reshape\n",
    "from keras.layers import average, Concatenate, Lambda, CuDNNLSTM, CuDNNGRU, Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import BatchNormalization\n",
    "K.set_floatx('float32')\n",
    "\n",
    "\n",
    "class NLP():\n",
    "    \n",
    "    \n",
    "    def __init__(self, num_classes, num_error, num_sites, max_sequence_length, embedding_matrix,\n",
    "                 cudnn = False, batch_norm = False, word_encoder = 'LSTM', encode_sites = True, attention = False,\n",
    "                 include_counts = False, avg_w2v = False, verbose = 1):\n",
    "    \n",
    "        self.embedding_matrix = embedding_matrix.astype('float32')\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.num_error = num_error\n",
    "        self.num_sites = num_sites\n",
    "        self.num_classes = num_classes\n",
    "        self.max_senten_num = num_error * num_sites\n",
    "        self.cudnn = cudnn\n",
    "        self.attention = attention\n",
    "        self.word_encoder = word_encoder\n",
    "        self.encode_sites = encode_sites\n",
    "        self.batch_norm = batch_norm\n",
    "        self.include_counts = include_counts\n",
    "        self.avg_w2v = avg_w2v\n",
    "        self.verbose = verbose\n",
    "        # Hyperparameters\n",
    "        self.hp = {\n",
    "            # Regularization\n",
    "            'l2_regulizer': 0.0001,\n",
    "            'dropout':0.2,\n",
    "            # Conv1D\n",
    "            'filters':256,\n",
    "            'kernel_size':3,\n",
    "            'conv_layers':3,\n",
    "            'max_pooling':3,\n",
    "            'units_conv':10,\n",
    "            # RNN with optional attention\n",
    "            'train_embedding': False,\n",
    "            'att_units':10,\n",
    "            'rec_dropout':0.0,\n",
    "            'rnn': LSTM, #TRY GRU\n",
    "            'rnncud': CuDNNLSTM, # TRY CuDNNGRU\n",
    "            'rnn_units' : 10,\n",
    "            # Site encoding\n",
    "            'encode_sites': False,\n",
    "            'activation_site': 'relu', #TRY linear\n",
    "            'units_site': 10,\n",
    "            # Final layers\n",
    "            'dense_layers': 3,\n",
    "            'dense_units': 20,\n",
    "            'learning_rate':0.0001,\n",
    "            'decay':0.0\n",
    "                    }\n",
    "\n",
    "        \n",
    "        \n",
    "    def set_hyperparameters(self, tweaked_instances):\n",
    "\n",
    "        for  key, value in tweaked_instances.items():\n",
    "            if key in self.hp:\n",
    "                self.hp[key] = value\n",
    "            else:\n",
    "                raise KeyError(key + ' does not exist in hyperparameters')\n",
    "\n",
    "            \n",
    "    def print_hyperparameters(self):\n",
    "\n",
    "        print('Hyperparameter\\tCorresponding Value')\n",
    "        for key, value in self.hp.items():\n",
    "            print(key, '\\t\\t', value)\n",
    "        \n",
    "        \n",
    "    def get_embedding_layer( self ):\n",
    "        \n",
    "        dims_embed = self.embedding_matrix.shape\n",
    "        \"\"\"\n",
    "        if self.cudnn == True or self.word_encoder == 'Conv1D':\n",
    "            embedding = Embedding(dims_embed[0], dims_embed[1], weights=[self.embedding_matrix], \\\n",
    "                                  input_length = self.max_sequence_length, trainable = self.train_embedding)\n",
    "        else:\n",
    "        \"\"\"\n",
    "        \n",
    "        embedding = Embedding(dims_embed[0], dims_embed[1], weights=[self.embedding_matrix], \\\n",
    "                  input_length = self.max_sequence_length, mask_zero = True, trainable = int(self.hp['train_embedding']))\n",
    "    \n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def word_encoder_lstm( self ):\n",
    "        \n",
    "        #TODO add recurrent_dropout\n",
    "        \n",
    "        word_input = Input(shape = ( None, ), dtype='int32')\n",
    "        word_sequences = self.get_embedding_layer()(word_input)\n",
    "                \n",
    "        if self.attention == False:\n",
    "            if self.cudnn == True:\n",
    "                word_lstm = self.hp['rnncud'](int(self.hp['rnn_units']), \n",
    "                                              kernel_regularizer=l2(self.hp['l2_regulizer']))(word_sequences)\n",
    "            else:\n",
    "                word_lstm = self.hp['rnn'](int(self.hp['rnn_units']), kernel_regularizer=l2(self.hp['l2_regulizer']),\n",
    "                                          recurrent_dropout = self.hp['rec_dropout'])(word_sequences)\n",
    "            wordEncoder = Model(word_input, word_lstm)\n",
    "        else:\n",
    "            if self.cudnn == True:\n",
    "                word_lstm = self.hp['rnncud'](int(self.hp['rnn_units']), kernel_regularizer=l2(self.hp['l2_regulizer']),\n",
    "                                             return_sequences=True)(word_sequences)\n",
    "            else:\n",
    "                word_lstm = self.hp['rnn'](int(self.hp['rnn_units']), kernel_regularizer=l2(self.hp['l2_regulizer']),\n",
    "                                          recurrent_dropout = self.hp['rec_dropout'], return_sequences=True)(word_sequences)\n",
    "            word_dense = TimeDistributed(Dense(int(self.hp['att_units'])))(word_lstm)\n",
    "            word_att = AttentionWithContext()(word_dense)\n",
    "            wordEncoder = Model(word_input, word_att)\n",
    "        \n",
    "        return wordEncoder\n",
    "    \n",
    "\n",
    "    def word_encoder_conv( self ):\n",
    "        \n",
    "        #TODO add spatial dropout\n",
    "        \n",
    "        word_input = Input(shape = ( self.max_sequence_length, ), dtype='float32')\n",
    "        word_sequences = self.get_embedding_layer()(word_input)\n",
    "\n",
    "        for i in range(self.hp['conv_layers']):\n",
    "            word_sequences = Conv1D(self.hp['filters'], self.hp['kernel_size'], \n",
    "                                    activation='relu',kernel_regularizer=l2(self.hp['l2_regulizer']))(word_sequences)\n",
    "            word_sequences = MaxPooling1D(self.hp['max_pooling'])(word_sequences)\n",
    "\n",
    "        word_sequences = GlobalMaxPooling1D()(word_sequences)\n",
    "        word_sequences = Dense(self.hp['units_conv'], activation='relu',\n",
    "                               kernel_regularizer=l2(self.hp['l2_regulizer']))(word_sequences)\n",
    "        \n",
    "        wordEncoder = Model(word_input, word_sequences)\n",
    "\n",
    "        return wordEncoder\n",
    "    \n",
    "        \n",
    "    def create_model( self ):\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            self.print_hyperparameters()\n",
    "        \n",
    "        \n",
    "        # Input layers\n",
    "        #sent_input = Input(shape = (self.num_error, self.num_sites, None), dtype='int32')\n",
    "        \n",
    "        # Reshape the matrix\n",
    "        #sent_input_reshaped = Reshape(( self.num_error * self.num_sites, ))(sent_input)\n",
    "       \n",
    "        if self.avg_w2v == False:\n",
    "            \n",
    "            sent_input = Input(shape = (self.num_error * self.num_sites, None), dtype='int32')\n",
    "            \n",
    "            # Encode the words of the sentences\n",
    "            if self.word_encoder == 'LSTM':\n",
    "                encoder_units = int(self.hp['rnn_units'])\n",
    "                sent_encoder = TimeDistributed(self.word_encoder_lstm())(sent_input)\n",
    "            elif self.word_encoder == 'Conv1D':\n",
    "                encoder_units = self.hp['units_conv']\n",
    "                sent_encoder = TimeDistributed(self.word_encoder_conv())(sent_input_reshaped)\n",
    "            else: \n",
    "                print( 'No valid encoder' )    \n",
    "\n",
    "\n",
    "            \"\"\"    \n",
    "            sent_encoder = Dropout(self.hp['dropout'])(sent_encoder)\n",
    "            if self.batch_norm == True:\n",
    "                sent_encoder = BatchNormalization()(sent_encoder)\n",
    "            \"\"\"\n",
    "\n",
    "            # Reshape the error sites matrix\n",
    "\n",
    "            sent_encoder_reshaped = Reshape(( self.num_error , self.num_sites, encoder_units))(sent_encoder)\n",
    "         \n",
    "        else:\n",
    "            \n",
    "            sent_input = Input(shape = (self.num_error * self.num_sites, self.max_sequence_length), dtype='float32')\n",
    "            sent_encoder_reshaped = Reshape(( self.num_error , self.num_sites, self.max_sequence_length))(sent_input)\n",
    "            sent_encoder_reshaped = TimeDistributed(Dense(int(self.hp['units_site']), activation = self.hp['activation_site'], \n",
    "                      kernel_regularizer=l2(self.hp['l2_regulizer'])))(sent_encoder_reshaped)\n",
    "            encoder_units = int(self.hp['units_site'])\n",
    "        \n",
    "        # Add the meta information\n",
    "        if self.include_counts == True:\n",
    "            \n",
    "            count_input = Input(shape = (self.num_error, self.num_sites, 2, ), dtype='float32')\n",
    "            print( count_input )\n",
    "            # Merge the counts and words\n",
    "            exit_code_site_repr = Concatenate(axis=3)([sent_encoder_reshaped, count_input])\n",
    "            print( exit_code_site_repr )\n",
    "            exit_code_site_repr = Reshape(( self.num_error , self.num_sites * (encoder_units+2)))(exit_code_site_repr)\n",
    "            print( exit_code_site_repr )\n",
    "        else:\n",
    "            exit_code_site_repr = sent_encoder_reshaped\n",
    "            exit_code_site_repr = Reshape(( self.num_error , self.num_sites * (encoder_units)))(exit_code_site_repr)\n",
    "        \n",
    "        \n",
    "        # Encode the site\n",
    "        if int(self.hp['encode_sites']) == True:\n",
    "            \n",
    "            exit_code_encoder = TimeDistributed(Dense(int(self.hp['units_site']), activation = self.hp['activation_site'], \n",
    "                      kernel_regularizer=l2(self.hp['l2_regulizer'])))(exit_code_site_repr)\n",
    "        else:\n",
    "            exit_code_encoder = exit_code_site_repr\n",
    "\n",
    "            \"\"\"\n",
    "            exit_code_encoder = Dropout(self.hp['dropout'])(exit_code_encoder)\n",
    "            if self.batch_norm == True:\n",
    "                exit_code_encoder = BatchNormalization()(exit_code_encoder)\n",
    "            \"\"\"\n",
    "            \n",
    "        # Flatten\n",
    "        flattened = Flatten()(exit_code_encoder)\n",
    "            \n",
    "        # Dense\n",
    "        dense = flattened\n",
    "        for _ in range(int(self.hp['dense_layers'])):\n",
    "            \n",
    "            dense = Dense( units=int(self.hp['dense_units']), activation='relu', \n",
    "                          kernel_regularizer=l2(self.hp['l2_regulizer']) )(dense)\n",
    "            dense = Dropout(self.hp['dropout'])(dense)\n",
    "            if self.batch_norm == True:\n",
    "                dense = BatchNormalization()(dense)            \n",
    "            \n",
    "        # Output layer\n",
    "        preds = Dense(1, activation='sigmoid', kernel_regularizer=l2(self.hp['l2_regulizer']) )(dense)\n",
    "                \n",
    "        # Final model\n",
    "        if self.include_counts == False:\n",
    "            self.model = Model(sent_input, preds)\n",
    "        else:\n",
    "            self.model = Model([sent_input, count_input], preds)\n",
    "        self.model.compile( loss='binary_crossentropy', optimizer = Adam(lr = self.hp['learning_rate'], \n",
    "                                                                         decay = self.hp['decay']) )\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            self.model.summary()\n",
    "        \n",
    "        return self.model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the keras wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_sites = len(list(set(sites_index.values())))\n",
    "dim_errors = len(list(set(codes_index.values())))\n",
    "embedding_dim = 400\n",
    "embedding_matrix = np.load(e['NLP_PARAM']['embedding_matrix_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dense_units):\n",
    "    \n",
    "    nlp = NLP(2, dim_errors, dim_sites, embedding_dim, \n",
    "                                         embedding_matrix = embedding_matrix,\n",
    "                                         cudnn = e['NLP_PARAM']['cudnn'],\n",
    "                                         batch_norm = e['NLP_PARAM']['batch_norm'], \n",
    "                                         word_encoder = e['NLP_PARAM']['word_encoder'], \n",
    "                                         include_counts = e['NLP_PARAM']['include_counts'], \n",
    "                                         avg_w2v = e['NLP_PARAM']['avg_w2v'],\n",
    "                                         attention = e['NLP_PARAM']['attention'] ) \n",
    "    \n",
    "    model_param = {}\n",
    "    model_param['dense_units'] = dense_units\n",
    "    nlp.set_hyperparameters(model_param)\n",
    "    model = nlp.create_model()\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0916 14:28:18.792243 139877145265984 deprecation_wrapper.py:119] From /cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Mon/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0916 14:28:18.832078 139877145265984 deprecation_wrapper.py:119] From /cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Mon/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0916 14:28:18.843162 139877145265984 deprecation_wrapper.py:119] From /cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Mon/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0916 14:28:18.864232 139877145265984 deprecation_wrapper.py:119] From /cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Mon/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0916 14:28:18.866537 139877145265984 deprecation_wrapper.py:119] From /cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Mon/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter\tCorresponding Value\n",
      "('units_conv', '\\t\\t', 10)\n",
      "('rnncud', '\\t\\t', <class 'keras.layers.cudnn_recurrent.CuDNNLSTM'>)\n",
      "('l2_regulizer', '\\t\\t', 0.0001)\n",
      "('encode_sites', '\\t\\t', False)\n",
      "('learning_rate', '\\t\\t', 0.0001)\n",
      "('rnn', '\\t\\t', <class 'keras.layers.recurrent.LSTM'>)\n",
      "('decay', '\\t\\t', 0.0)\n",
      "('dropout', '\\t\\t', 0.2)\n",
      "('units_site', '\\t\\t', 10)\n",
      "('dense_units', '\\t\\t', 10)\n",
      "('max_pooling', '\\t\\t', 3)\n",
      "('att_units', '\\t\\t', 10)\n",
      "('rec_dropout', '\\t\\t', 0.0)\n",
      "('dense_layers', '\\t\\t', 3)\n",
      "('filters', '\\t\\t', 256)\n",
      "('train_embedding', '\\t\\t', False)\n",
      "('conv_layers', '\\t\\t', 3)\n",
      "('activation_site', '\\t\\t', 'relu')\n",
      "('kernel_size', '\\t\\t', 3)\n",
      "('rnn_units', '\\t\\t', 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0916 14:28:19.296227 139877145265984 deprecation.py:323] From /cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Mon/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2974: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0916 14:28:19.706080 139877145265984 deprecation.py:506] From /cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Mon/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_3:0\", shape=(?, 77, 81, 2), dtype=float32)\n",
      "Tensor(\"concatenate_1/concat:0\", shape=(?, 77, 81, 12), dtype=float32)\n",
      "Tensor(\"reshape_2/Reshape:0\", shape=(?, 77, 972), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0916 14:28:19.856180 139877145265984 deprecation_wrapper.py:119] From /cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Mon/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 6237, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 6237, 10)     1536140     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 77, 81, 10)   0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 77, 81, 2)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 77, 81, 12)   0           reshape_1[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 77, 972)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 74844)        0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           748450      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 10)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           110         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           110         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 10)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,284,821\n",
      "Trainable params: 751,121\n",
      "Non-trainable params: 1,533,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "class InputBatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, frame, label, codes, sites, pad_dim, batch_size = 1, max_msg = 5, \n",
    "                 averaged = False, sequence = False, only_msg = False, cut_front = True):\n",
    "        \n",
    "        self.frame = frame\n",
    "        self.n_tasks = len(frame)\n",
    "        self.label = label\n",
    "        self.batch_size = batch_size\n",
    "        self.codes = codes\n",
    "        self.sites = sites\n",
    "        self.pad_dim = pad_dim\n",
    "        if sequence == False:\n",
    "            self.max_msg = 1\n",
    "        else:\n",
    "            self.max_msg = max_msg\n",
    "        self.averaged = averaged\n",
    "        self.sequence = sequence\n",
    "        self.cut_front = cut_front\n",
    "        self.only_msg = only_msg\n",
    "        self.unique_sites = len(list(set(self.sites.values())))\n",
    "        self.unique_codes = len(list(set(self.codes.values())))\n",
    "        self.n_tasks = len(frame)\n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "    def pad_along_axis(self, array, axis=0):\n",
    "\n",
    "        array = np.array(array)\n",
    "        pad_size = self.pad_dim - array.shape[axis]\n",
    "        axis_nb = len(array.shape)\n",
    "\n",
    "        if pad_size < 0:\n",
    "            if self.cut_front == True:\n",
    "                return array[-self.pad_dim : ]\n",
    "            else:\n",
    "                return array[ : self.pad_dim ]\n",
    "\n",
    "        npad = [(0, 0) for x in range(axis_nb)]\n",
    "        npad[axis] = (0, pad_size)\n",
    "\n",
    "        b = np.pad(array, pad_width=npad, mode='constant', constant_values=int(0))\n",
    "\n",
    "        return b\n",
    "    \n",
    "    \n",
    "    def fill_counts(self, index, error, site, site_state, count):\n",
    "        \n",
    "        # Encode good and bad sites\n",
    "        if site_state == 'good':\n",
    "            site_state_encoded = 0\n",
    "        else:\n",
    "            site_state_encoded = 1\n",
    "\n",
    "        self.error_site_counts[index, self.codes[error], self.sites[site], site_state_encoded] += count\n",
    "    \n",
    "    \n",
    "    def fill_first_message(self, index, error, site, error_message):\n",
    "        \n",
    "                               \n",
    "        # Pad the error message\n",
    "        if self.averaged == False:\n",
    "            error_message = self.pad_along_axis(error_message)\n",
    "        #print( error_message )\n",
    "        self.error_site_tokens[index, self.codes[error], self.sites[site]] = error_message\n",
    "\n",
    "    \n",
    " \n",
    "    def fill_messages_sequence(self, index, error, site, error_message_sequence):\n",
    "        \n",
    "        # Loop over the error message sequence\n",
    "        for counter, error_message in enumerate(error_message_sequence):\n",
    "           \n",
    "            # Stop when maximal message is reached\n",
    "            if counter == self.max_msg:\n",
    "                break           \n",
    "            \n",
    "            # Pad the error message\n",
    "            if self.averaged == False:\n",
    "                error_message = self.pad_along_axis(error_message)\n",
    "                \n",
    "            \n",
    "            # Sequence per task, error, site\n",
    "            self.error_site_tokens[index, self.codes[error], self.sites[site], counter ] = error_message    \n",
    "            \n",
    "    \n",
    "    def to_dense(self, index_matrix, values):\n",
    "        \n",
    "        errors, sites, counts, site_states, error_messages = values\n",
    "        \n",
    "        # Loop over the codes and sites\n",
    "        for i_key in range(len(counts)):\n",
    "            \n",
    "            error = errors[i_key]\n",
    "            site = sites[i_key]\n",
    "            count = counts[i_key]\n",
    "            site_state = site_states[i_key]\n",
    "    \n",
    "            \n",
    "            # Fill the counts\n",
    "            if self.only_msg == False:\n",
    "                self.fill_counts(index_matrix, error, site, site_state, count)\n",
    "           \n",
    "            if self.only_counts == True:\n",
    "                continue\n",
    "            \n",
    "            error_message_sequence = error_messages[i_key]\n",
    "            \n",
    "            # Only continue if there exists a message\n",
    "            if isinstance(error_message_sequence, (list,)):\n",
    "                \n",
    "                # Fill the error message\n",
    "                if self.sequence == True:\n",
    "                    self.fill_messages_sequence( index_matrix, error, site, error_message_sequence)\n",
    "                else:\n",
    "                    self.fill_first_message( index_matrix, error, site, error_message_sequence)\n",
    "                    \n",
    "\n",
    "                \n",
    "    def get_counts_matrix(self, sum_good_bad = False):\n",
    "        \n",
    "        self.only_counts = True\n",
    "        \n",
    "        self.error_site_counts = np.zeros((self.n_tasks, self.unique_codes, self.unique_sites, 2), dtype=np.int32)\n",
    "        batch = self.frame\n",
    "        [self.to_dense(counter, values) for counter, values in enumerate(zip(self.frame['error'], self.frame['site'], \n",
    "                                                                             self.frame['count'], self.frame['site_state'],\n",
    "                                                                             self.frame['msg_encoded'],))]        \n",
    "        if sum_good_bad == True:\n",
    "            return self.error_site_counts.sum(axis=3), self.frame[self.label].values\n",
    "        else:\n",
    "            return self.error_site_counts, self.frame[self.label].values        \n",
    "    \n",
    "    \n",
    "    def msg_batch(self, start_pos, end_pos):\n",
    "        \n",
    "        self.only_counts = False\n",
    "        \n",
    "        # Batch of frame\n",
    "        batch = self.frame.iloc[start_pos : end_pos]\n",
    "        chunk_size = len(batch)\n",
    "        \n",
    "        # Tokens\n",
    "        if self.averaged == False:\n",
    "            tokens_key = 'msg_encoded'\n",
    "            self.pad_dim = 1\n",
    "            msg_t = []\n",
    "            for key in batch[tokens_key]:\n",
    "                for msg in key:\n",
    "                    if isinstance(msg, (list,)):\n",
    "                        if len(msg) > self.pad_dim:\n",
    "                            msg_t = msg\n",
    "                            self.pad_dim = len(msg)\n",
    "                        \n",
    "            if self.pad_dim > 200:\n",
    "                self.pad_dim = 200\n",
    "        else:\n",
    "            tokens_key = 'avg'\n",
    "       \n",
    "        \n",
    "        #print( self.pad_dim )\n",
    "        #print( msg )\n",
    "        \n",
    "        # Error site matrix\n",
    "        self.error_site_counts = np.zeros((chunk_size, self.unique_codes, self.unique_sites, 2), dtype=np.int32)\n",
    "        \n",
    "        if self.sequence == True:\n",
    "            dim = (chunk_size, self.unique_codes, self.unique_sites, self.max_msg, self.pad_dim)\n",
    "        else:\n",
    "            dim = (chunk_size, self.unique_codes, self.unique_sites, self.pad_dim)    \n",
    "        \n",
    "        \n",
    "        # Error message matrix\n",
    "        self.error_site_tokens = np.zeros(dim, dtype=np.int32)\n",
    "        \n",
    "        [self.to_dense(counter, values) for counter, values in enumerate(zip(batch['error'], batch['site'], batch['count'],\n",
    "                                                                          batch['site_state'], batch[tokens_key]))]\n",
    "        \n",
    "        if self.only_msg == False:\n",
    "            #self.error_site_tokens = np.reshape(\n",
    "            #print self.error_site_tokens.shape\n",
    "            return [self.error_site_tokens.reshape((chunk_size, self.unique_codes * self.unique_sites, self.pad_dim)) , self.error_site_counts]   \n",
    "        else:\n",
    "            return self.error_site_tokens\n",
    "    \n",
    "    \n",
    "    def gen_batches(self):\n",
    "        \n",
    "        for cur_pos in range(0, self.n_tasks, self.batch_size):\n",
    " \n",
    "            next_pos = cur_pos + self.batch_size \n",
    "            if next_pos <= self.n_tasks:\n",
    "                yield (self.msg_batch( cur_pos, next_pos ), self.frame[self.label].iloc[cur_pos : next_pos].values)\n",
    "            else:\n",
    "                yield (self.msg_batch( cur_pos, self.n_tasks ), self.frame[self.label].iloc[cur_pos : self.n_tasks].values)   \n",
    "                  \n",
    "                    \n",
    "    def gen_inf_batches(self):\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                for B in self.gen_batches():\n",
    "                    yield B\n",
    "            except StopIteration:\n",
    "                logging.warning(\"start over generator loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "class KerasClassifierCustom(KerasClassifier):\n",
    "\n",
    "  \n",
    "    \n",
    "    def fit(self, x, y, **kwargs):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if self.build_fn is None:\n",
    "            self.model = self.__call__(**self.filter_sk_params(self.__call__))\n",
    "        elif not isinstance(self.build_fn, types.FunctionType):\n",
    "            self.model = self.build_fn(\n",
    "                **self.filter_sk_params(self.build_fn.__call__))\n",
    "        else:\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
    "\n",
    "        \"\"\"\n",
    "        self.model = self.build_fn(\n",
    "            **self.filter_sk_params(self.build_fn))    \n",
    "                \n",
    "        \"\"\"\n",
    "        self.model = self.build_fn()\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit_generator))\n",
    "        fit_args.update(kwargs)\n",
    "        \n",
    "        print( fit_args )\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = 1\n",
    "        self.classes_ = np.unique(y)\n",
    "        steps_per_epoch = int(len(x) / batch_size)\n",
    "\n",
    "        generator_train = InputBatchGenerator(x, 'label', codes_index, sites_index,\n",
    "                                                                    100, batch_size = batch_size)   \n",
    "    \n",
    "        self.model.fit_generator(generator = generator_train.gen_inf_batches(), steps_per_epoch = steps_per_epoch, \n",
    "                                  epochs = 1, workers=0)        \n",
    "        \n",
    "        \n",
    "        \n",
    "  \n",
    "    def predict_proba(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        preds = self.model.predict_generator(\n",
    "                    self.get_batch(x, None, self.sk_params[\"batch_size\"]), \n",
    "                                               val_samples=x.shape[0])\n",
    "        return preds\n",
    "        \"\"\"\n",
    "        generator_test = InputBatchGenerator(x, 'label', codes_index, sites_index,\n",
    "                                                            100, batch_size = 1)  \n",
    "        \n",
    "        y_pred_batches = []\n",
    "        for X,y in generator_test.gen_batches():\n",
    "            y_pred_batches.append(np.asarray(self.model.predict(X)))\n",
    "\n",
    "        y_pred = np.concatenate(y_pred_batches)  \n",
    "        y_pred_sk = [ [1-pred, pred] for pred in y_pred ]\n",
    "        \n",
    "        return np.array(y_pred_sk)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KerasClassifierCustom(build_fn=build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "majority_class_sampled = actionshist[actionshist['label'] == 0].sample(n_samples , random_state=42)\n",
    "minority_class_sampled = actionshist[actionshist['label'] == 1].sample(n_samples , random_state=42)\n",
    "t = pd.concat([minority_class_sampled, majority_class_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "t = t.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter\tCorresponding Value\n",
      "('units_conv', '\\t\\t', 10)\n",
      "('rnncud', '\\t\\t', <class 'keras.layers.cudnn_recurrent.CuDNNLSTM'>)\n",
      "('l2_regulizer', '\\t\\t', 0.0001)\n",
      "('encode_sites', '\\t\\t', False)\n",
      "('learning_rate', '\\t\\t', 0.0001)\n",
      "('rnn', '\\t\\t', <class 'keras.layers.recurrent.LSTM'>)\n",
      "('decay', '\\t\\t', 0.0)\n",
      "('dropout', '\\t\\t', 0.2)\n",
      "('units_site', '\\t\\t', 10)\n",
      "('dense_units', '\\t\\t', 3)\n",
      "('max_pooling', '\\t\\t', 3)\n",
      "('att_units', '\\t\\t', 10)\n",
      "('rec_dropout', '\\t\\t', 0.0)\n",
      "('dense_layers', '\\t\\t', 3)\n",
      "('filters', '\\t\\t', 256)\n",
      "('train_embedding', '\\t\\t', False)\n",
      "('conv_layers', '\\t\\t', 3)\n",
      "('activation_site', '\\t\\t', 'relu')\n",
      "('kernel_size', '\\t\\t', 3)\n",
      "('rnn_units', '\\t\\t', 10)\n",
      "Tensor(\"input_89:0\", shape=(?, 77, 81, 2), dtype=float32)\n",
      "Tensor(\"concatenate_29/concat:0\", shape=(?, 77, 81, 12), dtype=float32)\n",
      "Tensor(\"reshape_58/Reshape:0\", shape=(?, 77, 972), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_87 (InputLayer)           (None, 6237, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_29 (TimeDistri (None, 6237, 10)     1536140     input_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_57 (Reshape)            (None, 77, 81, 10)   0           time_distributed_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_89 (InputLayer)           (None, 77, 81, 2)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 77, 81, 12)   0           reshape_57[0][0]                 \n",
      "                                                                 input_89[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_58 (Reshape)            (None, 77, 972)      0           concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 74844)        0           reshape_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_113 (Dense)               (None, 3)            224535      flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 3)            0           dense_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_114 (Dense)               (None, 3)            12          dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 3)            0           dense_114[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_115 (Dense)               (None, 3)            12          dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 3)            0           dense_115[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_116 (Dense)               (None, 1)            4           dropout_87[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,760,703\n",
      "Trainable params: 227,003\n",
      "Non-trainable params: 1,533,700\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "(1, 77, 81, 157)\n",
      "  1/100 [..............................] - ETA: 14:26 - loss: 0.6990(1, 77, 81, 1)\n",
      "(1, 77, 81, 1)\n",
      "(1, 77, 81, 1)\n",
      "  4/100 [>.............................] - ETA: 3:31 - loss: 0.6990 (1, 77, 81, 1)\n",
      "(1, 77, 81, 365)\n",
      "  6/100 [>.............................] - ETA: 3:15 - loss: 0.6990(1, 77, 81, 64)\n",
      "  7/100 [=>............................] - ETA: 2:53 - loss: 0.6990(1, 77, 81, 22)\n",
      "  8/100 [=>............................] - ETA: 2:32 - loss: 0.7021(1, 77, 81, 1)\n",
      "(1, 77, 81, 18)\n",
      " 10/100 [==>...........................] - ETA: 2:00 - loss: 0.7014(1, 77, 81, 400)\n",
      " 11/100 [==>...........................] - ETA: 2:21 - loss: 0.7012(1, 77, 81, 310)\n",
      " 12/100 [==>...........................] - ETA: 2:30 - loss: 0.7010(1, 77, 81, 1)\n",
      "(1, 77, 81, 1)\n",
      "(1, 77, 81, 1)\n",
      " 15/100 [===>..........................] - ETA: 1:56 - loss: 0.7006(1, 77, 81, 74)\n",
      " 16/100 [===>..........................] - ETA: 1:51 - loss: 0.7005(1, 77, 81, 1)\n",
      " 17/100 [====>.........................] - ETA: 1:44 - loss: 0.7004(1, 77, 81, 354)\n",
      " 18/100 [====>.........................] - ETA: 1:52 - loss: 0.7003(1, 77, 81, 259)\n",
      " 19/100 [====>.........................] - ETA: 1:56 - loss: 0.7003(1, 77, 81, 400)\n",
      " 20/100 [=====>........................] - ETA: 2:04 - loss: 0.7002(1, 77, 81, 325)\n",
      " 21/100 [=====>........................] - ETA: 2:09 - loss: 0.7002(1, 77, 81, 1)\n",
      " 22/100 [=====>........................] - ETA: 2:02 - loss: 0.7001(1, 77, 81, 63)\n",
      " 23/100 [=====>........................] - ETA: 1:57 - loss: 0.7027(1, 77, 81, 152)\n",
      " 24/100 [======>.......................] - ETA: 1:55 - loss: 0.7026(1, 77, 81, 1)\n",
      "(1, 77, 81, 1)\n",
      " 26/100 [======>.......................] - ETA: 1:43 - loss: 0.7023(1, 77, 81, 1)\n",
      "(1, 77, 81, 151)\n",
      " 28/100 [=======>......................] - ETA: 1:37 - loss: 0.7020(1, 77, 81, 18)\n",
      " 29/100 [=======>......................] - ETA: 1:33 - loss: 0.7019(1, 77, 81, 26)\n",
      " 30/100 [========>.....................] - ETA: 1:29 - loss: 0.7018(1, 77, 81, 400)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-c855e2bfba5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_log_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          n_jobs=1, cv=2)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvalidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-40eb996bb113>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         self.model.fit_generator(generator = generator_train.gen_inf_batches(), steps_per_epoch = steps_per_epoch, \n\u001b[0;32m---> 43\u001b[0;31m                                   epochs = 1, workers=0)        \n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Sun/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validator = GridSearchCV(test,\n",
    "                         param_grid={'dense_units': [3, 6]},\n",
    "                         scoring='neg_log_loss',\n",
    "                         n_jobs=1, cv=2)\n",
    "validator.fit(t, t['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_sklearn import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def scorer(estimator, X, y):\n",
    "    \n",
    "    y_pred = estimator.predict_proba(X)[:,1]\n",
    "    score = roc_auc_score(X['label'], y_pred)\n",
    "    \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "Hyperparameter\tCorresponding Value\n",
      "('units_conv', '\\t\\t', 10)\n",
      "('rnncud', '\\t\\t', <class 'keras.layers.cudnn_recurrent.CuDNNLSTM'>)\n",
      "('l2_regulizer', '\\t\\t', 0.0001)\n",
      "('encode_sites', '\\t\\t', False)\n",
      "('learning_rate', '\\t\\t', 0.0001)\n",
      "('rnn', '\\t\\t', <class 'keras.layers.recurrent.LSTM'>)\n",
      "('decay', '\\t\\t', 0.0)\n",
      "('dropout', '\\t\\t', 0.2)\n",
      "('units_site', '\\t\\t', 10)\n",
      "('dense_units', '\\t\\t', 3)\n",
      "('max_pooling', '\\t\\t', 3)\n",
      "('att_units', '\\t\\t', 10)\n",
      "('rec_dropout', '\\t\\t', 0.0)\n",
      "('dense_layers', '\\t\\t', 3)\n",
      "('filters', '\\t\\t', 256)\n",
      "('train_embedding', '\\t\\t', False)\n",
      "('conv_layers', '\\t\\t', 3)\n",
      "('activation_site', '\\t\\t', 'relu')\n",
      "('kernel_size', '\\t\\t', 3)\n",
      "('rnn_units', '\\t\\t', 10)\n",
      "Tensor(\"input_6:0\", shape=(?, 77, 81, 2), dtype=float32)\n",
      "Tensor(\"concatenate_2/concat:0\", shape=(?, 77, 81, 12), dtype=float32)\n",
      "Tensor(\"reshape_4/Reshape:0\", shape=(?, 77, 972), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 6237, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 6237, 10)     1536140     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 77, 81, 10)   0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 77, 81, 2)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 77, 81, 12)   0           reshape_3[0][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 77, 972)      0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 74844)        0           reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            224535      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3)            0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            12          dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 3)            0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 3)            12          dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 3)            0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            4           dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,760,703\n",
      "Trainable params: 227,003\n",
      "Non-trainable params: 1,533,700\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.6988\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(sc, estimator=clf, param_grid={'dense_units': [3, 6]}, scoring=scorer, verbose=1)\n",
    "gridSearch_result = grid.fit(t, t['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'build_fn': <function __main__.build_model>, 'dense_units': 6}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridSearch_result.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'mean_train_score',\n",
       " 'param_dense_units',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split0_train_score',\n",
       " 'split1_test_score',\n",
       " 'split1_train_score',\n",
       " 'split2_test_score',\n",
       " 'split2_train_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score',\n",
       " 'std_train_score']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(gridSearch_result.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(gridSearch_result.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_dense_units</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.743652</td>\n",
       "      <td>7.659284</td>\n",
       "      <td>0.473393</td>\n",
       "      <td>0.535689</td>\n",
       "      <td>3</td>\n",
       "      <td>{u'dense_units': 3}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.377551</td>\n",
       "      <td>0.479290</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.530556</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>1.365076</td>\n",
       "      <td>0.684214</td>\n",
       "      <td>0.092749</td>\n",
       "      <td>0.048282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.066958</td>\n",
       "      <td>7.909493</td>\n",
       "      <td>0.350536</td>\n",
       "      <td>0.538900</td>\n",
       "      <td>6</td>\n",
       "      <td>{u'dense_units': 6}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.727811</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.427778</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.461111</td>\n",
       "      <td>1.475866</td>\n",
       "      <td>0.623274</td>\n",
       "      <td>0.034799</td>\n",
       "      <td>0.134271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      43.743652         7.659284         0.473393          0.535689   \n",
       "1      45.066958         7.909493         0.350536          0.538900   \n",
       "\n",
       "  param_dense_units               params  rank_test_score  split0_test_score  \\\n",
       "0                 3  {u'dense_units': 3}                1           0.377551   \n",
       "1                 6  {u'dense_units': 6}                2           0.397959   \n",
       "\n",
       "   split0_train_score  split1_test_score  split1_train_score  \\\n",
       "0            0.479290              0.450            0.530556   \n",
       "1            0.727811              0.325            0.427778   \n",
       "\n",
       "   split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
       "0              0.600            0.597222      1.365076        0.684214   \n",
       "1              0.325            0.461111      1.475866        0.623274   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.092749         0.048282  \n",
       "1        0.034799         0.134271  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive",
    "CMSSpark",
    "ComputeIntensive"
   ],
   "list_of_options": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
