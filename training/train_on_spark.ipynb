{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of the spark_sklearn fit for the NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Collecting spark_sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/3f/34b8dec7d2cfcfe0ba99d637b4f2d306c1ca0b404107c07c829e085f6b38/spark-sklearn-0.3.0.tar.gz\n",
      "Collecting scikit-learn<0.20,>=0.18.1 (from spark_sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/67/370aa248f54769a56216707ad7b9af19745e85a603fafa47bde353f327fb/scikit_learn-0.19.2-cp27-cp27mu-manylinux1_x86_64.whl (5.0MB)\n",
      "\u001b[K     |████████████████████████████████| 5.0MB 10.8MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: spark-sklearn\n",
      "  Building wheel for spark-sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/llayer/.cache/pip/wheels/64/28/e8/cb0250888675c630786f932dcc63ed96ac1aca299bcfb7235f\n",
      "Successfully built spark-sklearn\n",
      "Installing collected packages: scikit-learn, spark-sklearn\n",
      "Successfully installed scikit-learn-0.19.2 spark-sklearn-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spark_sklearn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_dense_units</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27938.136857</td>\n",
       "      <td>4320.852811</td>\n",
       "      <td>0.549173</td>\n",
       "      <td>0.648172</td>\n",
       "      <td>20</td>\n",
       "      <td>{u'dense_units': 20}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.528953</td>\n",
       "      <td>0.624694</td>\n",
       "      <td>0.535976</td>\n",
       "      <td>0.631246</td>\n",
       "      <td>0.582593</td>\n",
       "      <td>0.688577</td>\n",
       "      <td>1262.657048</td>\n",
       "      <td>387.809381</td>\n",
       "      <td>0.023804</td>\n",
       "      <td>0.028695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29402.235786</td>\n",
       "      <td>4394.115639</td>\n",
       "      <td>0.549175</td>\n",
       "      <td>0.648897</td>\n",
       "      <td>50</td>\n",
       "      <td>{u'dense_units': 50}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541926</td>\n",
       "      <td>0.626391</td>\n",
       "      <td>0.532695</td>\n",
       "      <td>0.644813</td>\n",
       "      <td>0.572905</td>\n",
       "      <td>0.675489</td>\n",
       "      <td>1178.839703</td>\n",
       "      <td>370.577449</td>\n",
       "      <td>0.017197</td>\n",
       "      <td>0.020251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0   27938.136857      4320.852811         0.549173          0.648172   \n",
       "1   29402.235786      4394.115639         0.549175          0.648897   \n",
       "\n",
       "  param_dense_units                params  rank_test_score  split0_test_score  \\\n",
       "0                20  {u'dense_units': 20}                2           0.528953   \n",
       "1                50  {u'dense_units': 50}                1           0.541926   \n",
       "\n",
       "   split0_train_score  split1_test_score  split1_train_score  \\\n",
       "0            0.624694           0.535976            0.631246   \n",
       "1            0.626391           0.532695            0.644813   \n",
       "\n",
       "   split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.582593            0.688577   1262.657048      387.809381   \n",
       "1           0.572905            0.675489   1178.839703      370.577449   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.023804         0.028695  \n",
       "1        0.017197         0.020251  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_hdf('results.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(sites, codes):\n",
    "\n",
    "    sites_index = {k: v for v, k in enumerate(sites)}\n",
    "    codes_index = {k: v for v, k in enumerate(codes)}\n",
    "    return sites_index, codes_index\n",
    "\n",
    "\n",
    "def prune_to_index(codes, sites, only_unknown = False, counts = False, error_threshold = 0, site_threshold = 0):\n",
    "    \n",
    "    all_sites = list(sites['site'])\n",
    "    all_codes = list(codes['error'])\n",
    "    good_sites = list(sites['site'])\n",
    "    good_codes = list(codes['error'])\n",
    "    \n",
    "    if only_unknown == True:\n",
    "        informative_sites = list(sites[sites['only_unknown'] == False]['site'])\n",
    "        good_sites = list(set(informative_sites) & set(good_sites))  \n",
    "\n",
    "    if site_threshold > 0:\n",
    "        if counts == False:\n",
    "            frequent_sites = list(sites[sites['frequency'] > site_threshold]['site'])\n",
    "        else:\n",
    "            frequent_sites = list(sites[sites['counts'] > site_threshold]['site'])\n",
    "        good_sites = list(set(frequent_sites) & set(good_sites))  \n",
    "            \n",
    "    if error_threshold > 0:\n",
    "        if counts == False:\n",
    "            frequent_errors = list(codes[codes['frequency'] > error_threshold]['error'])\n",
    "        else:\n",
    "            frequent_errors = list(codes[codes['counts'] > error_threshold]['error'])    \n",
    "        good_codes = list(set(frequent_errors) & set(good_codes)) \n",
    "        \n",
    "    # Get the pruned sites and codes\n",
    "    pruned_sites = list(set(all_sites) - set(good_sites))\n",
    "    pruned_codes = list(set(all_codes) - set(good_codes))\n",
    "    \n",
    "    # Index the results\n",
    "    good_sites_index = {k: v for v, k in enumerate(good_sites)}\n",
    "    pruned_sites_index = {k: len(good_sites) for k in pruned_sites}\n",
    "    good_codes_index = {k: v for v, k in enumerate(good_codes)}\n",
    "    pruned_codes_index = {k: len(good_codes) for k in pruned_codes}    \n",
    "        \n",
    "    def merge_dicts(x, y):\n",
    "        z = x.copy()   \n",
    "        z.update(y) \n",
    "        return z\n",
    "    \n",
    "    codes_index = merge_dicts(good_codes_index, pruned_codes_index)\n",
    "    sites_index = merge_dicts(good_sites_index, pruned_sites_index)\n",
    "    \n",
    "    return sites_index, codes_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, load_labels = True, msg_only = False, sample = False, sample_fact = 3):\n",
    "\n",
    "    actionshist = pd.read_hdf(path, 'frame')\n",
    "    print( len(actionshist) )\n",
    "    if sample == True:\n",
    "        minority_class = actionshist[actionshist['label'] == 1]\n",
    "        n_samples = int(sample_fact*len(minority_class))\n",
    "        majority_class_sampled = actionshist[actionshist['label'] == 0].sample(n_samples , random_state=42)\n",
    "        print('After sampling:', 'Minority class', len(minority_class), 'Majority class', len(majority_class_sampled) )\n",
    "        actionshist = pd.concat([minority_class, majority_class_sampled])\n",
    "\n",
    "    if msg_only == False:\n",
    "        sites = pd.read_hdf(path, 'frame2')\n",
    "        codes = pd.read_hdf(path, 'frame3')\n",
    "    else:\n",
    "        codes = pd.read_hdf(path, 'frame4')\n",
    "        sites = pd.read_hdf(path, 'frame5')\n",
    "        codes.rename({'errors_msg': 'error'}, axis=1, inplace=True)\n",
    "        sites.rename({'sites_msg': 'site'}, axis=1, inplace=True)\n",
    "\n",
    "    if load_labels == True:\n",
    "        actionshist = actionshist.drop(['label'], axis=1)\n",
    "        labels = pd.read_hdf('/eos/user/l/llayer/AIErrorLogAnalysis/data/input/labels.h5')\n",
    "        actionshist = pd.merge( actionshist, labels, on = ['task_name'], how='inner')\n",
    "\n",
    "    print( actionshist['label'].value_counts() )\n",
    "\n",
    "    return actionshist, codes, sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33586\n",
      "0    29897\n",
      "1     3689\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "\n",
    "# Include counts\n",
    "MSG_ONLY = False\n",
    "PRUNING = 'Neg'\n",
    "\n",
    "# sample\n",
    "SAMPLE = False\n",
    "SAMPLE_FACT = 5\n",
    "\n",
    "# batch generator param\n",
    "AVG_W2V = False\n",
    "MAX_WORDS = 400\n",
    "GEN_PARAM = {}\n",
    "GEN_PARAM['averaged'] = AVG_W2V\n",
    "GEN_PARAM['only_msg'] = MSG_ONLY \n",
    "GEN_PARAM['sequence'] = False\n",
    "GEN_PARAM['max_msg'] = 1\n",
    "GEN_PARAM['cut_front'] = True\n",
    "TRAIN_ON_BATCH = True\n",
    "\n",
    "# Model\n",
    "MODEL = 'nlp_msg'\n",
    "\n",
    "# Defines the input experiments for the machine learning\n",
    "EXPERIMENTS = [\n",
    "    \n",
    "    # 1st experiment initial parameter\n",
    "    {'NAME': 'VAR_DIM', 'DIM':20, 'VOCAB': -1, 'ALGO': 'sg',\n",
    "     'NLP_PARAM': {'cudnn': False, 'batch_norm': False, 'word_encoder': 'LSTM', \n",
    "                   'attention': True, 'include_counts': True, 'avg_w2v': False},\n",
    "     'CALLBACK': { 'es': True, 'patience': 3, 'kill_slowstarts': True, 'kill_threshold': 0.51 }\n",
    "    }\n",
    "]\n",
    "    \n",
    "e = EXPERIMENTS[ 0 ]\n",
    "\n",
    "# Load the data\n",
    "path = '/eos/user/l/llayer/AIErrorLogAnalysis/data/input/' + 'input_' + 'VAR_DIM' + '.h5'\n",
    "actionshist, codes, sites = load_data(path, msg_only=MSG_ONLY,\n",
    "                                                  sample=SAMPLE, sample_fact = SAMPLE_FACT)\n",
    "e['NLP_PARAM']['embedding_matrix_path'] = '/eos/user/l/llayer/AIErrorLogAnalysis/data/word2vec/' + 'embedding_matrix_' + 'VAR_DIM' + '.npy'\n",
    "\n",
    "sites_index, codes_index = sites_index, codes_index = prune_to_index(codes, sites, only_unknown = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>error</th>\n",
       "      <th>site</th>\n",
       "      <th>site_state</th>\n",
       "      <th>count</th>\n",
       "      <th>msg_encoded</th>\n",
       "      <th>exit_code</th>\n",
       "      <th>error_type</th>\n",
       "      <th>avg</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T1_US_FNAL_Disk, T3_US_FNALLPC]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...</td>\n",
       "      <td>[-1, -1, -1, -1, 85]</td>\n",
       "      <td>[T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...</td>\n",
       "      <td>[bad, bad, bad, bad, good]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...</td>\n",
       "      <td>[nan, nan, nan, nan, 8021.0]</td>\n",
       "      <td>[nan, nan, nan, nan, Fatal Exception]</td>\n",
       "      <td>[nan, nan, nan, nan, [-1.85501146317, 0.958182...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...</td>\n",
       "      <td>[50664, -1, -1]</td>\n",
       "      <td>[T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]</td>\n",
       "      <td>[good, good, good]</td>\n",
       "      <td>[2, 1, 1]</td>\n",
       "      <td>[[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...</td>\n",
       "      <td>[50664.0, nan, nan]</td>\n",
       "      <td>[PerformanceKill, nan, nan]</td>\n",
       "      <td>[[-2.10253405571, 0.910805642605, 0.9228593707...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           task_name                 error  \\\n",
       "0  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...              [-1, -1]   \n",
       "1  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...              [-1, -1]   \n",
       "2  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...              [-1, -1]   \n",
       "3  /amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...  [-1, -1, -1, -1, 85]   \n",
       "4  /amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...       [50664, -1, -1]   \n",
       "\n",
       "                                                site  \\\n",
       "0                   [T1_US_FNAL_Disk, T3_US_FNALLPC]   \n",
       "1                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "2                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "3  [T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...   \n",
       "4               [T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]   \n",
       "\n",
       "                   site_state            count  \\\n",
       "0                  [bad, bad]           [1, 1]   \n",
       "1                  [bad, bad]           [1, 1]   \n",
       "2                  [bad, bad]           [1, 1]   \n",
       "3  [bad, bad, bad, bad, good]  [1, 1, 1, 1, 1]   \n",
       "4          [good, good, good]        [2, 1, 1]   \n",
       "\n",
       "                                         msg_encoded  \\\n",
       "0                                         [nan, nan]   \n",
       "1                                         [nan, nan]   \n",
       "2                                         [nan, nan]   \n",
       "3  [nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...   \n",
       "4  [[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...   \n",
       "\n",
       "                      exit_code                             error_type  \\\n",
       "0                    [nan, nan]                             [nan, nan]   \n",
       "1                    [nan, nan]                             [nan, nan]   \n",
       "2                    [nan, nan]                             [nan, nan]   \n",
       "3  [nan, nan, nan, nan, 8021.0]  [nan, nan, nan, nan, Fatal Exception]   \n",
       "4           [50664.0, nan, nan]            [PerformanceKill, nan, nan]   \n",
       "\n",
       "                                                 avg  label  \n",
       "0                                         [nan, nan]      0  \n",
       "1                                         [nan, nan]      0  \n",
       "2                                         [nan, nan]      0  \n",
       "3  [nan, nan, nan, nan, [-1.85501146317, 0.958182...      0  \n",
       "4  [[-2.10253405571, 0.910805642605, 0.9228593707...      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionshist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionshist = actionshist.drop(['task_name', 'avg', 'error_type', 'exit_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "      <th>site</th>\n",
       "      <th>site_state</th>\n",
       "      <th>count</th>\n",
       "      <th>msg_encoded</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T1_US_FNAL_Disk, T3_US_FNALLPC]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-1, -1, -1, -1, 85]</td>\n",
       "      <td>[T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...</td>\n",
       "      <td>[bad, bad, bad, bad, good]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[50664, -1, -1]</td>\n",
       "      <td>[T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]</td>\n",
       "      <td>[good, good, good]</td>\n",
       "      <td>[2, 1, 1]</td>\n",
       "      <td>[[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  error                                               site  \\\n",
       "0              [-1, -1]                   [T1_US_FNAL_Disk, T3_US_FNALLPC]   \n",
       "1              [-1, -1]                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "2              [-1, -1]                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "3  [-1, -1, -1, -1, 85]  [T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...   \n",
       "4       [50664, -1, -1]               [T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]   \n",
       "\n",
       "                   site_state            count  \\\n",
       "0                  [bad, bad]           [1, 1]   \n",
       "1                  [bad, bad]           [1, 1]   \n",
       "2                  [bad, bad]           [1, 1]   \n",
       "3  [bad, bad, bad, bad, good]  [1, 1, 1, 1, 1]   \n",
       "4          [good, good, good]        [2, 1, 1]   \n",
       "\n",
       "                                         msg_encoded  label  \n",
       "0                                         [nan, nan]      0  \n",
       "1                                         [nan, nan]      0  \n",
       "2                                         [nan, nan]      0  \n",
       "3  [nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...      0  \n",
       "4  [[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionshist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_list(msgs):\n",
    "    new_array = []\n",
    "    for msg in msgs:\n",
    "        if not isinstance(msg, list):\n",
    "            new_array.append([])\n",
    "        else:\n",
    "            new_array.append(msg)\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionshist['msg'] = actionshist['msg_encoded'].apply(insert_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>error</th>\n",
       "      <th>site</th>\n",
       "      <th>site_state</th>\n",
       "      <th>count</th>\n",
       "      <th>msg_encoded</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T1_US_FNAL_Disk, T3_US_FNALLPC]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, 85]</td>\n",
       "      <td>[T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...</td>\n",
       "      <td>[bad, bad, bad, bad, good]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...</td>\n",
       "      <td>[[], [], [], [], [35, 12, 10, 37, 186, 34, 25,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[50664, -1, -1]</td>\n",
       "      <td>[T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]</td>\n",
       "      <td>[good, good, good]</td>\n",
       "      <td>[2, 1, 1]</td>\n",
       "      <td>[[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...</td>\n",
       "      <td>[[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                 error  \\\n",
       "0      0              [-1, -1]   \n",
       "1      0              [-1, -1]   \n",
       "2      0              [-1, -1]   \n",
       "3      0  [-1, -1, -1, -1, 85]   \n",
       "4      0       [50664, -1, -1]   \n",
       "\n",
       "                                                site  \\\n",
       "0                   [T1_US_FNAL_Disk, T3_US_FNALLPC]   \n",
       "1                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "2                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "3  [T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...   \n",
       "4               [T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]   \n",
       "\n",
       "                   site_state            count  \\\n",
       "0                  [bad, bad]           [1, 1]   \n",
       "1                  [bad, bad]           [1, 1]   \n",
       "2                  [bad, bad]           [1, 1]   \n",
       "3  [bad, bad, bad, bad, good]  [1, 1, 1, 1, 1]   \n",
       "4          [good, good, good]        [2, 1, 1]   \n",
       "\n",
       "                                         msg_encoded  \\\n",
       "0                                         [nan, nan]   \n",
       "1                                         [nan, nan]   \n",
       "2                                         [nan, nan]   \n",
       "3  [nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...   \n",
       "4  [[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...   \n",
       "\n",
       "                                                 msg  \n",
       "0                                           [[], []]  \n",
       "1                                           [[], []]  \n",
       "2                                           [[], []]  \n",
       "3  [[], [], [], [], [35, 12, 10, 37, 186, 34, 25,...  \n",
       "4  [[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionshist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = actionshist[['label', 'error', 'site', 'msg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = [[[[2,1], []], [1,0]]]\n",
    "tf = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[2, 1], []]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0       1\n",
       "0  [[2, 1], []]  [1, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, error=[u'-1', u'-1'], site=[u'T1_US_FNAL_Disk', u'T3_US_FNALLPC'], msg=[[], []])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number epsilon to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed, Dropout, Flatten, Reshape\n",
    "from keras.layers import average, Concatenate, Lambda, CuDNNLSTM, CuDNNGRU, Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers import AveragePooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import BatchNormalization\n",
    "K.set_floatx('float32')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NLP():\n",
    "    \n",
    "    \n",
    "    def __init__(self, num_classes, num_error, num_sites, max_sequence_length, embedding_matrix,\n",
    "                 cudnn = False, batch_norm = False, word_encoder = 'LSTM', encode_sites = True, attention = False,\n",
    "                 include_counts = False, avg_w2v = False, verbose = 1):\n",
    "    \n",
    "        self.embedding_matrix = embedding_matrix.astype('float32')\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.num_error = num_error\n",
    "        self.num_sites = num_sites\n",
    "        self.num_classes = num_classes\n",
    "        self.max_senten_num = num_error * num_sites\n",
    "        self.cudnn = cudnn\n",
    "        self.attention = attention\n",
    "        self.word_encoder = word_encoder\n",
    "        self.encode_sites = encode_sites\n",
    "        self.batch_norm = batch_norm\n",
    "        self.include_counts = include_counts\n",
    "        self.avg_w2v = avg_w2v\n",
    "        self.verbose = verbose\n",
    "        # Hyperparameters\n",
    "        self.hp = {\n",
    "            # Regularization\n",
    "            'l2_regulizer': 0.0001,\n",
    "            'dropout':0.2,\n",
    "            # Conv1D\n",
    "            'filters':256,\n",
    "            'kernel_size':3,\n",
    "            'conv_layers':3,\n",
    "            'max_pooling':3,\n",
    "            'units_conv':10,\n",
    "            # RNN with optional attention\n",
    "            'train_embedding': True,\n",
    "            'att_units':10,\n",
    "            'rec_dropout':0.0,\n",
    "            'rnn': LSTM, #TRY GRU\n",
    "            'rnncud': CuDNNLSTM, # TRY CuDNNGRU\n",
    "            'rnn_units' : 10,\n",
    "            # Site encoding\n",
    "            'encode_sites': False,\n",
    "            'activation_site': 'relu', #TRY linear\n",
    "            'units_site': 10,\n",
    "            # Final layers\n",
    "            'dense_layers': 3,\n",
    "            'dense_units': 20,\n",
    "            'learning_rate':0.0001,\n",
    "            'decay':0.0\n",
    "                    }\n",
    "\n",
    "        \n",
    "        \n",
    "    def set_hyperparameters(self, tweaked_instances):\n",
    "\n",
    "        for  key, value in tweaked_instances.items():\n",
    "            if key in self.hp:\n",
    "                self.hp[key] = value\n",
    "            else:\n",
    "                raise KeyError(key + ' does not exist in hyperparameters')\n",
    "\n",
    "            \n",
    "    def print_hyperparameters(self):\n",
    "\n",
    "        print('Hyperparameter\\tCorresponding Value')\n",
    "        for key, value in self.hp.items():\n",
    "            print(key, '\\t\\t', value)\n",
    "        \n",
    "        \n",
    "    def get_embedding_layer( self ):\n",
    "        \n",
    "        dims_embed = self.embedding_matrix.shape\n",
    "        \"\"\"\n",
    "        if self.cudnn == True or self.word_encoder == 'Conv1D':\n",
    "            embedding = Embedding(dims_embed[0], dims_embed[1], weights=[self.embedding_matrix], \\\n",
    "                                  input_length = self.max_sequence_length, trainable = self.train_embedding)\n",
    "        else:\n",
    "        \"\"\"\n",
    "        \n",
    "        embedding = Embedding(dims_embed[0], dims_embed[1], weights=[self.embedding_matrix], \\\n",
    "                  input_length = None, mask_zero = True, trainable = int(self.hp['train_embedding']))\n",
    "    \n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def word_encoder_lstm( self ):\n",
    "        \n",
    "        #TODO add recurrent_dropout\n",
    "        \n",
    "        word_input = Input(shape = ( None, ), dtype='int32')\n",
    "        word_sequences = self.get_embedding_layer()(word_input)\n",
    "                \n",
    "        if self.attention == False:\n",
    "            if self.cudnn == True:\n",
    "                word_lstm = self.hp['rnncud'](int(self.hp['rnn_units']), \n",
    "                                              kernel_regularizer=l2(self.hp['l2_regulizer']))(word_sequences)\n",
    "            else:\n",
    "                word_lstm = self.hp['rnn'](int(self.hp['rnn_units']), kernel_regularizer=l2(self.hp['l2_regulizer']),\n",
    "                                          recurrent_dropout = self.hp['rec_dropout'])(word_sequences)\n",
    "            wordEncoder = Model(word_input, word_lstm)\n",
    "        else:\n",
    "            if self.cudnn == True:\n",
    "                word_lstm = self.hp['rnncud'](int(self.hp['rnn_units']), kernel_regularizer=l2(self.hp['l2_regulizer']),\n",
    "                                             return_sequences=True)(word_sequences)\n",
    "            else:\n",
    "                word_lstm = self.hp['rnn'](int(self.hp['rnn_units']), kernel_regularizer=l2(self.hp['l2_regulizer']),\n",
    "                                          recurrent_dropout = self.hp['rec_dropout'], return_sequences=True)(word_sequences)\n",
    "            word_dense = TimeDistributed(Dense(int(self.hp['att_units'])))(word_lstm)\n",
    "            word_att = AttentionWithContext()(word_dense)\n",
    "            wordEncoder = Model(word_input, word_att)\n",
    "        wordEncoder.summary()\n",
    "        return wordEncoder\n",
    "    \n",
    "\n",
    "    def word_encoder_conv( self ):\n",
    "        \n",
    "        #TODO add spatial dropout\n",
    "        \n",
    "        word_input = Input(shape = ( self.max_sequence_length, ), dtype='float32')\n",
    "        word_sequences = self.get_embedding_layer()(word_input)\n",
    "\n",
    "        for i in range(self.hp['conv_layers']):\n",
    "            word_sequences = Conv1D(self.hp['filters'], self.hp['kernel_size'], \n",
    "                                    activation='relu',kernel_regularizer=l2(self.hp['l2_regulizer']))(word_sequences)\n",
    "            word_sequences = MaxPooling1D(self.hp['max_pooling'])(word_sequences)\n",
    "\n",
    "        word_sequences = GlobalMaxPooling1D()(word_sequences)\n",
    "        word_sequences = Dense(self.hp['units_conv'], activation='relu',\n",
    "                               kernel_regularizer=l2(self.hp['l2_regulizer']))(word_sequences)\n",
    "        \n",
    "        wordEncoder = Model(word_input, word_sequences)\n",
    "\n",
    "        return wordEncoder\n",
    "    \n",
    "        \n",
    "    def create_model( self ):\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            self.print_hyperparameters()\n",
    "        \n",
    "        \n",
    "        # Input layers\n",
    "        #sent_input = Input(shape = (self.num_error, self.num_sites, None), dtype='int32')\n",
    "        \n",
    "        # Reshape the matrix\n",
    "        #sent_input_reshaped = Reshape(( self.num_error * self.num_sites, ))(sent_input)\n",
    "       \n",
    "        if self.avg_w2v == False:\n",
    "            \n",
    "            sent_input = Input(shape = (self.num_error * self.num_sites, None), dtype='int32')\n",
    "            \n",
    "            # Encode the words of the sentences\n",
    "            if self.word_encoder == 'LSTM':\n",
    "                encoder_units = int(self.hp['att_units'])\n",
    "                sent_encoder = TimeDistributed(self.word_encoder_lstm())(sent_input)\n",
    "            elif self.word_encoder == 'Conv1D':\n",
    "                encoder_units = self.hp['units_conv']\n",
    "                sent_encoder = TimeDistributed(self.word_encoder_conv())(sent_input_reshaped)\n",
    "            else: \n",
    "                print( 'No valid encoder' )    \n",
    "\n",
    "\n",
    "            \"\"\"    \n",
    "            sent_encoder = Dropout(self.hp['dropout'])(sent_encoder)\n",
    "            if self.batch_norm == True:\n",
    "                sent_encoder = BatchNormalization()(sent_encoder)\n",
    "            \"\"\"\n",
    "\n",
    "            # Reshape the error sites matrix\n",
    "\n",
    "            sent_encoder_reshaped = Reshape(( self.num_error , self.num_sites, encoder_units))(sent_encoder)\n",
    "         \n",
    "        else:\n",
    "            \n",
    "            sent_input = Input(shape = (self.num_error * self.num_sites, self.max_sequence_length), dtype='float32')\n",
    "            sent_encoder_reshaped = Reshape(( self.num_error , self.num_sites, self.max_sequence_length))(sent_input)\n",
    "            sent_encoder_reshaped = TimeDistributed(Dense(int(self.hp['units_site']), activation = self.hp['activation_site'], \n",
    "                      kernel_regularizer=l2(self.hp['l2_regulizer'])))(sent_encoder_reshaped)\n",
    "            encoder_units = int(self.hp['units_site'])\n",
    "        \n",
    "        # Add the meta information\n",
    "        if self.include_counts == True:\n",
    "            \n",
    "            count_input = Input(shape = (self.num_error, self.num_sites, 2, ), dtype='float32')\n",
    "            print( count_input )\n",
    "            # Merge the counts and words\n",
    "            exit_code_site_repr = Concatenate(axis=3)([sent_encoder_reshaped, count_input])\n",
    "            print( exit_code_site_repr )\n",
    "            exit_code_site_repr = Reshape(( self.num_error , self.num_sites * (encoder_units+2)))(exit_code_site_repr)\n",
    "            print( exit_code_site_repr )\n",
    "        else:\n",
    "            exit_code_site_repr = sent_encoder_reshaped\n",
    "            exit_code_site_repr = Reshape(( self.num_error , self.num_sites * (encoder_units)))(exit_code_site_repr)\n",
    "        \n",
    "        \n",
    "        # Encode the site\n",
    "        if int(self.hp['encode_sites']) == True:\n",
    "            \n",
    "            exit_code_encoder = TimeDistributed(Dense(int(self.hp['units_site']), activation = self.hp['activation_site'], \n",
    "                      kernel_regularizer=l2(self.hp['l2_regulizer'])))(exit_code_site_repr)\n",
    "        else:\n",
    "            exit_code_encoder = exit_code_site_repr\n",
    "\n",
    "            \"\"\"\n",
    "            exit_code_encoder = Dropout(self.hp['dropout'])(exit_code_encoder)\n",
    "            if self.batch_norm == True:\n",
    "                exit_code_encoder = BatchNormalization()(exit_code_encoder)\n",
    "            \"\"\"\n",
    "            \n",
    "        #exit_code_encoder = AveragePooling1D(pool_size = 2, data_format='channels_first')(exit_code_site_repr)        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Flatten\n",
    "        flattened = Flatten()(exit_code_encoder)\n",
    "            \n",
    "        # Dense\n",
    "        dense = flattened\n",
    "        for _ in range(int(self.hp['dense_layers'])):\n",
    "            \n",
    "            dense = Dense( units=int(self.hp['dense_units']), activation='relu', \n",
    "                          kernel_regularizer=l2(self.hp['l2_regulizer']) )(dense)\n",
    "            dense = Dropout(self.hp['dropout'])(dense)\n",
    "            if self.batch_norm == True:\n",
    "                dense = BatchNormalization()(dense)            \n",
    "            \n",
    "        # Output layer\n",
    "        preds = Dense(1, activation='sigmoid', kernel_regularizer=l2(self.hp['l2_regulizer']) )(dense)\n",
    "                \n",
    "        # Final model\n",
    "        if self.include_counts == False:\n",
    "            self.model = Model(sent_input, preds)\n",
    "        else:\n",
    "            self.model = Model([sent_input, count_input], preds)\n",
    "        self.model.compile( loss='binary_crossentropy', optimizer = Adam(lr = self.hp['learning_rate'], \n",
    "                                                                         decay = self.hp['decay']) )\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            self.model.summary()\n",
    "        \n",
    "        return self.model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the keras wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_sites = len(list(set(sites_index.values())))\n",
    "dim_errors = len(list(set(codes_index.values())))\n",
    "embedding_dim = 400\n",
    "embedding_matrix = np.load(e['NLP_PARAM']['embedding_matrix_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dense_units, learning_rate, dropout, rnn_units, dense_layers, att_units):\n",
    "    \n",
    "    nlp = NLP(2, dim_errors, dim_sites, embedding_dim, \n",
    "                                         embedding_matrix = embedding_matrix,\n",
    "                                         cudnn = e['NLP_PARAM']['cudnn'],\n",
    "                                         batch_norm = e['NLP_PARAM']['batch_norm'], \n",
    "                                         word_encoder = e['NLP_PARAM']['word_encoder'], \n",
    "                                         include_counts = e['NLP_PARAM']['include_counts'], \n",
    "                                         avg_w2v = e['NLP_PARAM']['avg_w2v'],\n",
    "                                         attention = e['NLP_PARAM']['attention'] ) \n",
    "    \n",
    "    model_param = {}\n",
    "    model_param['dense_units'] = dense_units\n",
    "    model_param['learning_rate'] = learning_rate\n",
    "    model_param['dropout'] = dropout\n",
    "    model_param['rnn_units'] = rnn_units\n",
    "    model_param['dense_layers'] = dense_layers\n",
    "    model_param['att_units'] = att_units\n",
    "    nlp.set_hyperparameters(model_param)\n",
    "    model = nlp.create_model()\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter\tCorresponding Value\n",
      "('units_conv', '\\t\\t', 10)\n",
      "('rnncud', '\\t\\t', <class 'keras.layers.cudnn_recurrent.CuDNNLSTM'>)\n",
      "('l2_regulizer', '\\t\\t', 0.0001)\n",
      "('encode_sites', '\\t\\t', False)\n",
      "('learning_rate', '\\t\\t', 0.01)\n",
      "('rnn', '\\t\\t', <class 'keras.layers.recurrent.LSTM'>)\n",
      "('decay', '\\t\\t', 0.0)\n",
      "('dropout', '\\t\\t', 0.1)\n",
      "('units_site', '\\t\\t', 10)\n",
      "('dense_units', '\\t\\t', 10)\n",
      "('max_pooling', '\\t\\t', 3)\n",
      "('att_units', '\\t\\t', 20)\n",
      "('rec_dropout', '\\t\\t', 0.0)\n",
      "('dense_layers', '\\t\\t', 3)\n",
      "('filters', '\\t\\t', 256)\n",
      "('train_embedding', '\\t\\t', True)\n",
      "('conv_layers', '\\t\\t', 3)\n",
      "('activation_site', '\\t\\t', 'relu')\n",
      "('kernel_size', '\\t\\t', 3)\n",
      "('rnn_units', '\\t\\t', 10)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 20)          613480    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 10)          1240      \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, None, 20)          220       \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 20)                440       \n",
      "=================================================================\n",
      "Total params: 615,380\n",
      "Trainable params: 615,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Tensor(\"input_6:0\", shape=(?, 77, 81, 2), dtype=float32)\n",
      "Tensor(\"concatenate_2/concat:0\", shape=(?, 77, 81, 22), dtype=float32)\n",
      "Tensor(\"reshape_4/Reshape:0\", shape=(?, 77, 1782), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 6237, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 6237, 20)     615380      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 77, 81, 20)   0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 77, 81, 2)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 77, 81, 22)   0           reshape_3[0][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 77, 1782)     0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 137214)       0           reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           1372150     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 10)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 10)           110         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 10)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 10)           110         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 10)           0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            11          dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,987,761\n",
      "Trainable params: 1,987,761\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(10, 0.01, 0.1, 10, 3, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "class InputBatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, frame, label, codes, sites, pad_dim, batch_size = 1, max_msg = 5, \n",
    "                 averaged = False, sequence = False, only_msg = False, cut_front = True):\n",
    "        \n",
    "        self.frame = frame\n",
    "        self.n_tasks = len(frame)\n",
    "        self.label = label\n",
    "        self.batch_size = batch_size\n",
    "        self.codes = codes\n",
    "        self.sites = sites\n",
    "        self.pad_dim = pad_dim\n",
    "        if sequence == False:\n",
    "            self.max_msg = 1\n",
    "        else:\n",
    "            self.max_msg = max_msg\n",
    "        self.averaged = averaged\n",
    "        self.sequence = sequence\n",
    "        self.cut_front = cut_front\n",
    "        self.only_msg = only_msg\n",
    "        self.unique_sites = len(list(set(self.sites.values())))\n",
    "        self.unique_codes = len(list(set(self.codes.values())))\n",
    "        self.n_tasks = len(frame)\n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "    def pad_along_axis(self, array, axis=0):\n",
    "\n",
    "        array = np.array(array)\n",
    "        pad_size = self.pad_dim - array.shape[axis]\n",
    "        axis_nb = len(array.shape)\n",
    "\n",
    "        if pad_size < 0:\n",
    "            if self.cut_front == True:\n",
    "                return array[-self.pad_dim : ]\n",
    "            else:\n",
    "                return array[ : self.pad_dim ]\n",
    "\n",
    "        npad = [(0, 0) for x in range(axis_nb)]\n",
    "        npad[axis] = (0, pad_size)\n",
    "\n",
    "        b = np.pad(array, pad_width=npad, mode='constant', constant_values=int(0))\n",
    "\n",
    "        return b\n",
    "    \n",
    "    \n",
    "    def fill_counts(self, index, error, site, site_state, count):\n",
    "        \n",
    "        # Encode good and bad sites\n",
    "        if site_state == 'good':\n",
    "            site_state_encoded = 0\n",
    "        else:\n",
    "            site_state_encoded = 1\n",
    "\n",
    "        self.error_site_counts[index, self.codes[error], self.sites[site], site_state_encoded] += count\n",
    "    \n",
    "    \n",
    "    def fill_first_message(self, index, error, site, error_message):\n",
    "        \n",
    "                               \n",
    "        # Pad the error message\n",
    "        if self.averaged == False:\n",
    "            error_message = self.pad_along_axis(error_message)\n",
    "        #print( error_message )\n",
    "        self.error_site_tokens[index, self.codes[error], self.sites[site]] = error_message\n",
    "\n",
    "    \n",
    " \n",
    "    def fill_messages_sequence(self, index, error, site, error_message_sequence):\n",
    "        \n",
    "        # Loop over the error message sequence\n",
    "        for counter, error_message in enumerate(error_message_sequence):\n",
    "           \n",
    "            # Stop when maximal message is reached\n",
    "            if counter == self.max_msg:\n",
    "                break           \n",
    "            \n",
    "            # Pad the error message\n",
    "            if self.averaged == False:\n",
    "                error_message = self.pad_along_axis(error_message)\n",
    "                \n",
    "            \n",
    "            # Sequence per task, error, site\n",
    "            self.error_site_tokens[index, self.codes[error], self.sites[site], counter ] = error_message    \n",
    "            \n",
    "    \n",
    "    def to_dense(self, index_matrix, values):\n",
    "        \n",
    "        errors, sites, counts, site_states, error_messages = values\n",
    "        \n",
    "        # Loop over the codes and sites\n",
    "        for i_key in range(len(counts)):\n",
    "            \n",
    "            error = errors[i_key]\n",
    "            site = sites[i_key]\n",
    "            count = counts[i_key]\n",
    "            site_state = site_states[i_key]\n",
    "    \n",
    "            \n",
    "            # Fill the counts\n",
    "            if self.only_msg == False:\n",
    "                self.fill_counts(index_matrix, error, site, site_state, count)\n",
    "           \n",
    "            if self.only_counts == True:\n",
    "                continue\n",
    "            \n",
    "            error_message_sequence = error_messages[i_key]\n",
    "            \n",
    "            # Only continue if there exists a message\n",
    "            if isinstance(error_message_sequence, (list,)):\n",
    "                \n",
    "                # Fill the error message\n",
    "                if self.sequence == True:\n",
    "                    self.fill_messages_sequence( index_matrix, error, site, error_message_sequence)\n",
    "                else:\n",
    "                    self.fill_first_message( index_matrix, error, site, error_message_sequence)\n",
    "                    \n",
    "\n",
    "                \n",
    "    def get_counts_matrix(self, sum_good_bad = False):\n",
    "        \n",
    "        self.only_counts = True\n",
    "        \n",
    "        self.error_site_counts = np.zeros((self.n_tasks, self.unique_codes, self.unique_sites, 2), dtype=np.int32)\n",
    "        batch = self.frame\n",
    "        [self.to_dense(counter, values) for counter, values in enumerate(zip(self.frame['error'], self.frame['site'], \n",
    "                                                                             self.frame['count'], self.frame['site_state'],\n",
    "                                                                             self.frame['msg_encoded'],))]        \n",
    "        if sum_good_bad == True:\n",
    "            return self.error_site_counts.sum(axis=3), self.frame[self.label].values\n",
    "        else:\n",
    "            return self.error_site_counts, self.frame[self.label].values        \n",
    "    \n",
    "    \n",
    "    def msg_batch(self, start_pos, end_pos):\n",
    "        \n",
    "        self.only_counts = False\n",
    "        \n",
    "        # Batch of frame\n",
    "        batch = self.frame.iloc[start_pos : end_pos]\n",
    "        chunk_size = len(batch)\n",
    "        \n",
    "        # Tokens\n",
    "        if self.averaged == False:\n",
    "            tokens_key = 'msg_encoded'\n",
    "            self.pad_dim = 1\n",
    "            msg_t = []\n",
    "            for key in batch[tokens_key]:\n",
    "                for msg in key:\n",
    "                    if isinstance(msg, (list,)):\n",
    "                        if len(msg) > self.pad_dim:\n",
    "                            msg_t = msg\n",
    "                            self.pad_dim = len(msg)\n",
    "                        \n",
    "            if self.pad_dim > 300:\n",
    "                self.pad_dim = 300\n",
    "        else:\n",
    "            tokens_key = 'avg'\n",
    "       \n",
    "        \n",
    "        #print( self.pad_dim )\n",
    "        #print( msg )\n",
    "        \n",
    "        # Error site matrix\n",
    "        self.error_site_counts = np.zeros((chunk_size, self.unique_codes, self.unique_sites, 2), dtype=np.int32)\n",
    "        \n",
    "        if self.sequence == True:\n",
    "            dim = (chunk_size, self.unique_codes, self.unique_sites, self.max_msg, self.pad_dim)\n",
    "        else:\n",
    "            dim = (chunk_size, self.unique_codes, self.unique_sites, self.pad_dim)    \n",
    "        \n",
    "        \n",
    "        # Error message matrix\n",
    "        self.error_site_tokens = np.zeros(dim, dtype=np.int32)\n",
    "        \n",
    "        [self.to_dense(counter, values) for counter, values in enumerate(zip(batch['error'], batch['site'], batch['count'],\n",
    "                                                                          batch['site_state'], batch[tokens_key]))]\n",
    "        \n",
    "        if self.only_msg == False:\n",
    "            #self.error_site_tokens = np.reshape(\n",
    "            #print self.error_site_tokens.shape\n",
    "            return [self.error_site_tokens.reshape((chunk_size, self.unique_codes * self.unique_sites, self.pad_dim)) , self.error_site_counts]   \n",
    "        else:\n",
    "            return self.error_site_tokens\n",
    "    \n",
    "    \n",
    "    def gen_batches(self):\n",
    "        \n",
    "        for cur_pos in range(0, self.n_tasks, self.batch_size):\n",
    " \n",
    "            next_pos = cur_pos + self.batch_size \n",
    "            if next_pos <= self.n_tasks:\n",
    "                yield (self.msg_batch( cur_pos, next_pos ), self.frame[self.label].iloc[cur_pos : next_pos].values)\n",
    "            else:\n",
    "                yield (self.msg_batch( cur_pos, self.n_tasks ), self.frame[self.label].iloc[cur_pos : self.n_tasks].values)   \n",
    "                  \n",
    "                    \n",
    "    def gen_inf_batches(self):\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                for B in self.gen_batches():\n",
    "                    yield B\n",
    "            except StopIteration:\n",
    "                logging.warning(\"start over generator loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "class KerasClassifierCustom(KerasClassifier):\n",
    "\n",
    "  \n",
    "    \n",
    "    def fit(self, x, y, **kwargs):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if self.build_fn is None:\n",
    "            self.model = self.__call__(**self.filter_sk_params(self.__call__))\n",
    "        elif not isinstance(self.build_fn, types.FunctionType):\n",
    "            self.model = self.build_fn(\n",
    "                **self.filter_sk_params(self.build_fn.__call__))\n",
    "        else:\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
    "\n",
    "        \"\"\"\n",
    "        self.model = self.build_fn(\n",
    "            **self.filter_sk_params(self.build_fn))    \n",
    "                \n",
    "        \"\"\"\n",
    "        self.model = self.build_fn()\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit_generator))\n",
    "        fit_args.update(kwargs)\n",
    "        \n",
    "        print( fit_args )\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = 1\n",
    "        self.classes_ = np.unique(y)\n",
    "        steps_per_epoch = int(len(x) / batch_size)\n",
    "\n",
    "        generator_train = InputBatchGenerator(x, 'label', codes_index, sites_index,\n",
    "                                                                    400, batch_size = batch_size)   \n",
    "    \n",
    "        self.model.fit_generator(generator = generator_train.gen_inf_batches(), steps_per_epoch = steps_per_epoch, \n",
    "                                  epochs = 12, workers=0)        \n",
    "        \n",
    "        \n",
    "        \n",
    "  \n",
    "    def predict_proba(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        preds = self.model.predict_generator(\n",
    "                    self.get_batch(x, None, self.sk_params[\"batch_size\"]), \n",
    "                                               val_samples=x.shape[0])\n",
    "        return preds\n",
    "        \"\"\"\n",
    "        generator_test = InputBatchGenerator(x, 'label', codes_index, sites_index,\n",
    "                                                            400, batch_size = 1)  \n",
    "        \n",
    "        y_pred_batches = []\n",
    "        for X,y in generator_test.gen_batches():\n",
    "            y_pred_batches.append(np.asarray(self.model.predict(X)))\n",
    "\n",
    "        y_pred = np.concatenate(y_pred_batches)  \n",
    "        y_pred_sk = [ [1-pred, pred] for pred in y_pred ]\n",
    "        \n",
    "        return np.array(y_pred_sk)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KerasClassifierCustom(build_fn=build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "majority_class_sampled = actionshist[actionshist['label'] == 0].sample(n_samples , random_state=42)\n",
    "minority_class_sampled = actionshist[actionshist['label'] == 1].sample(n_samples , random_state=42)\n",
    "t = pd.concat([minority_class_sampled, majority_class_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "t = t.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "class log_uniform():        \n",
    "    def __init__(self, a=-1, b=0, base=10):\n",
    "        self.loc = a\n",
    "        self.scale = b - a\n",
    "        self.base = base\n",
    "\n",
    "    def rvs(self, size=1, random_state=None):\n",
    "        uniform = sp.stats.uniform(loc=self.loc, scale=self.scale)\n",
    "        return np.power(self.base, uniform.rvs(size=size, random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "param_dist={\n",
    "    'dense_units': sp_randint(10, 100),\n",
    "    'learning_rate': [1e-5, 1e-4, 1e-3],\n",
    "    'dropout' : [1e-3, 1e-2, 0.1],\n",
    "    'rnn_units': sp_randint(2, 20),\n",
    "    'dense_layers': sp_randint(1, 5),\n",
    "    'att_units': sp_randint(2, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter\tCorresponding Value\n",
      "('units_conv', '\\t\\t', 10)\n",
      "('rnncud', '\\t\\t', <class 'keras.layers.cudnn_recurrent.CuDNNLSTM'>)\n",
      "('l2_regulizer', '\\t\\t', 0.0001)\n",
      "('encode_sites', '\\t\\t', False)\n",
      "('learning_rate', '\\t\\t', 1e-05)\n",
      "('rnn', '\\t\\t', <class 'keras.layers.recurrent.LSTM'>)\n",
      "('decay', '\\t\\t', 0.0)\n",
      "('dropout', '\\t\\t', 0.1)\n",
      "('units_site', '\\t\\t', 10)\n",
      "('dense_units', '\\t\\t', 72)\n",
      "('max_pooling', '\\t\\t', 3)\n",
      "('att_units', '\\t\\t', 10)\n",
      "('rec_dropout', '\\t\\t', 0.0)\n",
      "('dense_layers', '\\t\\t', 4)\n",
      "('filters', '\\t\\t', 256)\n",
      "('train_embedding', '\\t\\t', False)\n",
      "('conv_layers', '\\t\\t', 3)\n",
      "('activation_site', '\\t\\t', 'relu')\n",
      "('kernel_size', '\\t\\t', 3)\n",
      "('rnn_units', '\\t\\t', 16)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, None, 20)          613480    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 16)                2368      \n",
      "=================================================================\n",
      "Total params: 615,848\n",
      "Trainable params: 2,368\n",
      "Non-trainable params: 613,480\n",
      "_________________________________________________________________\n",
      "Tensor(\"input_18:0\", shape=(?, 77, 81, 2), dtype=float32)\n",
      "Tensor(\"concatenate_6/concat:0\", shape=(?, 77, 81, 18), dtype=float32)\n",
      "Tensor(\"reshape_12/Reshape:0\", shape=(?, 77, 1458), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 6237, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 6237, 16)     615848      input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 77, 81, 16)   0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 77, 81, 2)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 77, 81, 18)   0           reshape_11[0][0]                 \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 77, 1458)     0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 112266)       0           reshape_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 72)           8083224     flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 72)           0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 72)           5256        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 72)           0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 72)           5256        dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 72)           0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 72)           5256        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 72)           0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1)            73          dropout_19[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 8,714,913\n",
      "Trainable params: 8,101,433\n",
      "Non-trainable params: 613,480\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-698f88bdcda1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mn_iter_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvalidator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter_search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvalidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1514\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-40eb996bb113>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         self.model.fit_generator(generator = generator_train.gen_inf_batches(), steps_per_epoch = steps_per_epoch, \n\u001b[0;32m---> 43\u001b[0;31m                                   epochs = 1, workers=0)        \n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft-nightlies.cern.ch/lcg/views/dev3/Fri/x86_64-centos7-gcc8-opt/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "validator = GridSearchCV(clf,\n",
    "                         param_grid={'dense_units': [20, 50]},\n",
    "                         scoring='neg_log_loss',\n",
    "                         n_jobs=1, cv=2)\n",
    "\"\"\"\n",
    "n_iter_search = 2\n",
    "validator = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=2)\n",
    "validator.fit(t, t['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_sklearn import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def scorer(estimator, X, y):\n",
    "    \n",
    "    y_pred = estimator.predict_proba(X)[:,1]\n",
    "    score = roc_auc_score(X['label'], y_pred)\n",
    "    \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "      <th>site</th>\n",
       "      <th>site_state</th>\n",
       "      <th>count</th>\n",
       "      <th>msg_encoded</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T1_US_FNAL_Disk, T3_US_FNALLPC]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[T3_US_FNALLPC, T1_US_FNAL_Disk]</td>\n",
       "      <td>[bad, bad]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-1, -1, -1, -1, 85]</td>\n",
       "      <td>[T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...</td>\n",
       "      <td>[bad, bad, bad, bad, good]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[50664, -1, -1]</td>\n",
       "      <td>[T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]</td>\n",
       "      <td>[good, good, good]</td>\n",
       "      <td>[2, 1, 1]</td>\n",
       "      <td>[[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  error                                               site  \\\n",
       "0              [-1, -1]                   [T1_US_FNAL_Disk, T3_US_FNALLPC]   \n",
       "1              [-1, -1]                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "2              [-1, -1]                   [T3_US_FNALLPC, T1_US_FNAL_Disk]   \n",
       "3  [-1, -1, -1, -1, 85]  [T1_DE_KIT_Disk, T0_CH_CERN_MSS, T0_CH_CERN_Ex...   \n",
       "4       [50664, -1, -1]               [T2_DE_RWTH, T2_DE_RWTH, T2_CH_CERN]   \n",
       "\n",
       "                   site_state            count  \\\n",
       "0                  [bad, bad]           [1, 1]   \n",
       "1                  [bad, bad]           [1, 1]   \n",
       "2                  [bad, bad]           [1, 1]   \n",
       "3  [bad, bad, bad, bad, good]  [1, 1, 1, 1, 1]   \n",
       "4          [good, good, good]        [2, 1, 1]   \n",
       "\n",
       "                                         msg_encoded  label  \n",
       "0                                         [nan, nan]      0  \n",
       "1                                         [nan, nan]      0  \n",
       "2                                         [nan, nan]      0  \n",
       "3  [nan, nan, nan, nan, [35, 12, 10, 37, 186, 34,...      0  \n",
       "4  [[53, 2, 74, 141, 129, 198, 10, 200, 4, 32, 42...      0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionshist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-928a18bb0467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'dense_units'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgridSearch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactionshist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactionshist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_iter'"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(sc, estimator=clf, param_grid={'dense_units': [20, 50]}, scoring=scorer, verbose=1)\n",
    "gridSearch_result = grid.fit(actionshist, actionshist['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "grid = RandomizedSearchCV(sc, estimator=clf, param_distributions=param_dist, n_iter = 20, scoring=scorer, verbose=1)\n",
    "gridSearch_result = grid.fit(actionshist, actionshist['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch_result.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(gridSearch_result.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(gridSearch_result.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_hdf('results.h5', 'frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actionshist', 95763168),\n",
       " ('embedding_matrix', 12269712),\n",
       " ('t', 118820),\n",
       " ('minority_class_sampled', 70872),\n",
       " ('majority_class_sampled', 47980),\n",
       " ('sites', 12638),\n",
       " ('codes', 7672),\n",
       " ('codes_index', 6424),\n",
       " ('sites_index', 6424),\n",
       " ('e', 1048),\n",
       " ('res', 912),\n",
       " ('Adam', 904),\n",
       " ('BatchNormalization', 904),\n",
       " ('Bidirectional', 904),\n",
       " ('Concatenate', 904),\n",
       " ('Conv1D', 904),\n",
       " ('CuDNNGRU', 904),\n",
       " ('CuDNNLSTM', 904),\n",
       " ('Dense', 904),\n",
       " ('Dropout', 904),\n",
       " ('Embedding', 904),\n",
       " ('Flatten', 904),\n",
       " ('GRU', 904),\n",
       " ('GlobalMaxPooling1D', 904),\n",
       " ('GridSearchCV', 904),\n",
       " ('InputBatchGenerator', 904),\n",
       " ('KerasClassifier', 904),\n",
       " ('KerasClassifierCustom', 904),\n",
       " ('LSTM', 904),\n",
       " ('Lambda', 904),\n",
       " ('MaxPooling1D', 904),\n",
       " ('Model', 904),\n",
       " ('Reshape', 904),\n",
       " ('TimeDistributed', 904),\n",
       " ('GEN_PARAM', 280),\n",
       " ('Input', 120),\n",
       " ('average', 120),\n",
       " ('build_model', 120),\n",
       " ('l2', 120),\n",
       " ('load_data', 120),\n",
       " ('prune_to_index', 120),\n",
       " ('roc_auc_score', 120),\n",
       " ('scorer', 120),\n",
       " ('to_categorical', 120),\n",
       " ('to_index', 120),\n",
       " ('NLP', 104),\n",
       " ('path', 102),\n",
       " ('EXPERIMENTS', 80),\n",
       " ('clf', 64),\n",
       " ('grid', 64),\n",
       " ('gridSearch_result', 64),\n",
       " ('model', 64),\n",
       " ('sc', 64),\n",
       " ('spark', 64),\n",
       " ('swan_spark_conf', 64),\n",
       " ('test', 64),\n",
       " ('K', 56),\n",
       " ('np', 56),\n",
       " ('pd', 56),\n",
       " ('MODEL', 44),\n",
       " ('PRUNING', 40),\n",
       " ('AVG_W2V', 24),\n",
       " ('MAX_WORDS', 24),\n",
       " ('MSG_ONLY', 24),\n",
       " ('SAMPLE', 24),\n",
       " ('SAMPLE_FACT', 24),\n",
       " ('TRAIN_ON_BATCH', 24),\n",
       " ('dim_errors', 24),\n",
       " ('dim_sites', 24),\n",
       " ('embedding_dim', 24),\n",
       " ('n_samples', 24)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') \n",
    "        and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.executor.instances",
     "value": "60"
    },
    {
     "name": "spark.executor.memory",
     "value": "12g"
    },
    {
     "name": "spark.executor.cores",
     "value": "3"
    },
    {
     "name": "spark.kubernetes.executor.request.cores",
     "value": "3"
    },
    {
     "name": "spark.dynamicAllocation.enabled",
     "value": "false"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
