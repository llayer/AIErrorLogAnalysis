{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import InputGenerator\n",
    "import base_model\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import baseline_model\n",
    "import importlib\n",
    "import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import InputBatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'InputGenerator' from '/nfshome/llayer/AIErrorLogAnalysis/training/InputGenerator.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(InputGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'InputBatchGenerator' from '/nfshome/llayer/AIErrorLogAnalysis/training/InputBatchGenerator.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(InputBatchGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = InputGenerator.InputGenerator('/nfshome/llayer/data/actionshistory_300719.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.set_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>errors</th>\n",
       "      <th>parameters</th>\n",
       "      <th>action</th>\n",
       "      <th>action_binary_encoded</th>\n",
       "      <th>splitting</th>\n",
       "      <th>splitting_encoded</th>\n",
       "      <th>xrootd</th>\n",
       "      <th>xrootd_encoded</th>\n",
       "      <th>memory</th>\n",
       "      <th>memory_encoded</th>\n",
       "      <th>action_encoded</th>\n",
       "      <th>action_split</th>\n",
       "      <th>action_split_encoded</th>\n",
       "      <th>action_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>{'good_sites': {}, 'bad_sites': {'-1': {'T3_US...</td>\n",
       "      <td>{'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...</td>\n",
       "      <td>acdc</td>\n",
       "      <td>0</td>\n",
       "      <td>1x</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>acdc_1x</td>\n",
       "      <td>2</td>\n",
       "      <td>acdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>{'good_sites': {}, 'bad_sites': {'-1': {'T3_US...</td>\n",
       "      <td>{'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...</td>\n",
       "      <td>acdc</td>\n",
       "      <td>0</td>\n",
       "      <td>1x</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>acdc_1x</td>\n",
       "      <td>2</td>\n",
       "      <td>acdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>{'good_sites': {}, 'bad_sites': {'-1': {'T3_US...</td>\n",
       "      <td>{'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...</td>\n",
       "      <td>acdc</td>\n",
       "      <td>0</td>\n",
       "      <td>1x</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>acdc_1x</td>\n",
       "      <td>2</td>\n",
       "      <td>acdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...</td>\n",
       "      <td>{'good_sites': {'85': {'T1_UK_RAL': 1}}, 'bad_...</td>\n",
       "      <td>{'action': 'acdc', 'cores': '', 'xrootd': 'ena...</td>\n",
       "      <td>acdc</td>\n",
       "      <td>0</td>\n",
       "      <td>1x</td>\n",
       "      <td>0</td>\n",
       "      <td>enabled</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>acdc_1x</td>\n",
       "      <td>2</td>\n",
       "      <td>acdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...</td>\n",
       "      <td>{'good_sites': {'50664': {'T2_DE_RWTH': 2}, '-...</td>\n",
       "      <td>{'action': 'acdc', 'cores': '', 'sites': ['T2_...</td>\n",
       "      <td>acdc</td>\n",
       "      <td>0</td>\n",
       "      <td>1x</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>acdc_1x</td>\n",
       "      <td>2</td>\n",
       "      <td>acdc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           task_name  \\\n",
       "0  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...   \n",
       "1  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...   \n",
       "2  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...   \n",
       "3  /amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...   \n",
       "4  /amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...   \n",
       "\n",
       "                                              errors  \\\n",
       "0  {'good_sites': {}, 'bad_sites': {'-1': {'T3_US...   \n",
       "1  {'good_sites': {}, 'bad_sites': {'-1': {'T3_US...   \n",
       "2  {'good_sites': {}, 'bad_sites': {'-1': {'T3_US...   \n",
       "3  {'good_sites': {'85': {'T1_UK_RAL': 1}}, 'bad_...   \n",
       "4  {'good_sites': {'50664': {'T2_DE_RWTH': 2}, '-...   \n",
       "\n",
       "                                          parameters action  \\\n",
       "0  {'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...   acdc   \n",
       "1  {'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...   acdc   \n",
       "2  {'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...   acdc   \n",
       "3  {'action': 'acdc', 'cores': '', 'xrootd': 'ena...   acdc   \n",
       "4  {'action': 'acdc', 'cores': '', 'sites': ['T2_...   acdc   \n",
       "\n",
       "   action_binary_encoded splitting  splitting_encoded   xrootd  \\\n",
       "0                      0        1x                  0      NaN   \n",
       "1                      0        1x                  0      NaN   \n",
       "2                      0        1x                  0      NaN   \n",
       "3                      0        1x                  0  enabled   \n",
       "4                      0        1x                  0      NaN   \n",
       "\n",
       "   xrootd_encoded memory  memory_encoded  action_encoded action_split  \\\n",
       "0               2                      3               0      acdc_1x   \n",
       "1               2                      3               0      acdc_1x   \n",
       "2               2                      3               0      acdc_1x   \n",
       "3               0                      3               0      acdc_1x   \n",
       "4               2                      3               0      acdc_1x   \n",
       "\n",
       "   action_split_encoded action_binary  \n",
       "0                     2          acdc  \n",
       "1                     2          acdc  \n",
       "2                     2          acdc  \n",
       "3                     2          acdc  \n",
       "4                     2          acdc  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.actionshistory.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.set_padded_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33586, 77, 154, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.get_input_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'test_generator' from '/nfshome/llayer/AIErrorLogAnalysis/training/test_generator.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.test_count_matrix(gen, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nlp_model' from '/nfshome/llayer/AIErrorLogAnalysis/training/nlp_model.py'>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(nlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(base_model)\n",
    "importlib.reload(baseline_model)\n",
    "importlib.reload(nlp_model) \n",
    "importlib.reload(InputBatchGenerator)\n",
    "\n",
    "class FitHandler(object):\n",
    "    \n",
    "    def __init__(self, gen):\n",
    "        \n",
    "        self.gen = gen\n",
    "        self.dim_tasks, self.dim_errors, self.dim_sites, self.dim_msg = gen.get_input_shape()\n",
    "        self.sites = gen.sites_to_tiers(gen.sites) # gen.sites\n",
    "        self.codes = gen.codes\n",
    "        #print (self.sites)\n",
    "        \n",
    "        \n",
    "    def print_sample_summary(self, frame):\n",
    "        \n",
    "        print(frame.value_counts())\n",
    "        \n",
    "    \n",
    "    def split_frame(self, label, split_level):\n",
    "        \n",
    "        train, test, _, _ = train_test_split(gen.actionshistory, gen.actionshistory[label], test_size=split_level)\n",
    "        return train, test\n",
    "        \n",
    "    def k_fold_indices(self, kfold_function=KFold, kfold_splits=5, verbose=0):\n",
    "        \n",
    "        enum = enumerate(kfold_function(n_splits=kfold_splits, shuffle=True, random_state=seed).split(X,Y))\n",
    "        if verbose != 0:\n",
    "            enum = tqdm(enum, total=kfold_splits, desc='kfold', leave=False, initial=0)\n",
    "        return enum\n",
    "        \n",
    "    def fit_base_model(self, label, batch_size = 100, test_split=0.2, val_split=0.2, verbose=0):\n",
    "        \n",
    "        \n",
    "        # Split in train and test frames\n",
    "        train, test = self.split_frame(label, test_split)\n",
    "        \n",
    "        #self.print_sample_summary(train[label]) \n",
    "        #self.print_sample_summary(test[label]) \n",
    "        \n",
    "        # Setup the generator of the input batches\n",
    "        train_gen = InputBatchGenerator.InputBatchGenerator(train, label, self.codes, self.sites, self.dim_msg)\n",
    "        test_gen = InputBatchGenerator.InputBatchGenerator(test, label, self.codes, self.sites, self.dim_msg)        \n",
    "        X_train, y_train = train_gen.count_matrix(sum_good_bad = True)\n",
    "        X_test, y_test = test_gen.count_matrix(sum_good_bad = True)\n",
    "        \n",
    "        print(type(y_test))\n",
    "        \n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        #print(np.unique(y_train), return_counts = True)\n",
    "        #print(np.unique(y_test), return_counts = True)\n",
    "        \n",
    "        uniqueValues, occurCount = np.unique(y_test, return_counts=True)\n",
    "        print(uniqueValues, occurCount)\n",
    "        # Set the baseline model\n",
    "        n_sites = len(list(set(self.sites.values())))\n",
    "        model = baseline_model.FF(2, self.dim_errors, n_sites)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #history = model.train(X_train, y_train, X_test, y_test, max_epochs = 200, batch_size = 100, early_stopping = True)\n",
    "        \n",
    "        #return history\n",
    "        \n",
    "        \n",
    "        model.find_optimal_parameters(X_train, y_train, X_test, y_test, max_epochs = 100, batch_size = 100, num_calls=20)\n",
    "\n",
    "        \n",
    "    def fit_nlp_model(self, label, batch_size = 1, test_split=0.2, val_split=0.2, verbose=1):\n",
    "\n",
    "        # Set the baseline model\n",
    "        model = nlp_model.NLP_Model(2, self.dim_errors, self.dim_sites, self.dim_msg)            \n",
    "        #model.print_summary()\n",
    "        \n",
    "        # Split in train and test frames\n",
    "        train, test = self.split_frame(label, test_split)\n",
    "        \n",
    "        # Setup the generator of the input batches\n",
    "        train_gen = InputBatchGenerator.InputBatchGenerator(train, label, \\\n",
    "                                                            batch_size, self.codes, self.sites, self.dim_msg)\n",
    "        test_gen = InputBatchGenerator.InputBatchGenerator(test, label, \\\n",
    "                                                            batch_size, self.codes, self.sites, self.dim_msg) \n",
    "        \n",
    "        \"\"\"\n",
    "        for x,y in train_gen.gen_inf_count_msg_batches():\n",
    "            print( x[0].shape )\n",
    "        \"\"\"\n",
    "        \n",
    "        steps_per_epoch = int(float(len(train)) / float(batch_size))\n",
    "        validation_steps = int(float(len(test)) / float(batch_size))\n",
    "        \n",
    "        model.train_on_batch(training_generator = train_gen, validation_generator = test_gen, \n",
    "                             epochs = 1, steps_per_epoch = steps_per_epoch, validation_steps = validation_steps )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2\n",
      "T1\n",
      "T0\n",
      "T2\n",
      "Un\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T2\n",
      "T1\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T1\n",
      "T3\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T2\n",
      "T1\n",
      "T3\n",
      "T0\n",
      "T2\n",
      "T2\n",
      "T2\n",
      "T3\n",
      "T1\n",
      "T3\n",
      "T1\n",
      "T3\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T1\n",
      "T3\n",
      "T3\n",
      "T1\n",
      "T2\n",
      "T2\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T1\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T2\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T2\n",
      "T2\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T3\n",
      "T1\n",
      "T3\n",
      "T1\n",
      "T2\n",
      "T2\n",
      "T1\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T2\n",
      "T2\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T3\n",
      "T0\n",
      "T0\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T2\n",
      "T1\n",
      "No\n",
      "T3\n",
      "T2\n",
      "T2\n",
      "T1\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T3\n",
      "T0\n",
      "T2\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T1\n",
      "T1\n",
      "T3\n",
      "nu\n",
      "T2\n",
      "T2\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T1\n",
      "T3\n",
      "T3\n",
      "T3\n",
      "T1\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T3\n",
      "T2\n",
      "T2\n",
      "T2\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T2\n",
      "T3\n",
      "T2\n",
      "T2\n",
      "T1\n",
      "T3\n",
      "T3\n",
      "T3\n"
     ]
    }
   ],
   "source": [
    "handler = FitHandler(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshome/llayer/AIErrorLogAnalysis/training/InputBatchGenerator.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.frame['unique_index'] = self.frame.reset_index().index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(26868, 77, 5)\n",
      "(6718, 77, 5)\n",
      "[0 1] [6359  359]\n",
      "\n",
      " \t ::: 1 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_109 (InputLayer)       (None, 77, 5)             0         \n",
      "_________________________________________________________________\n",
      "flatten_109 (Flatten)        (None, 385)               0         \n",
      "_________________________________________________________________\n",
      "dense_537 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dropout_429 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_538 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_430 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_539 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_431 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_540 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 8,581\n",
      "Trainable params: 8,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 5)\n",
      "(6718, 77, 5)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/100\n",
      "26868/26868 [==============================] - 16s 597us/step - loss: 0.2603 - acc: 0.9483 - val_loss: 0.2303 - val_acc: 0.9466\n",
      "Epoch 2/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2151 - acc: 0.9483 - val_loss: 0.2138 - val_acc: 0.9466\n",
      "Epoch 3/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2109 - acc: 0.9483 - val_loss: 0.2197 - val_acc: 0.9466\n",
      "Epoch 4/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2097 - acc: 0.9483 - val_loss: 0.2111 - val_acc: 0.9466\n",
      "Epoch 5/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2073 - acc: 0.9483 - val_loss: 0.2108 - val_acc: 0.9466\n",
      "Epoch 6/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2062 - acc: 0.9483 - val_loss: 0.2100 - val_acc: 0.9466\n",
      "Epoch 7/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2056 - acc: 0.9483 - val_loss: 0.2101 - val_acc: 0.9466\n",
      "Epoch 8/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2051 - acc: 0.9483 - val_loss: 0.2101 - val_acc: 0.9466\n",
      "Epoch 9/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2060 - acc: 0.9483 - val_loss: 0.2136 - val_acc: 0.9466\n",
      "Epoch 00009: early stopping\n",
      "Result: -0.5525496072725649\n",
      "\n",
      " \t ::: 2 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.0013087252454918436}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_110 (InputLayer)       (None, 77, 5)             0         \n",
      "_________________________________________________________________\n",
      "flatten_110 (Flatten)        (None, 385)               0         \n",
      "_________________________________________________________________\n",
      "dense_541 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dropout_432 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_542 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_433 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_543 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_434 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_544 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 8,581\n",
      "Trainable params: 8,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 5)\n",
      "(6718, 77, 5)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/100\n",
      "26868/26868 [==============================] - 16s 599us/step - loss: 0.3493 - acc: 0.9477 - val_loss: 0.2629 - val_acc: 0.9466\n",
      "Epoch 2/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2451 - acc: 0.9482 - val_loss: 0.2476 - val_acc: 0.9452\n",
      "Epoch 3/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2375 - acc: 0.9480 - val_loss: 0.2388 - val_acc: 0.9466\n",
      "Epoch 4/100\n",
      "26868/26868 [==============================] - 2s 61us/step - loss: 0.2291 - acc: 0.9483 - val_loss: 0.2332 - val_acc: 0.9466\n",
      "Epoch 5/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2243 - acc: 0.9483 - val_loss: 0.2312 - val_acc: 0.9466\n",
      "Epoch 6/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2203 - acc: 0.9484 - val_loss: 0.2318 - val_acc: 0.9466\n",
      "Epoch 7/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2199 - acc: 0.9484 - val_loss: 0.2255 - val_acc: 0.9466\n",
      "Epoch 8/100\n",
      "26868/26868 [==============================] - 2s 66us/step - loss: 0.2165 - acc: 0.9484 - val_loss: 0.2242 - val_acc: 0.9466\n",
      "Epoch 9/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2156 - acc: 0.9483 - val_loss: 0.2201 - val_acc: 0.9466\n",
      "Epoch 10/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2132 - acc: 0.9483 - val_loss: 0.2167 - val_acc: 0.9466\n",
      "Epoch 11/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2106 - acc: 0.9483 - val_loss: 0.2210 - val_acc: 0.9466\n",
      "Epoch 12/100\n",
      "26868/26868 [==============================] - 2s 66us/step - loss: 0.2106 - acc: 0.9483 - val_loss: 0.2159 - val_acc: 0.9466\n",
      "Epoch 13/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2096 - acc: 0.9483 - val_loss: 0.2146 - val_acc: 0.9466\n",
      "Epoch 14/100\n",
      "26868/26868 [==============================] - 2s 65us/step - loss: 0.2079 - acc: 0.9483 - val_loss: 0.2167 - val_acc: 0.9466\n",
      "Epoch 15/100\n",
      "26868/26868 [==============================] - 2s 64us/step - loss: 0.2083 - acc: 0.9483 - val_loss: 0.2191 - val_acc: 0.9466\n",
      "Epoch 16/100\n",
      "26868/26868 [==============================] - 2s 65us/step - loss: 0.2082 - acc: 0.9483 - val_loss: 0.2176 - val_acc: 0.9466\n",
      "Epoch 00016: early stopping\n",
      "Result: -0.6330277399478991\n",
      "\n",
      " \t ::: 3 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.06865719805353464}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_111 (InputLayer)       (None, 77, 5)             0         \n",
      "_________________________________________________________________\n",
      "flatten_111 (Flatten)        (None, 385)               0         \n",
      "_________________________________________________________________\n",
      "dense_545 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dropout_435 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_546 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_436 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_547 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_437 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_548 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 8,581\n",
      "Trainable params: 8,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 5)\n",
      "(6718, 77, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/100\n",
      "26868/26868 [==============================] - 16s 605us/step - loss: 0.2903 - acc: 0.9448 - val_loss: 0.2925 - val_acc: 0.9466\n",
      "Epoch 2/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2593 - acc: 0.9482 - val_loss: 0.2567 - val_acc: 0.9466\n",
      "Epoch 3/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2381 - acc: 0.9481 - val_loss: 0.2144 - val_acc: 0.9466\n",
      "Epoch 4/100\n",
      "26868/26868 [==============================] - 2s 61us/step - loss: 0.2057 - acc: 0.9483 - val_loss: 0.2093 - val_acc: 0.9466\n",
      "Epoch 5/100\n",
      "26868/26868 [==============================] - 2s 60us/step - loss: 0.2042 - acc: 0.9483 - val_loss: 0.2088 - val_acc: 0.9466\n",
      "Epoch 6/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2037 - acc: 0.9483 - val_loss: 0.2086 - val_acc: 0.9466\n",
      "Epoch 7/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2039 - acc: 0.9483 - val_loss: 0.2091 - val_acc: 0.9466\n",
      "Epoch 8/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2037 - acc: 0.9483 - val_loss: 0.2087 - val_acc: 0.9466\n",
      "Epoch 9/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2037 - acc: 0.9483 - val_loss: 0.2088 - val_acc: 0.9466\n",
      "Epoch 00009: early stopping\n",
      "Result: -0.4986858710550397\n",
      "\n",
      " \t ::: 4 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.005406397030212645}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_112 (InputLayer)       (None, 77, 5)             0         \n",
      "_________________________________________________________________\n",
      "flatten_112 (Flatten)        (None, 385)               0         \n",
      "_________________________________________________________________\n",
      "dense_549 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dropout_438 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_550 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_439 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_551 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_440 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_552 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 8,581\n",
      "Trainable params: 8,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 5)\n",
      "(6718, 77, 5)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/100\n",
      "26868/26868 [==============================] - 17s 617us/step - loss: 0.2758 - acc: 0.9480 - val_loss: 0.2342 - val_acc: 0.9466\n",
      "Epoch 2/100\n",
      "26868/26868 [==============================] - 2s 64us/step - loss: 0.2242 - acc: 0.9483 - val_loss: 0.2255 - val_acc: 0.9466\n",
      "Epoch 3/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2145 - acc: 0.9483 - val_loss: 0.2149 - val_acc: 0.9466\n",
      "Epoch 4/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2103 - acc: 0.9483 - val_loss: 0.2156 - val_acc: 0.9466\n",
      "Epoch 5/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2087 - acc: 0.9483 - val_loss: 0.2139 - val_acc: 0.9466\n",
      "Epoch 6/100\n",
      "26868/26868 [==============================] - 2s 65us/step - loss: 0.2061 - acc: 0.9483 - val_loss: 0.2157 - val_acc: 0.9466\n",
      "Epoch 7/100\n",
      "26868/26868 [==============================] - 2s 64us/step - loss: 0.2079 - acc: 0.9483 - val_loss: 0.2115 - val_acc: 0.9466\n",
      "Epoch 8/100\n",
      "26868/26868 [==============================] - 2s 64us/step - loss: 0.2075 - acc: 0.9483 - val_loss: 0.2147 - val_acc: 0.9466\n",
      "Epoch 9/100\n",
      "26868/26868 [==============================] - 2s 64us/step - loss: 0.2071 - acc: 0.9483 - val_loss: 0.2114 - val_acc: 0.9466\n",
      "Epoch 10/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2071 - acc: 0.9483 - val_loss: 0.2153 - val_acc: 0.9466\n",
      "Epoch 11/100\n",
      "26868/26868 [==============================] - 2s 63us/step - loss: 0.2074 - acc: 0.9483 - val_loss: 0.2098 - val_acc: 0.9466\n",
      "Epoch 12/100\n",
      "26868/26868 [==============================] - 2s 62us/step - loss: 0.2077 - acc: 0.9483 - val_loss: 0.2130 - val_acc: 0.9466\n",
      "Epoch 13/100\n",
      "26868/26868 [==============================] - 2s 61us/step - loss: 0.2060 - acc: 0.9483 - val_loss: 0.2115 - val_acc: 0.9466\n",
      "Epoch 14/100\n",
      "26868/26868 [==============================] - 2s 64us/step - loss: 0.2058 - acc: 0.9483 - val_loss: 0.2113 - val_acc: 0.9466\n",
      "Epoch 00014: early stopping\n",
      "Result: -0.5539403937393145\n",
      "\n",
      " \t ::: 5 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.06709728339247478}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_113 (InputLayer)       (None, 77, 5)             0         \n",
      "_________________________________________________________________\n",
      "flatten_113 (Flatten)        (None, 385)               0         \n",
      "_________________________________________________________________\n",
      "dense_553 (Dense)            (None, 20)                7720      \n",
      "_________________________________________________________________\n",
      "dropout_441 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_554 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_442 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_555 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_443 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_556 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 8,581\n",
      "Trainable params: 8,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 5)\n",
      "(6718, 77, 5)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/100\n",
      "26100/26868 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9448"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-ecaab41b5a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'action_binary_encoded'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-231-c680437faa30>\u001b[0m in \u001b[0;36mfit_base_model\u001b[0;34m(self, label, batch_size, test_split, val_split, verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_optimal_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mfind_optimal_parameters\u001b[0;34m(self, X_train, y_train, X_test, y_test, num_calls, evaluation_function, max_epochs, batch_size, early_stopping_callback, seed, verbose, summary_txt_path)\u001b[0m\n\u001b[1;32m    231\u001b[0m         search_result = gp_minimize( func = fitness, dimensions = self.dimensions,\n\u001b[1;32m    232\u001b[0m                                      \u001b[0macq_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Expected Improvement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                                      n_calls = num_calls, x0 = prior_values )\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         s = create_skopt_results_string( search_result, prior_names, \n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(**p)\u001b[0m\n\u001b[1;32m    219\u001b[0m             self.train( X_train, y_train, X_test, y_test, max_epochs=max_epochs, batch_size=batch_size,  \n\u001b[1;32m    220\u001b[0m                         \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                         seed=seed, verbose=verbose )\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, X_val, y_val, max_epochs, batch_size, seed, verbose, early_stopping_callback, early_stopping)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         history = self.model.fit( X_train, y_train, validation_data = (X_val, y_val), \\\n\u001b[0;32m--> 183\u001b[0;31m                                  epochs = max_epochs, batch_size = batch_size, callbacks = [es] )#[metrics] + keras_callbacks)\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \"\"\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \"\"\"\n\u001b[1;32m   1488\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1444\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1446\u001b[0;31m             session._session, options_ptr)\n\u001b[0m\u001b[1;32m   1447\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = handler.fit_base_model('action_binary_encoded', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-0dc589e4ae49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_nlp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'action_binary_encoded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-4ef265cfeb5f>\u001b[0m in \u001b[0;36mfit_nlp_model\u001b[0;34m(self, label, batch_size, test_split, val_split, verbose)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         model.train_on_batch(training_generator = train_gen, validation_generator = test_gen, \n\u001b[0;32m---> 90\u001b[0;31m                              epochs = 1, steps_per_epoch = steps_per_epoch, validation_steps = validation_steps )\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, training_generator, validation_generator, epochs, steps_per_epoch, validation_steps, early_stopping_callback, early_stopping)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#self.class_weights = get_class_weights(data.train.y, self.num_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/nlp_model.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Encode the words of the sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0msent_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_encoder_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_input_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Shape back to concat the matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/nlp_model.py\u001b[0m in \u001b[0;36mword_encoder_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mword_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mword_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mword_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwordEncoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \"\"\"\n\u001b[1;32m   2419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "handler.fit_nlp_model('action_binary_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit_generator in module keras.engine.training:\n",
      "\n",
      "fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      "    Trains the model on data generated batch-by-batch by a Python generator\n",
      "    (or an instance of `Sequence`).\n",
      "    \n",
      "    The generator is run in parallel to the model, for efficiency.\n",
      "    For instance, this allows you to do real-time data augmentation\n",
      "    on images on CPU in parallel to training your model on GPU.\n",
      "    \n",
      "    The use of `keras.utils.Sequence` guarantees the ordering\n",
      "    and guarantees the single use of every input per epoch when\n",
      "    using `use_multiprocessing=True`.\n",
      "    \n",
      "    # Arguments\n",
      "        generator: A generator or an instance of `Sequence`\n",
      "            (`keras.utils.Sequence`) object in order to avoid\n",
      "            duplicate data when using multiprocessing.\n",
      "            The output of the generator must be either\n",
      "            - a tuple `(inputs, targets)`\n",
      "            - a tuple `(inputs, targets, sample_weights)`.\n",
      "            This tuple (a single output of the generator) makes a single\n",
      "            batch. Therefore, all arrays in this tuple must have the same\n",
      "            length (equal to the size of this batch). Different batches may\n",
      "            have different sizes. For example, the last batch of the epoch\n",
      "            is commonly smaller than the others, if the size of the dataset\n",
      "            is not divisible by the batch size.\n",
      "            The generator is expected to loop over its data\n",
      "            indefinitely. An epoch finishes when `steps_per_epoch`\n",
      "            batches have been seen by the model.\n",
      "        steps_per_epoch: Integer.\n",
      "            Total number of steps (batches of samples)\n",
      "            to yield from `generator` before declaring one epoch\n",
      "            finished and starting the next epoch. It should typically\n",
      "            be equal to the number of samples of your dataset\n",
      "            divided by the batch size.\n",
      "            Optional for `Sequence`: if unspecified, will use\n",
      "            the `len(generator)` as a number of steps.\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire data provided,\n",
      "            as defined by `steps_per_epoch`.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See [callbacks](/callbacks).\n",
      "        validation_data: This can be either\n",
      "            - a generator or a `Sequence` object for the validation data\n",
      "            - tuple `(x_val, y_val)`\n",
      "            - tuple `(x_val, y_val, val_sample_weights)`\n",
      "            on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data.\n",
      "        validation_steps: Only relevant if `validation_data`\n",
      "            is a generator. Total number of steps (batches of samples)\n",
      "            to yield from `validation_data` generator before stopping\n",
      "            at the end of every epoch. It should typically\n",
      "            be equal to the number of samples of your\n",
      "            validation dataset divided by the batch size.\n",
      "            Optional for `Sequence`: if unspecified, will use\n",
      "            the `len(validation_data)` as a number of steps.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only). This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples\n",
      "            from an under-represented class.\n",
      "        max_queue_size: Integer. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Maximum number of processes to spin up\n",
      "            when using process-based threading.\n",
      "            If unspecified, `workers` will default to 1. If 0, will\n",
      "            execute the generator on the main thread.\n",
      "        use_multiprocessing: Boolean.\n",
      "            If `True`, use process-based threading.\n",
      "            If unspecified, `use_multiprocessing` will default to `False`.\n",
      "            Note that because this implementation\n",
      "            relies on multiprocessing,\n",
      "            you should not pass non-picklable arguments to the generator\n",
      "            as they can't be passed easily to children processes.\n",
      "        shuffle: Boolean. Whether to shuffle the order of the batches at\n",
      "            the beginning of each epoch. Only used with instances\n",
      "            of `Sequence` (`keras.utils.Sequence`).\n",
      "            Has no effect when `steps_per_epoch` is not `None`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "    \n",
      "    # Returns\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    # Raises\n",
      "        ValueError: In case the generator yields data in an invalid format.\n",
      "    \n",
      "    # Example\n",
      "    \n",
      "    ```python\n",
      "    def generate_arrays_from_file(path):\n",
      "        while True:\n",
      "            with open(path) as f:\n",
      "                for line in f:\n",
      "                    # create numpy arrays of input data\n",
      "                    # and labels, from each line in the file\n",
      "                    x1, x2, y = process_line(line)\n",
      "                    yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
      "    \n",
      "    model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
      "                        steps_per_epoch=10000, epochs=10)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(keras.models.Model.fit_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "\n",
      " \t ::: 1 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 50, 'regulizer_value': 0.0015, 'dropout_value': 0.015, 'learning_rate': 0.001}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        (None, 64, 151, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 19328)             0         \n",
      "_________________________________________________________________\n",
      "dense_375 (Dense)            (None, 50)                966450    \n",
      "_________________________________________________________________\n",
      "dropout_325 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_376 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_326 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_377 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_327 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_378 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 971,652\n",
      "Trainable params: 971,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "[<utils_train.model_utils.PredictDataCallback object at 0x7f1cdfbe3048>, <utils_train.model_utils.PredictDataCallback object at 0x7f1cdfbe39b0>, <keras.callbacks.EarlyStopping object at 0x7f1cdfbe3e80>]\n",
      "Train on 20072 samples, validate on 5018 samples\n",
      "Epoch 1/200\n",
      "20072/20072 [==============================] - 12s 585us/step - loss: 0.2588 - val_loss: 0.1660\n",
      "Epoch 2/200\n",
      "20072/20072 [==============================] - 3s 154us/step - loss: 0.1370 - val_loss: 0.1384\n",
      "Epoch 3/200\n",
      "20072/20072 [==============================] - 3s 143us/step - loss: 0.1265 - val_loss: 0.1339\n",
      "Epoch 4/200\n",
      "20072/20072 [==============================] - 3s 140us/step - loss: 0.1239 - val_loss: 0.1324\n",
      "Epoch 5/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1225 - val_loss: 0.1308\n",
      "Epoch 6/200\n",
      "20072/20072 [==============================] - 3s 142us/step - loss: 0.1217 - val_loss: 0.1298\n",
      "Epoch 7/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1207 - val_loss: 0.1291\n",
      "Epoch 8/200\n",
      "20072/20072 [==============================] - 3s 140us/step - loss: 0.1203 - val_loss: 0.1288\n",
      "Epoch 9/200\n",
      "20072/20072 [==============================] - 3s 142us/step - loss: 0.1200 - val_loss: 0.1286\n",
      "Epoch 10/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1199 - val_loss: 0.1284\n",
      "Epoch 11/200\n",
      "20072/20072 [==============================] - 3s 143us/step - loss: 0.1198 - val_loss: 0.1282\n",
      "Epoch 12/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1196 - val_loss: 0.1281\n",
      "Epoch 13/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1196 - val_loss: 0.1281\n",
      "Epoch 14/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1195 - val_loss: 0.1280\n",
      "Epoch 15/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1194 - val_loss: 0.1280\n",
      "Epoch 16/200\n",
      "20072/20072 [==============================] - 3s 146us/step - loss: 0.1193 - val_loss: 0.1279\n",
      "Epoch 17/200\n",
      "20072/20072 [==============================] - 3s 142us/step - loss: 0.1193 - val_loss: 0.1279\n",
      "Epoch 18/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1193 - val_loss: 0.1278\n",
      "Epoch 19/200\n",
      "20072/20072 [==============================] - 3s 144us/step - loss: 0.1192 - val_loss: 0.1278\n",
      "Epoch 20/200\n",
      "20072/20072 [==============================] - 3s 144us/step - loss: 0.1192 - val_loss: 0.1278\n",
      "Epoch 21/200\n",
      "20072/20072 [==============================] - 3s 146us/step - loss: 0.1191 - val_loss: 0.1277\n",
      "Epoch 22/200\n",
      "20072/20072 [==============================] - 3s 146us/step - loss: 0.1192 - val_loss: 0.1277\n",
      "Epoch 23/200\n",
      "20072/20072 [==============================] - 3s 145us/step - loss: 0.1191 - val_loss: 0.1277\n",
      "Epoch 00023: early stopping\n",
      "Result: 0.5\n",
      "\n",
      " \t ::: 2 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 10, 'dense_units': 12, 'regulizer_value': 0.15189689846309112, 'dropout_value': 0.2686533062685247, 'learning_rate': 8.836464508307472e-05}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_52 (InputLayer)        (None, 64, 151, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 19328)             0         \n",
      "_________________________________________________________________\n",
      "dense_379 (Dense)            (None, 12)                231948    \n",
      "_________________________________________________________________\n",
      "dropout_328 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_380 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_329 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_381 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_330 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_382 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_331 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_383 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_332 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_384 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_333 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_385 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_334 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_386 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_335 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_387 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_336 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_388 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_337 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_389 (Dense)            (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 233,378\n",
      "Trainable params: 233,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "[<utils_train.model_utils.PredictDataCallback object at 0x7f1cdf0b6b00>, <utils_train.model_utils.PredictDataCallback object at 0x7f1cdf01ffd0>, <keras.callbacks.EarlyStopping object at 0x7f1cdf03eb70>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20072 samples, validate on 5018 samples\n",
      "Epoch 1/200\n",
      "20072/20072 [==============================] - 13s 638us/step - loss: 16.2009 - val_loss: 15.0451\n",
      "Epoch 2/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 14.2880 - val_loss: 13.6084\n",
      "Epoch 3/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 12.9366 - val_loss: 12.3275\n",
      "Epoch 4/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 11.7176 - val_loss: 11.1673\n",
      "Epoch 5/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 10.6118 - val_loss: 10.1134\n",
      "Epoch 6/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 9.6057 - val_loss: 9.1542\n",
      "Epoch 7/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 8.6895 - val_loss: 8.2782\n",
      "Epoch 8/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 7.8524 - val_loss: 7.4753\n",
      "Epoch 9/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 7.0870 - val_loss: 6.7436\n",
      "Epoch 10/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 6.3896 - val_loss: 6.0776\n",
      "Epoch 11/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 5.7528 - val_loss: 5.4705\n",
      "Epoch 12/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 5.1744 - val_loss: 4.9171\n",
      "Epoch 13/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 4.6458 - val_loss: 4.4128\n",
      "Epoch 14/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 4.1652 - val_loss: 3.9536\n",
      "Epoch 15/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 3.7270 - val_loss: 3.5361\n",
      "Epoch 16/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 3.3292 - val_loss: 3.1566\n",
      "Epoch 17/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 2.9677 - val_loss: 2.8124\n",
      "Epoch 18/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 2.6403 - val_loss: 2.5006\n",
      "Epoch 19/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 2.3440 - val_loss: 2.2187\n",
      "Epoch 20/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 2.0767 - val_loss: 1.9646\n",
      "Epoch 21/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 1.8357 - val_loss: 1.7358\n",
      "Epoch 22/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 1.6196 - val_loss: 1.5306\n",
      "Epoch 23/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 1.4251 - val_loss: 1.3470\n",
      "Epoch 24/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 1.2514 - val_loss: 1.1833\n",
      "Epoch 25/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 1.0974 - val_loss: 1.0379\n",
      "Epoch 26/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.9605 - val_loss: 0.9094\n",
      "Epoch 27/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.8402 - val_loss: 0.7962\n",
      "Epoch 28/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.7342 - val_loss: 0.6969\n",
      "Epoch 29/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.6418 - val_loss: 0.6103\n",
      "Epoch 30/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 0.5605 - val_loss: 0.5353\n",
      "Epoch 31/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 0.4923 - val_loss: 0.4706\n",
      "Epoch 32/200\n",
      "20072/20072 [==============================] - 4s 189us/step - loss: 0.4314 - val_loss: 0.4152\n",
      "Epoch 33/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.3806 - val_loss: 0.3680\n",
      "Epoch 34/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.3369 - val_loss: 0.3282\n",
      "Epoch 35/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.3005 - val_loss: 0.2947\n",
      "Epoch 36/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.2702 - val_loss: 0.2668\n",
      "Epoch 37/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.2450 - val_loss: 0.2438\n",
      "Epoch 38/200\n",
      "20072/20072 [==============================] - 4s 187us/step - loss: 0.2239 - val_loss: 0.2250\n",
      "Epoch 39/200\n",
      "20072/20072 [==============================] - 4s 186us/step - loss: 0.2074 - val_loss: 0.2096\n",
      "Epoch 40/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1937 - val_loss: 0.1972\n",
      "Epoch 41/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1827 - val_loss: 0.1873\n",
      "Epoch 42/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1739 - val_loss: 0.1793\n",
      "Epoch 43/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.1663 - val_loss: 0.1730\n",
      "Epoch 44/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1610 - val_loss: 0.1680\n",
      "Epoch 45/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1567 - val_loss: 0.1640\n",
      "Epoch 46/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 0.1532 - val_loss: 0.1608\n",
      "Epoch 47/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1502 - val_loss: 0.1581\n",
      "Epoch 48/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.1476 - val_loss: 0.1560\n",
      "Epoch 49/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1461 - val_loss: 0.1541\n",
      "Epoch 50/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1444 - val_loss: 0.1525\n",
      "Epoch 51/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1428 - val_loss: 0.1510\n",
      "Epoch 52/200\n",
      "20072/20072 [==============================] - 4s 179us/step - loss: 0.1412 - val_loss: 0.1497\n",
      "Epoch 53/200\n",
      "20072/20072 [==============================] - 4s 185us/step - loss: 0.1400 - val_loss: 0.1485\n",
      "Epoch 54/200\n",
      "20072/20072 [==============================] - 4s 179us/step - loss: 0.1390 - val_loss: 0.1473\n",
      "Epoch 55/200\n",
      "20072/20072 [==============================] - 4s 183us/step - loss: 0.1380 - val_loss: 0.1462\n",
      "Epoch 56/200\n",
      "20072/20072 [==============================] - 4s 185us/step - loss: 0.1368 - val_loss: 0.1451\n",
      "Epoch 57/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1354 - val_loss: 0.1441\n",
      "Epoch 58/200\n",
      "20072/20072 [==============================] - 4s 184us/step - loss: 0.1346 - val_loss: 0.1432\n",
      "Epoch 59/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1341 - val_loss: 0.1423\n",
      "Epoch 60/200\n",
      "20072/20072 [==============================] - 4s 181us/step - loss: 0.1331 - val_loss: 0.1414\n",
      "Epoch 61/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.1322 - val_loss: 0.1406\n",
      "Epoch 62/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.1314 - val_loss: 0.1398\n",
      "Epoch 63/200\n",
      "20072/20072 [==============================] - 4s 182us/step - loss: 0.1306 - val_loss: 0.1390\n",
      "Epoch 64/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1300 - val_loss: 0.1383\n",
      "Epoch 65/200\n",
      "20072/20072 [==============================] - 4s 174us/step - loss: 0.1291 - val_loss: 0.1376\n",
      "Epoch 66/200\n",
      "20072/20072 [==============================] - 4s 187us/step - loss: 0.1286 - val_loss: 0.1370\n",
      "Epoch 67/200\n",
      "20072/20072 [==============================] - 4s 186us/step - loss: 0.1279 - val_loss: 0.1363\n",
      "Epoch 68/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1273 - val_loss: 0.1358\n",
      "Epoch 69/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1266 - val_loss: 0.1352\n",
      "Epoch 70/200\n",
      "20072/20072 [==============================] - 4s 189us/step - loss: 0.1261 - val_loss: 0.1347\n",
      "Epoch 71/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1257 - val_loss: 0.1342\n",
      "Epoch 72/200\n",
      "20072/20072 [==============================] - 4s 182us/step - loss: 0.1252 - val_loss: 0.1337\n",
      "Epoch 73/200\n",
      "20072/20072 [==============================] - 4s 179us/step - loss: 0.1247 - val_loss: 0.1332\n",
      "Epoch 74/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 0.1242 - val_loss: 0.1328\n",
      "Epoch 75/200\n",
      "20072/20072 [==============================] - 4s 180us/step - loss: 0.1239 - val_loss: 0.1324\n",
      "Epoch 76/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1235 - val_loss: 0.1320\n",
      "Epoch 77/200\n",
      "20072/20072 [==============================] - 4s 174us/step - loss: 0.1231 - val_loss: 0.1316\n",
      "Epoch 78/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1227 - val_loss: 0.1313\n",
      "Epoch 79/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1223 - val_loss: 0.1310\n",
      "Epoch 80/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1221 - val_loss: 0.1307\n",
      "Epoch 81/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1218 - val_loss: 0.1304\n",
      "Epoch 82/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1215 - val_loss: 0.1301\n",
      "Epoch 83/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1213 - val_loss: 0.1298\n",
      "Epoch 84/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1210 - val_loss: 0.1296\n",
      "Epoch 85/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1207 - val_loss: 0.1294\n",
      "Epoch 86/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1205 - val_loss: 0.1292\n",
      "Epoch 87/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1203 - val_loss: 0.1290\n",
      "Epoch 88/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1202 - val_loss: 0.1288\n",
      "Epoch 89/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1200 - val_loss: 0.1287\n",
      "Epoch 90/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1198 - val_loss: 0.1285\n",
      "Epoch 91/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1197 - val_loss: 0.1284\n",
      "Epoch 92/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1195 - val_loss: 0.1282\n",
      "Epoch 93/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1194 - val_loss: 0.1281\n",
      "Epoch 94/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1193 - val_loss: 0.1280\n",
      "Epoch 95/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1192 - val_loss: 0.1279\n",
      "Epoch 96/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1191 - val_loss: 0.1278\n",
      "Epoch 97/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 0.1190 - val_loss: 0.1278\n",
      "Epoch 98/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 0.1190 - val_loss: 0.1277\n",
      "Epoch 99/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 0.1189 - val_loss: 0.1276\n",
      "Epoch 100/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1188 - val_loss: 0.1276\n",
      "Epoch 101/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1188 - val_loss: 0.1275\n",
      "Epoch 102/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1187 - val_loss: 0.1275\n",
      "Epoch 103/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1187 - val_loss: 0.1274\n",
      "Epoch 104/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 0.1187 - val_loss: 0.1274\n",
      "Epoch 105/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1186 - val_loss: 0.1274\n",
      "Epoch 106/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1274\n",
      "Epoch 107/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 0.1186 - val_loss: 0.1274\n",
      "Epoch 108/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 109/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 110/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 111/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 112/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 113/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 114/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 115/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 116/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 117/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 118/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 119/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 120/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 121/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 122/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 00122: early stopping\n",
      "Result: 0.5\n",
      "\n",
      " \t ::: 3 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 73, 'regulizer_value': 0.5158612809419505, 'dropout_value': 0.3225979543593234, 'learning_rate': 0.00032554867843791457}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_53 (InputLayer)        (None, 64, 151, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 19328)             0         \n",
      "_________________________________________________________________\n",
      "dense_390 (Dense)            (None, 73)                1411017   \n",
      "_________________________________________________________________\n",
      "dropout_338 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_391 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_339 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_392 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_340 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_393 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_341 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_394 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_342 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_395 (Dense)            (None, 2)                 148       \n",
      "=================================================================\n",
      "Total params: 1,432,773\n",
      "Trainable params: 1,432,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "[<utils_train.model_utils.PredictDataCallback object at 0x7f1cdddcc240>, <utils_train.model_utils.PredictDataCallback object at 0x7f1cdddccac8>, <keras.callbacks.EarlyStopping object at 0x7f1cdddccb00>]\n",
      "Train on 20072 samples, validate on 5018 samples\n",
      "Epoch 1/200\n",
      "20072/20072 [==============================] - 13s 627us/step - loss: 102.8321 - val_loss: 61.0412\n",
      "Epoch 2/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 39.6956 - val_loss: 23.6965\n",
      "Epoch 3/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 15.1019 - val_loss: 8.7539\n",
      "Epoch 4/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 5.4479 - val_loss: 3.0754\n",
      "Epoch 5/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 1.8900 - val_loss: 1.0784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.6856 - val_loss: 0.4395\n",
      "Epoch 7/200\n",
      "20072/20072 [==============================] - 3s 162us/step - loss: 0.3154 - val_loss: 0.2530\n",
      "Epoch 8/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.2107 - val_loss: 0.2009\n",
      "Epoch 9/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1802 - val_loss: 0.1838\n",
      "Epoch 10/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1688 - val_loss: 0.1752\n",
      "Epoch 11/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1619 - val_loss: 0.1691\n",
      "Epoch 12/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1566 - val_loss: 0.1639\n",
      "Epoch 13/200\n",
      "20072/20072 [==============================] - 3s 162us/step - loss: 0.1520 - val_loss: 0.1596\n",
      "Epoch 14/200\n",
      "20072/20072 [==============================] - 3s 161us/step - loss: 0.1481 - val_loss: 0.1558\n",
      "Epoch 15/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1448 - val_loss: 0.1525\n",
      "Epoch 16/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1417 - val_loss: 0.1495\n",
      "Epoch 17/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1392 - val_loss: 0.1470\n",
      "Epoch 18/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1368 - val_loss: 0.1447\n",
      "Epoch 19/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.1347 - val_loss: 0.1427\n",
      "Epoch 20/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1329 - val_loss: 0.1410\n",
      "Epoch 21/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1310 - val_loss: 0.1392\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-ecaab41b5a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'action_binary_encoded'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-135-288214bb3174>\u001b[0m in \u001b[0;36mfit_base_model\u001b[0;34m(self, label, batch_size, test_split, val_split, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_optimal_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mfind_optimal_parameters\u001b[0;34m(self, X_train, y_train, X_test, y_test, num_calls, evaluation_function, max_epochs, batch_size, early_stopping_callback, seed, verbose, summary_txt_path)\u001b[0m\n\u001b[1;32m    149\u001b[0m         search_result = gp_minimize( func = fitness, dimensions = self.dimensions,\n\u001b[1;32m    150\u001b[0m                                      \u001b[0macq_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Expected Improvement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                                      n_calls = num_calls, x0 = prior_values )\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         s = create_skopt_results_string( search_result, prior_names, \n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(**p)\u001b[0m\n\u001b[1;32m    138\u001b[0m             self.train( X_train, y_train, X_test, y_test, max_epochs=max_epochs, batch_size=batch_size,  \n\u001b[1;32m    139\u001b[0m                         \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                         seed=seed, verbose=verbose )\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, X_val, y_val, max_epochs, batch_size, seed, verbose, early_stopping_callback, early_stopping)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         history = self.model.fit( X_train, y_train, validation_data = (X_val, y_val), \\\n\u001b[0;32m--> 104\u001b[0;31m                                  epochs = max_epochs, batch_size = batch_size, callbacks = keras_callbacks)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/utils_train/model_utils.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_word\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_word\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'loss', 'predictions', 'labels', 'val_predictions', 'val_labels'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "##############\n",
    "# Functions to yield batches for the training\n",
    "##############\n",
    "\n",
    "\n",
    "def train_generator(self, batch_size):\n",
    "\n",
    "    train_gen = InputBatchGenerator(self.X_train, self.y_train, batch_size, self.codes, self.sites, self.dim_msg)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            for B in train_gen.generate_msg_count_batches():\n",
    "                yield B\n",
    "        except StopIteration:\n",
    "            logging.warning(\"start over generator loop\")          \n",
    "\n",
    "def test_generator(self, batch_size):\n",
    "\n",
    "    test_gen = InputBatchGenerator(self.X_test, self.y_test, batch_size, self.codes, self.sites, self.dim_msg)\n",
    "    for B in test_gen.generate_msg_count_batches():\n",
    "        yield B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
