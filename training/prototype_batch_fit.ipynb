{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import InputGenerator\n",
    "import base_model\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import baseline_model\n",
    "import importlib\n",
    "import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import InputBatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'baseline_model' from '/nfshome/llayer/AIErrorLogAnalysis/training/baseline_model.py'>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'InputGenerator' from '/nfshome/llayer/AIErrorLogAnalysis/training/InputGenerator.py'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(InputGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'InputBatchGenerator' from '/nfshome/llayer/AIErrorLogAnalysis/training/InputBatchGenerator.py'>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(InputBatchGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = InputGenerator.InputGenerator('/nfshome/llayer/data/actionshistory.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.set_padded_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25090, 64, 151, 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.get_input_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nlp_model' from '/nfshome/llayer/AIErrorLogAnalysis/training/nlp_model.py'>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(nlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NLP_Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-30cf730c11a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnlp_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLP_Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NLP_Model'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(base_model)\n",
    "importlib.reload(baseline_model)\n",
    "importlib.reload(nlp_model) \n",
    "\n",
    "class FitHandler(object):\n",
    "    \n",
    "    def __init__(self, gen):\n",
    "        \n",
    "        self.gen = gen\n",
    "        self.dim_tasks, self.dim_errors, self.dim_sites, self.dim_msg = gen.get_input_shape()\n",
    "        self.sites = gen.sites\n",
    "        self.codes = gen.codes\n",
    "    \n",
    "    def split_frame(self, label, split_level):\n",
    "        \n",
    "        train, test, _, _ = train_test_split(gen.actionshistory, gen.actionshistory[label], test_size=split_level)\n",
    "        return train, test\n",
    "        \n",
    "    def k_fold_indices(self, kfold_function=KFold, kfold_splits=5, verbose=0):\n",
    "        \n",
    "        enum = enumerate(kfold_function(n_splits=kfold_splits, shuffle=True, random_state=seed).split(X,Y))\n",
    "        if verbose != 0:\n",
    "            enum = tqdm(enum, total=kfold_splits, desc='kfold', leave=False, initial=0)\n",
    "        return enum\n",
    "        \n",
    "    def fit_base_model(self, label, batch_size = 100, test_split=0.2, val_split=0.2, verbose=0):\n",
    "        \n",
    "        \n",
    "        # Split in train and test frames\n",
    "        train, test = self.split_frame(label, test_split)\n",
    "        \n",
    "        # Setup the generator of the input batches\n",
    "        train_gen = InputBatchGenerator.InputBatchGenerator(train, label, \\\n",
    "                                                            batch_size, self.codes, self.sites, self.dim_msg)\n",
    "        test_gen = InputBatchGenerator.InputBatchGenerator(test, label, \\\n",
    "                                                            batch_size, self.codes, self.sites, self.dim_msg)        \n",
    "        X_train, y_train = train_gen.count_matrix()\n",
    "        X_test, y_test = test_gen.count_matrix()\n",
    "        \n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        \n",
    "        # Set the baseline model\n",
    "        model = baseline_model.FF(2, self.dim_errors, self.dim_sites)\n",
    "        \n",
    "        \"\"\"\n",
    "        history = model.train(X_train, y_train, X_test, y_test, max_epochs = 200, batch_size = 100, early_stopping = True)\n",
    "        \n",
    "        return history\n",
    "        \"\"\"\n",
    "        \n",
    "        model.find_optimal_parameters(X_train, y_train, X_test, y_test, max_epochs = 200, batch_size = 100, num_calls=12)\n",
    "\n",
    "        \n",
    "    def fit_nlp_model(self):\n",
    "\n",
    "        # Set the baseline model\n",
    "        model = nlp_model.NLP_Model(2, self.dim_errors, self.dim_sites, self.dim_msg)            \n",
    "        model.print_summary()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = FitHandler(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word encoder model\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_85 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 200, 50)           2868400   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 10)                2440      \n",
      "=================================================================\n",
      "Total params: 2,870,840\n",
      "Trainable params: 2,440\n",
      "Non-trainable params: 2,868,400\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Site encoder model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_86 (InputLayer)        (None, 64, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_67 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_413 (Dense)            (None, 10)                7690      \n",
      "=================================================================\n",
      "Total params: 7,690\n",
      "Trainable params: 7,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Full model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_88 (InputLayer)           (None, 9664, 200)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 9664, 10)     2870840     input_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_87 (InputLayer)           (None, 9664, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 9664, 12)     0           time_distributed_10[0][0]        \n",
      "                                                                 input_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 64, 151, 12)  0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 64, 10)       7690        reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_69 (Flatten)            (None, 640)          0           time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_415 (Dense)               (None, 10)           6410        flatten_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_416 (Dense)               (None, 1)            11          dense_415[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,884,951\n",
      "Trainable params: 16,551\n",
      "Non-trainable params: 2,868,400\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "handler.fit_nlp_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "\n",
      " \t ::: 1 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 50, 'regulizer_value': 0.0015, 'dropout_value': 0.015, 'learning_rate': 0.001}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        (None, 64, 151, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 19328)             0         \n",
      "_________________________________________________________________\n",
      "dense_375 (Dense)            (None, 50)                966450    \n",
      "_________________________________________________________________\n",
      "dropout_325 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_376 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_326 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_377 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_327 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_378 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 971,652\n",
      "Trainable params: 971,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "[<utils_train.model_utils.PredictDataCallback object at 0x7f1cdfbe3048>, <utils_train.model_utils.PredictDataCallback object at 0x7f1cdfbe39b0>, <keras.callbacks.EarlyStopping object at 0x7f1cdfbe3e80>]\n",
      "Train on 20072 samples, validate on 5018 samples\n",
      "Epoch 1/200\n",
      "20072/20072 [==============================] - 12s 585us/step - loss: 0.2588 - val_loss: 0.1660\n",
      "Epoch 2/200\n",
      "20072/20072 [==============================] - 3s 154us/step - loss: 0.1370 - val_loss: 0.1384\n",
      "Epoch 3/200\n",
      "20072/20072 [==============================] - 3s 143us/step - loss: 0.1265 - val_loss: 0.1339\n",
      "Epoch 4/200\n",
      "20072/20072 [==============================] - 3s 140us/step - loss: 0.1239 - val_loss: 0.1324\n",
      "Epoch 5/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1225 - val_loss: 0.1308\n",
      "Epoch 6/200\n",
      "20072/20072 [==============================] - 3s 142us/step - loss: 0.1217 - val_loss: 0.1298\n",
      "Epoch 7/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1207 - val_loss: 0.1291\n",
      "Epoch 8/200\n",
      "20072/20072 [==============================] - 3s 140us/step - loss: 0.1203 - val_loss: 0.1288\n",
      "Epoch 9/200\n",
      "20072/20072 [==============================] - 3s 142us/step - loss: 0.1200 - val_loss: 0.1286\n",
      "Epoch 10/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1199 - val_loss: 0.1284\n",
      "Epoch 11/200\n",
      "20072/20072 [==============================] - 3s 143us/step - loss: 0.1198 - val_loss: 0.1282\n",
      "Epoch 12/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1196 - val_loss: 0.1281\n",
      "Epoch 13/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1196 - val_loss: 0.1281\n",
      "Epoch 14/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1195 - val_loss: 0.1280\n",
      "Epoch 15/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1194 - val_loss: 0.1280\n",
      "Epoch 16/200\n",
      "20072/20072 [==============================] - 3s 146us/step - loss: 0.1193 - val_loss: 0.1279\n",
      "Epoch 17/200\n",
      "20072/20072 [==============================] - 3s 142us/step - loss: 0.1193 - val_loss: 0.1279\n",
      "Epoch 18/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1193 - val_loss: 0.1278\n",
      "Epoch 19/200\n",
      "20072/20072 [==============================] - 3s 144us/step - loss: 0.1192 - val_loss: 0.1278\n",
      "Epoch 20/200\n",
      "20072/20072 [==============================] - 3s 144us/step - loss: 0.1192 - val_loss: 0.1278\n",
      "Epoch 21/200\n",
      "20072/20072 [==============================] - 3s 146us/step - loss: 0.1191 - val_loss: 0.1277\n",
      "Epoch 22/200\n",
      "20072/20072 [==============================] - 3s 146us/step - loss: 0.1192 - val_loss: 0.1277\n",
      "Epoch 23/200\n",
      "20072/20072 [==============================] - 3s 145us/step - loss: 0.1191 - val_loss: 0.1277\n",
      "Epoch 00023: early stopping\n",
      "Result: 0.5\n",
      "\n",
      " \t ::: 2 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 10, 'dense_units': 12, 'regulizer_value': 0.15189689846309112, 'dropout_value': 0.2686533062685247, 'learning_rate': 8.836464508307472e-05}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_52 (InputLayer)        (None, 64, 151, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 19328)             0         \n",
      "_________________________________________________________________\n",
      "dense_379 (Dense)            (None, 12)                231948    \n",
      "_________________________________________________________________\n",
      "dropout_328 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_380 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_329 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_381 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_330 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_382 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_331 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_383 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_332 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_384 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_333 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_385 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_334 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_386 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_335 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_387 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_336 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_388 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_337 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_389 (Dense)            (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 233,378\n",
      "Trainable params: 233,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "[<utils_train.model_utils.PredictDataCallback object at 0x7f1cdf0b6b00>, <utils_train.model_utils.PredictDataCallback object at 0x7f1cdf01ffd0>, <keras.callbacks.EarlyStopping object at 0x7f1cdf03eb70>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20072 samples, validate on 5018 samples\n",
      "Epoch 1/200\n",
      "20072/20072 [==============================] - 13s 638us/step - loss: 16.2009 - val_loss: 15.0451\n",
      "Epoch 2/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 14.2880 - val_loss: 13.6084\n",
      "Epoch 3/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 12.9366 - val_loss: 12.3275\n",
      "Epoch 4/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 11.7176 - val_loss: 11.1673\n",
      "Epoch 5/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 10.6118 - val_loss: 10.1134\n",
      "Epoch 6/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 9.6057 - val_loss: 9.1542\n",
      "Epoch 7/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 8.6895 - val_loss: 8.2782\n",
      "Epoch 8/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 7.8524 - val_loss: 7.4753\n",
      "Epoch 9/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 7.0870 - val_loss: 6.7436\n",
      "Epoch 10/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 6.3896 - val_loss: 6.0776\n",
      "Epoch 11/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 5.7528 - val_loss: 5.4705\n",
      "Epoch 12/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 5.1744 - val_loss: 4.9171\n",
      "Epoch 13/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 4.6458 - val_loss: 4.4128\n",
      "Epoch 14/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 4.1652 - val_loss: 3.9536\n",
      "Epoch 15/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 3.7270 - val_loss: 3.5361\n",
      "Epoch 16/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 3.3292 - val_loss: 3.1566\n",
      "Epoch 17/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 2.9677 - val_loss: 2.8124\n",
      "Epoch 18/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 2.6403 - val_loss: 2.5006\n",
      "Epoch 19/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 2.3440 - val_loss: 2.2187\n",
      "Epoch 20/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 2.0767 - val_loss: 1.9646\n",
      "Epoch 21/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 1.8357 - val_loss: 1.7358\n",
      "Epoch 22/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 1.6196 - val_loss: 1.5306\n",
      "Epoch 23/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 1.4251 - val_loss: 1.3470\n",
      "Epoch 24/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 1.2514 - val_loss: 1.1833\n",
      "Epoch 25/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 1.0974 - val_loss: 1.0379\n",
      "Epoch 26/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.9605 - val_loss: 0.9094\n",
      "Epoch 27/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.8402 - val_loss: 0.7962\n",
      "Epoch 28/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.7342 - val_loss: 0.6969\n",
      "Epoch 29/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.6418 - val_loss: 0.6103\n",
      "Epoch 30/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 0.5605 - val_loss: 0.5353\n",
      "Epoch 31/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 0.4923 - val_loss: 0.4706\n",
      "Epoch 32/200\n",
      "20072/20072 [==============================] - 4s 189us/step - loss: 0.4314 - val_loss: 0.4152\n",
      "Epoch 33/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.3806 - val_loss: 0.3680\n",
      "Epoch 34/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.3369 - val_loss: 0.3282\n",
      "Epoch 35/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.3005 - val_loss: 0.2947\n",
      "Epoch 36/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.2702 - val_loss: 0.2668\n",
      "Epoch 37/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.2450 - val_loss: 0.2438\n",
      "Epoch 38/200\n",
      "20072/20072 [==============================] - 4s 187us/step - loss: 0.2239 - val_loss: 0.2250\n",
      "Epoch 39/200\n",
      "20072/20072 [==============================] - 4s 186us/step - loss: 0.2074 - val_loss: 0.2096\n",
      "Epoch 40/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1937 - val_loss: 0.1972\n",
      "Epoch 41/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1827 - val_loss: 0.1873\n",
      "Epoch 42/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1739 - val_loss: 0.1793\n",
      "Epoch 43/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.1663 - val_loss: 0.1730\n",
      "Epoch 44/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1610 - val_loss: 0.1680\n",
      "Epoch 45/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1567 - val_loss: 0.1640\n",
      "Epoch 46/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 0.1532 - val_loss: 0.1608\n",
      "Epoch 47/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1502 - val_loss: 0.1581\n",
      "Epoch 48/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.1476 - val_loss: 0.1560\n",
      "Epoch 49/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1461 - val_loss: 0.1541\n",
      "Epoch 50/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1444 - val_loss: 0.1525\n",
      "Epoch 51/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1428 - val_loss: 0.1510\n",
      "Epoch 52/200\n",
      "20072/20072 [==============================] - 4s 179us/step - loss: 0.1412 - val_loss: 0.1497\n",
      "Epoch 53/200\n",
      "20072/20072 [==============================] - 4s 185us/step - loss: 0.1400 - val_loss: 0.1485\n",
      "Epoch 54/200\n",
      "20072/20072 [==============================] - 4s 179us/step - loss: 0.1390 - val_loss: 0.1473\n",
      "Epoch 55/200\n",
      "20072/20072 [==============================] - 4s 183us/step - loss: 0.1380 - val_loss: 0.1462\n",
      "Epoch 56/200\n",
      "20072/20072 [==============================] - 4s 185us/step - loss: 0.1368 - val_loss: 0.1451\n",
      "Epoch 57/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1354 - val_loss: 0.1441\n",
      "Epoch 58/200\n",
      "20072/20072 [==============================] - 4s 184us/step - loss: 0.1346 - val_loss: 0.1432\n",
      "Epoch 59/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1341 - val_loss: 0.1423\n",
      "Epoch 60/200\n",
      "20072/20072 [==============================] - 4s 181us/step - loss: 0.1331 - val_loss: 0.1414\n",
      "Epoch 61/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.1322 - val_loss: 0.1406\n",
      "Epoch 62/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.1314 - val_loss: 0.1398\n",
      "Epoch 63/200\n",
      "20072/20072 [==============================] - 4s 182us/step - loss: 0.1306 - val_loss: 0.1390\n",
      "Epoch 64/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1300 - val_loss: 0.1383\n",
      "Epoch 65/200\n",
      "20072/20072 [==============================] - 4s 174us/step - loss: 0.1291 - val_loss: 0.1376\n",
      "Epoch 66/200\n",
      "20072/20072 [==============================] - 4s 187us/step - loss: 0.1286 - val_loss: 0.1370\n",
      "Epoch 67/200\n",
      "20072/20072 [==============================] - 4s 186us/step - loss: 0.1279 - val_loss: 0.1363\n",
      "Epoch 68/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1273 - val_loss: 0.1358\n",
      "Epoch 69/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1266 - val_loss: 0.1352\n",
      "Epoch 70/200\n",
      "20072/20072 [==============================] - 4s 189us/step - loss: 0.1261 - val_loss: 0.1347\n",
      "Epoch 71/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1257 - val_loss: 0.1342\n",
      "Epoch 72/200\n",
      "20072/20072 [==============================] - 4s 182us/step - loss: 0.1252 - val_loss: 0.1337\n",
      "Epoch 73/200\n",
      "20072/20072 [==============================] - 4s 179us/step - loss: 0.1247 - val_loss: 0.1332\n",
      "Epoch 74/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 0.1242 - val_loss: 0.1328\n",
      "Epoch 75/200\n",
      "20072/20072 [==============================] - 4s 180us/step - loss: 0.1239 - val_loss: 0.1324\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1235 - val_loss: 0.1320\n",
      "Epoch 77/200\n",
      "20072/20072 [==============================] - 4s 174us/step - loss: 0.1231 - val_loss: 0.1316\n",
      "Epoch 78/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1227 - val_loss: 0.1313\n",
      "Epoch 79/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1223 - val_loss: 0.1310\n",
      "Epoch 80/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1221 - val_loss: 0.1307\n",
      "Epoch 81/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1218 - val_loss: 0.1304\n",
      "Epoch 82/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1215 - val_loss: 0.1301\n",
      "Epoch 83/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1213 - val_loss: 0.1298\n",
      "Epoch 84/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1210 - val_loss: 0.1296\n",
      "Epoch 85/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1207 - val_loss: 0.1294\n",
      "Epoch 86/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1205 - val_loss: 0.1292\n",
      "Epoch 87/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1203 - val_loss: 0.1290\n",
      "Epoch 88/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1202 - val_loss: 0.1288\n",
      "Epoch 89/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1200 - val_loss: 0.1287\n",
      "Epoch 90/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1198 - val_loss: 0.1285\n",
      "Epoch 91/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1197 - val_loss: 0.1284\n",
      "Epoch 92/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1195 - val_loss: 0.1282\n",
      "Epoch 93/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1194 - val_loss: 0.1281\n",
      "Epoch 94/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1193 - val_loss: 0.1280\n",
      "Epoch 95/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1192 - val_loss: 0.1279\n",
      "Epoch 96/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1191 - val_loss: 0.1278\n",
      "Epoch 97/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 0.1190 - val_loss: 0.1278\n",
      "Epoch 98/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 0.1190 - val_loss: 0.1277\n",
      "Epoch 99/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 0.1189 - val_loss: 0.1276\n",
      "Epoch 100/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1188 - val_loss: 0.1276\n",
      "Epoch 101/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1188 - val_loss: 0.1275\n",
      "Epoch 102/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1187 - val_loss: 0.1275\n",
      "Epoch 103/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1187 - val_loss: 0.1274\n",
      "Epoch 104/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 0.1187 - val_loss: 0.1274\n",
      "Epoch 105/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1186 - val_loss: 0.1274\n",
      "Epoch 106/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1274\n",
      "Epoch 107/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 0.1186 - val_loss: 0.1274\n",
      "Epoch 108/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 109/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 110/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 111/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 112/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 113/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 114/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 115/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 116/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 117/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 118/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 119/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 120/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 121/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 122/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 00122: early stopping\n",
      "Result: 0.5\n",
      "\n",
      " \t ::: 3 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 73, 'regulizer_value': 0.5158612809419505, 'dropout_value': 0.3225979543593234, 'learning_rate': 0.00032554867843791457}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_53 (InputLayer)        (None, 64, 151, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 19328)             0         \n",
      "_________________________________________________________________\n",
      "dense_390 (Dense)            (None, 73)                1411017   \n",
      "_________________________________________________________________\n",
      "dropout_338 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_391 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_339 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_392 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_340 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_393 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_341 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_394 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_342 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_395 (Dense)            (None, 2)                 148       \n",
      "=================================================================\n",
      "Total params: 1,432,773\n",
      "Trainable params: 1,432,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "[<utils_train.model_utils.PredictDataCallback object at 0x7f1cdddcc240>, <utils_train.model_utils.PredictDataCallback object at 0x7f1cdddccac8>, <keras.callbacks.EarlyStopping object at 0x7f1cdddccb00>]\n",
      "Train on 20072 samples, validate on 5018 samples\n",
      "Epoch 1/200\n",
      "20072/20072 [==============================] - 13s 627us/step - loss: 102.8321 - val_loss: 61.0412\n",
      "Epoch 2/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 39.6956 - val_loss: 23.6965\n",
      "Epoch 3/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 15.1019 - val_loss: 8.7539\n",
      "Epoch 4/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 5.4479 - val_loss: 3.0754\n",
      "Epoch 5/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 1.8900 - val_loss: 1.0784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.6856 - val_loss: 0.4395\n",
      "Epoch 7/200\n",
      "20072/20072 [==============================] - 3s 162us/step - loss: 0.3154 - val_loss: 0.2530\n",
      "Epoch 8/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.2107 - val_loss: 0.2009\n",
      "Epoch 9/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1802 - val_loss: 0.1838\n",
      "Epoch 10/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1688 - val_loss: 0.1752\n",
      "Epoch 11/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1619 - val_loss: 0.1691\n",
      "Epoch 12/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1566 - val_loss: 0.1639\n",
      "Epoch 13/200\n",
      "20072/20072 [==============================] - 3s 162us/step - loss: 0.1520 - val_loss: 0.1596\n",
      "Epoch 14/200\n",
      "20072/20072 [==============================] - 3s 161us/step - loss: 0.1481 - val_loss: 0.1558\n",
      "Epoch 15/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1448 - val_loss: 0.1525\n",
      "Epoch 16/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1417 - val_loss: 0.1495\n",
      "Epoch 17/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1392 - val_loss: 0.1470\n",
      "Epoch 18/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1368 - val_loss: 0.1447\n",
      "Epoch 19/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.1347 - val_loss: 0.1427\n",
      "Epoch 20/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1329 - val_loss: 0.1410\n",
      "Epoch 21/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1310 - val_loss: 0.1392\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-ecaab41b5a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'action_binary_encoded'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-135-288214bb3174>\u001b[0m in \u001b[0;36mfit_base_model\u001b[0;34m(self, label, batch_size, test_split, val_split, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_optimal_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mfind_optimal_parameters\u001b[0;34m(self, X_train, y_train, X_test, y_test, num_calls, evaluation_function, max_epochs, batch_size, early_stopping_callback, seed, verbose, summary_txt_path)\u001b[0m\n\u001b[1;32m    149\u001b[0m         search_result = gp_minimize( func = fitness, dimensions = self.dimensions,\n\u001b[1;32m    150\u001b[0m                                      \u001b[0macq_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Expected Improvement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                                      n_calls = num_calls, x0 = prior_values )\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         s = create_skopt_results_string( search_result, prior_names, \n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(**p)\u001b[0m\n\u001b[1;32m    138\u001b[0m             self.train( X_train, y_train, X_test, y_test, max_epochs=max_epochs, batch_size=batch_size,  \n\u001b[1;32m    139\u001b[0m                         \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                         seed=seed, verbose=verbose )\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, X_val, y_val, max_epochs, batch_size, seed, verbose, early_stopping_callback, early_stopping)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         history = self.model.fit( X_train, y_train, validation_data = (X_val, y_val), \\\n\u001b[0;32m--> 104\u001b[0;31m                                  epochs = max_epochs, batch_size = batch_size, callbacks = keras_callbacks)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/utils_train/model_utils.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_word\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_word\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = handler.fit_base_model('action_binary_encoded', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'loss', 'predictions', 'labels', 'val_predictions', 'val_labels'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5hcdZ3v+/enq6+5dpMEQneQREUlgiYa4gW34wU0UQeYEREUR2ZzjG6HrR4dNjAqc+QMezM6z+g4w7hBRXREEFGGzDbITXD0IJqI3BIuiYimcyEh5EIufa3v+WOtSld3ujtV6V6pdNXn9Tz11Fq/tdavvtV0+PS6/hQRmJmZlaqu0gWYmdnE4uAwM7OyODjMzKwsDg4zMyuLg8PMzMri4DAzs7I4OMwyJOl6SX9X4rrPSDptrP2YZc3BYWZmZXFwmJlZWRwcVvPSQ0QXS3pE0h5J35R0jKTbJb0g6W5JbUXrnyFptaQdku6TdGLRsoWSHky3+z7QPOSz3iPpoXTb+yW96hBr/oikdZKel7RcUnvaLklflrRF0i5Jj0o6KV32Lklr0to2SPrrQ/qBWc1zcJgl3gucDrwM+FPgduBvgFkk/04+ASDpZcCNwKfSZSuA/5DUKKkR+Hfg34CjgB+k/ZJuuxC4DvgoMAO4BlguqamcQiW9DfhfwDnAscAfgJvSxe8A3px+j+npOtvSZd8EPhoRU4GTgJ+W87lmBQ4Os8Q/R8SzEbEB+Dnwq4j4bUR0AbcCC9P13g/8OCLuiohe4B+AFuCNwOuBBuArEdEbEbcAK4s+YxlwTUT8KiL6I+LbQHe6XTk+CFwXEQ9GRDdwGfAGSXOBXmAq8ApAEfF4RGxKt+sF5kuaFhHbI+LBMj/XDHBwmBU8WzS9b5j5Kel0O8lf+ABERB5YD3SkyzbE4CeH/qFo+njgM+lhqh2SdgDHpduVY2gNu0n2Kjoi4qfAvwBXA1skXStpWrrqe4F3AX+Q9DNJbyjzc80AB4dZuTaSBACQnFMg+Z//BmAT0JG2FbyoaHo9cGVEtBa9JkXEjWOsYTLJoa8NABHx1Yh4LTCf5JDVxWn7yog4Ezia5JDazWV+rhng4DAr183AuyW9XVID8BmSw033A78E+oBPSGqQ9OfA4qJtvw58TNLr0pPYkyW9W9LUMmu4EfhLSQvS8yP/k+TQ2jOSTkn7bwD2AF1APj0H80FJ09NDbLuA/Bh+DlbDHBxmZYiIJ4HzgX8GniM5kf6nEdETET3AnwMXAM+TnA/5UdG2q4CPkBxK2g6sS9ctt4a7gc8DPyTZy3kJcG66eBpJQG0nOZy1DfhSuuxDwDOSdgEfIzlXYlY2eSAnMzMrh/c4zMysLA4OMzMri4PDzMzK4uAwM7Oy1Fe6gMNh5syZMXfu3EqXYWY2ofzmN795LiJmDW2vieCYO3cuq1atqnQZZmYTiqQ/DNfuQ1VmZlYWB4eZmZXFwWFmZmWpiXMcw+nt7aWzs5Ourq5Kl5Kp5uZm5syZQ0NDQ6VLMbMqUbPB0dnZydSpU5k7dy6DH2ZaPSKCbdu20dnZybx58ypdjplViZo9VNXV1cWMGTOqNjQAJDFjxoyq36sys8OrZoMDqOrQKKiF72hmh1dNB8fBbN/bw7bd3ZUuw8zsiOLgGMXOvb1s29OTSd87duzgX//1X8ve7l3vehc7duzIoCIzs9I4OEbRWF9HT1+eLMYsGSk4+vr6Rt1uxYoVtLa2jns9ZmalqtmrqkrRkKsjH0F/PqjPje+5gksvvZTf/e53LFiwgIaGBpqbm2lra+OJJ57gqaee4qyzzmL9+vV0dXXxyU9+kmXLlgEDj0/ZvXs3S5cu5U1vehP3338/HR0d3HbbbbS0tIxrnWZmQzk4gC/8x2rWbNx1QHt/Pujq7aelMUddmSeZ57dP42//9JUjLr/qqqt47LHHeOihh7jvvvt497vfzWOPPbb/stnrrruOo446in379nHKKafw3ve+lxkzZgzqY+3atdx44418/etf55xzzuGHP/wh559/fll1mpmVy8ExikJWRAAZX5y0ePHiQfdafPWrX+XWW28FYP369axdu/aA4Jg3bx4LFiwA4LWvfS3PPPNMtkWameHgABhxz6C3P8/jm3bR3trCzClNmdYwefLk/dP33Xcfd999N7/85S+ZNGkSb3nLW4a9F6OpaaCmXC7Hvn37Mq3RzAx8cnxU9XWiTqK3Pz/ufU+dOpUXXnhh2GU7d+6kra2NSZMm8cQTT/DAAw+M++ebmR2qTIND0hJJT0paJ+nSYZZ/TNKjkh6S9AtJ84uWXZZu96Skd5ba5zjXT0MuubJqvM2YMYNTTz2Vk046iYsvvnjQsiVLltDX18eJJ57IpZdeyutf//px/3wzs0OlLC41BZCUA54CTgc6gZXAeRGxpmidaRGxK50+A/h4RCxJA+RGYDHQDtwNvCzdbNQ+h7No0aIYOpDT448/zoknnnjQ7/H01t3kA1569JSDf+kjVKnf1cysmKTfRMSioe1Z7nEsBtZFxNMR0QPcBJxZvEIhNFKTgUKKnQncFBHdEfF7YF3a30H7HG+NuTp6MjhUZWY2UWUZHB3A+qL5zrRtEEl/Jel3wBeBTxxk25L6TPtdJmmVpFVbt2495C/RUF9HX3+efD6bPTMzs4mm4ifHI+LqiHgJcAnwuXHs99qIWBQRi2bNOmCs9ZI15JIfURYnyM3MJqIsg2MDcFzR/Jy0bSQ3AWcdZNty+xyzRgeHmdkgWQbHSuAESfMkNQLnAsuLV5B0QtHsu4G16fRy4FxJTZLmAScAvy6lz/HWUJ/c+dfT70NVZmaQ4Q2AEdEn6SLgDiAHXBcRqyVdAayKiOXARZJOA3qB7cCH021XS7oZWAP0AX8VEf0Aw/WZ1XcAH6oyMxsq0zvHI2IFsGJI2+VF058cZdsrgStL6TNLdRndy7Fjxw6+973v8fGPf7zsbb/yla+wbNkyJk2aNK41mZmVouInxyeChlzduO9xHOp4HJAEx969e8e1HjOzUvlZVSVozNWxt3f0cTLKVfxY9dNPP52jjz6am2++me7ubv7sz/6ML3zhC+zZs4dzzjmHzs5O+vv7+fznP8+zzz7Lxo0beetb38rMmTO59957x7UuM7ODcXAA3H4pbH50xMXH9PfT2x9EYw6V+pjc2SfD0qtGXFz8WPU777yTW265hV//+tdEBGeccQb/+Z//ydatW2lvb+fHP/4xkDzDavr06fzjP/4j9957LzNnzizra5qZjQcfqipBHYIYuK19vN15553ceeedLFy4kNe85jU88cQTrF27lpNPPpm77rqLSy65hJ///OdMnz49owrMzErnPQ4Ydc8AYN++Xp7ZtoeXzprCpKbx/5FFBJdddhkf/ehHD1j24IMPsmLFCj73uc/x9re/ncsvv3yYHszMDh/vcZSgoT75MY3nM6uKH6v+zne+k+uuu47du3cDsGHDBrZs2cLGjRuZNGkS559/PhdffDEPPvjgAduamR1u3uMoQWM63vh4XllV/Fj1pUuX8oEPfIA3vOENAEyZMoXvfve7rFu3josvvpi6ujoaGhr42te+BsCyZctYsmQJ7e3tPjluZoddZo9VP5KM5bHqBas37qR1UiMdrS3jXV7m/Fh1MzsUlXiselVpyNXRm8GATmZmE42Do0Qel8PMLFHTwVHOYbqG+vG/e/xwqIVDkWZ2eNVscDQ3N7Nt27aS/8fakBP9+aA/P3HCIyLYtm0bzc3NlS7FzKpIzV5VNWfOHDo7Oyl1dMC9Pf08v6cHdjTtf2LuRNDc3MycOXMqXYaZVZGaDY6GhgbmzZtX8vq/+cN2PnLj/XzrglN46yuOzrAyM7Mj28T507nC5rQll+F27thX4UrMzCrLwVGiWVOaaMiJjQ4OM6txmQaHpCWSnpS0TtKlwyz/tKQ1kh6RdI+k49P2t0p6qOjVJemsdNn1kn5ftGxBlt+hoK5OHDu9hQ3bHRxmVtsyO8chKQdcDZwOdAIrJS2PiDVFq/0WWBQReyX9N+CLwPsj4l5gQdrPUcA64M6i7S6OiFuyqn0kHa0tbPAeh5nVuCz3OBYD6yLi6YjoAW4CzixeISLujYjCUHYPAMNd/nM2cHvRehXT3triQ1VmVvOyDI4OYH3RfGfaNpILgduHaT8XuHFI25Xp4a0vS2oarjNJyyStkrSq1EtuD6ajrYVnd3VNyBsBzczGyxFxclzS+cAi4EtD2o8FTgbuKGq+DHgFcApwFHDJcH1GxLURsSgiFs2aNWtc6uxobSYfsHln17j0Z2Y2EWUZHBuA44rm56Rtg0g6DfgscEZEdA9ZfA5wa0T0FhoiYlMkuoFvkRwSOyw6WicB+DyHmdW0LINjJXCCpHmSGkkOOS0vXkHSQuAaktDYMkwf5zHkMFW6F4IkAWcBj2VQ+7DaW5NHd/g8h5nVssyuqoqIPkkXkRxmygHXRcRqSVcAqyJiOcmhqSnAD5Ic4I8RcQaApLkkeyw/G9L1DZJmAQIeAj6W1XcYqj0di8OX5JpZLcv0kSMRsQJYMaTt8qLp00bZ9hmGOZkeEW8bxxLL0tyQY+aURjbudHCYWe06Ik6OTyQdrS10eo/DzGqYg6NMHW2+CdDMapuDo0zt05ObAD1AkpnVKgdHmTraWujqzSdjc5iZ1SAHR5kKV1Zt3OGbAM2sNjk4ytRRuCR3R8UfnWVmVhEOjjINBIf3OMysNjk4ytQ6qYFJjTnfBGhmNcvBUSZJfry6mdU0B8ch8IBOZlbLHByHwDcBmlktc3Acgo7WFp7f08O+nv5Kl2Jmdtg5OA7BwJVV3usws9rj4DgEAzcBOjjMrPY4OA5BR5v3OMysdjk4DsExU5vI1cl7HGZWkxwch6A+V8fsac2+CdDMalKmwSFpiaQnJa2TdOkwyz8taY2kRyTdI+n4omX9kh5KX8uL2udJ+lXa5/fT8cwPu/bWZjq9x2FmNSiz4JCUA64GlgLzgfMkzR+y2m+BRRHxKuAW4ItFy/ZFxIL0dUZR+98DX46IlwLbgQuz+g6j6fDd42ZWo7Lc41gMrIuIpyOiB7gJOLN4hYi4NyIKj5l9AJgzWoeSBLyNJGQAvg2cNa5Vl6ijrYXNO7voz3tAJzOrLVkGRwewvmi+M20byYXA7UXzzZJWSXpAUiEcZgA7IqLvYH1KWpZuv2rr1q2H9g1G0d7aQl8+2PKCn5JrZrWlvtIFAEg6H1gE/ElR8/ERsUHSi4GfSnoU2FlqnxFxLXAtwKJFi8Z9t2D/TYDb93Hs9Jbx7t7M7IiV5R7HBuC4ovk5adsgkk4DPgucERHdhfaI2JC+Pw3cBywEtgGtkgqBN2yfh4PvHjezWpVlcKwETkivgmoEzgWWF68gaSFwDUlobClqb5PUlE7PBE4F1kREAPcCZ6erfhi4LcPvMKJ2B4eZ1ajMgiM9D3ERcAfwOHBzRKyWdIWkwlVSXwKmAD8YctnticAqSQ+TBMVVEbEmXXYJ8GlJ60jOeXwzq+8wmslN9bROavCVVWZWczI9xxERK4AVQ9ouL5o+bYTt7gdOHmHZ0yRXbFVcR2uLbwI0s5rjO8fHoN0DOplZDXJwjEFhjyM59WJmVhscHGMwp62FPT397NrXd/CVzcyqhINjDHxllZnVIgfHGPheDjOrRQ6OMfBIgGZWixwcYzBzSiON9XXe4zCzmuLgGANJyZVVDg4zqyEOjjHyTYBmVmscHGPU3trsPQ4zqykOjjHqaJ3E1he66e7rr3QpZmaHhYNjjDrakiurNu3wgE5mVhscHGPU3toM+JJcM6sdDo4xmtM6CYBOB4eZ1QgHxxjNnt6M5D0OM6sdDo4xaqyv4+ipTb4k18xqhoNjHHhcDjOrJZkGh6Qlkp6UtE7SpcMs/7SkNZIekXSPpOPT9gWSfilpdbrs/UXbXC/p9+lQsw9JWpDldyhFR2uLD1WZWc3ILDgk5YCrgaXAfOA8SfOHrPZbYFFEvAq4Bfhi2r4X+IuIeCWwBPiKpNai7S6OiAXp66GsvkOpkuDoIp/3gE5mVv2y3ONYDKyLiKcjoge4CTizeIWIuDci9qazDwBz0vanImJtOr0R2ALMyrDWMeloa6GnP89ze7orXYqZWeayDI4OYH3RfGfaNpILgduHNkpaDDQCvytqvjI9hPVlSU3DdSZpmaRVklZt3bq1/OrLsH9cDp8gN7MacEScHJd0PrAI+NKQ9mOBfwP+MiLyafNlwCuAU4CjgEuG6zMiro2IRRGxaNasbHdWBsbl8N3jZlb9sgyODcBxRfNz0rZBJJ0GfBY4IyK6i9qnAT8GPhsRDxTaI2JTJLqBb5EcEquowmNHNuzYe5A1zcwmviyDYyVwgqR5khqBc4HlxStIWghcQxIaW4raG4Fbge9ExC1Dtjk2fRdwFvBYht+hJNOaG5jaVO89DjOrCfVZdRwRfZIuAu4AcsB1EbFa0hXAqohYTnJoagrwgyQH+GNEnAGcA7wZmCHpgrTLC9IrqG6QNAsQ8BDwsay+Qzk62lro9DkOM6sBmQUHQESsAFYMabu8aPq0Ebb7LvDdEZa9bTxrHC++CdDMakVJh6okfVLSNCW+KelBSe/IuriJxDcBmlmtKPUcx3+NiF3AO4A24EPAVZlVNQG1t7awc18vu7v7Kl2KmVmmSg0Ope/vAv4tIlYXtRkDV1Z5r8PMql2pwfEbSXeSBMcdkqYC+YNsU1N8E6CZ1YpST45fCCwAno6IvZKOAv4yu7Imnv3B4T0OM6type5xvAF4MiJ2pHd5fw7YmV1ZE8/RU5toyMnBYWZVr9Tg+BqwV9Krgc+QPDfqO5lVNQHV1YnZ05t9jsPMql6pwdEXEUHydNt/iYirganZlTUxdbS2+ByHmVW9UoPjBUmXkVyG+2NJdUBDdmVNTL4J0MxqQanB8X6gm+R+js0kDyz80uib1J45rS08u6uL3n5fcGZm1auk4EjD4gZguqT3AF0R4XMcQ7S3tpAP2LzTDzs0s+pV6iNHzgF+DbyP5AGEv5J0dpaFTUS+CdDMakGp93F8Fjil8Ojz9Om0d5OME24p38thZrWg1HMcdcXjZQDbyti2ZgyMBOjgMLPqVeoex08k3QHcmM6/nyGPSzdobsgxc0qj9zjMrKqVFBwRcbGk9wKnpk3XRsSt2ZU1cbW3ekAnM6tuJQ/kFBE/BH6YYS1VoaO1haeefaHSZZiZZWbU8xSSXpC0a5jXC5J2HaxzSUskPSlpnaRLh1n+aUlrJD0i6R5Jxxct+7Cktenrw0Xtr5X0aNrnV9Oxx48YhZsAkxvtzcyqz6jBERFTI2LaMK+pETFttG0l5YCrgaXAfOA8SfOHrPZbYFFEvIrkCq0vptseBfwt8DpgMfC3ktrSbb4GfAQ4IX0tKeP7Zq6jtYWu3jzb9/ZWuhQzs0xkeWXUYmBdRDwdET3ATSTPutovIu6NiL3p7AMkd6QDvBO4KyKej4jtwF3AEknHAtMi4oH02VnfAc7K8DuUrd3jcphZlcsyODqA9UXznWnbSC4Ebj/Ith3p9EH7lLRM0ipJq7Zu3Vpm6YduTpvv5TCz6nZE3IuRjvGxiHF8/lVEXBsRiyJi0axZs8ar24PyTYBmVu2yDI4NwHFF83PStkEknUZyZ/oZEdF9kG03MHA4a8Q+K6l1UgMtDTnfBGhmVSvL4FgJnCBpnqRG4FxgefEKkhYC15CERvGd6XcA75DUlp4UfwdwR0RsAnZJen16NdVfALdl+B3KJomONo/LYWbVq+T7OMoVEX2SLiIJgRxwXUSslnQFsCoilpMcmpoC/CC9qvaPEXFGRDwv6f8lCR+AKyLi+XT648D1QAvJOZHbOcJ4XA4zq2aZBQdARKxgyKNJIuLyounTRtn2OuC6YdpXASeNY5njrqO1hdUbPCS7mVWnI+LkeLXpaG1m254e9vX0V7oUM7Nx5+DIwP5xOXb6cJWZVR8HRwbap/smQDOrXg6ODHgkQDOrZg6ODMye1kydfBOgmVUnB0cG6nN1zJ7W7OAws6rk4MiIbwI0s2rl4MiIbwI0s2rl4MhIR2sLm3d20Z/3gE5mVl0cHBlpb22hLx9seaGr0qWYmY0rB0dGfEmumVUrB0dGCuNydPoEuZlVGQdHRgrBsXGHD1WZWXVxcGRkclM9rZMa2LBj78FXNjObQBwcGWqf3uI9DjOrOg6ODPkmQDOrRpkGh6Qlkp6UtE7SpcMsf7OkByX1STq7qP2tkh4qenVJOitddr2k3xctW5DldxiLjvQmwAjfy2Fm1SOzEQAl5YCrgdOBTmClpOURsaZotT8CFwB/XbxtRNwLLEj7OQpYB9xZtMrFEXFLVrWPl47WFnZ397Grq4/pLQ2VLsfMbFxkucexGFgXEU9HRA9wE3Bm8QoR8UxEPALkR+nnbOD2iJhwZ5nbWz0uh5lVnyyDowNYXzTfmbaV61zgxiFtV0p6RNKXJTUNt5GkZZJWSVq1devWQ/jYsfNNgGZWjY7ok+OSjgVOBu4oar4MeAVwCnAUcMlw20bEtRGxKCIWzZo1K/Nah9Pe2gx4XA4zqy5ZBscG4Lii+TlpWznOAW6NiN5CQ0RsikQ38C2SQ2JHpJmTm2isr/Meh5lVlSyDYyVwgqR5khpJDjktL7OP8xhymCrdC0GSgLOAx8ah1kzU1YmO1hY6HRxmVkUyC46I6AMuIjnM9Dhwc0SslnSFpDMAJJ0iqRN4H3CNpNWF7SXNJdlj+dmQrm+Q9CjwKDAT+LusvsN4aG9t9slxM6sqmV2OCxARK4AVQ9ouL5peSXIIa7htn2GYk+kR8bbxrTJbHa0t3PdkZU7Om5ll4Yg+OV4N2ltb2PJCN919/ZUuxcxsXDg4MlZ4Su7mnX5mlZlVBwdHxjp8E6CZVRkHR8YKNwH6Xg4zqxYOjozNnu6bAM2sujg4MtZUn+PoqU2+CdDMqoaD4zDoaGvxHoeZVQ0Hx2HQ3uoBncysejg4DoM5rS1s3NlFPu8Bncxs4nNwHAbtrS309OV5bk93pUsxMxszB8dhULiXY+MO3wRoZhOfg+Mw8EiAZlZNHByHgUcCNLNq4uA4DKa3NDC1qd6X5JpZVXBwHCbtrb6Xw8yqg4PjMOlo870cZlYdHByHSXtrs/c4zKwqZBockpZIelLSOkmXDrP8zZIelNQn6ewhy/olPZS+lhe1z5P0q7TP76fjmR/xOlonsXNfL7u7+ypdipnZmGQWHJJywNXAUmA+cJ6k+UNW+yNwAfC9YbrYFxEL0tcZRe1/D3w5Il4KbAcuHPfiM9Demjwl11dWmdlEl+Uex2JgXUQ8HRE9wE3AmcUrRMQzEfEIkC+lQ0kC3gbckjZ9Gzhr/ErOzhyPy2FmVSLL4OgA1hfNd6ZtpWqWtErSA5IK4TAD2BERheM9I/YpaVm6/aqtW7eWW3si3w89ew5t2yF8E6CZVYsj+eT48RGxCPgA8BVJLyln44i4NiIWRcSiWbNmlf/pEfDDC+GmD0J/b/nbD3H01Gbq6+RDVWY24WUZHBuA44rm56RtJYmIDen708B9wEJgG9Aqqf5Q+iyLBC89HZ6+F5Z/IgmSMcjViWN9ZZWZVYEsg2MlcEJ6FVQjcC6w/CDbACCpTVJTOj0TOBVYExEB3AsUrsD6MHDbuFdesPCD8JbL4OHvwX3/a8zdtU/3vRxmNvFlFhzpeYiLgDuAx4GbI2K1pCsknQEg6RRJncD7gGskrU43PxFYJelhkqC4KiLWpMsuAT4taR3JOY9vZvUdAPiTS2Dh+fCzv4cHvzOmrjraWnyoyswmvPqDr3LoImIFsGJI2+VF0ytJDjcN3e5+4OQR+nya5Iqtw0OC93wFdm2C//gUTG2HE047pK46WlvYvKuL3v48Dbkj+fSSmdnI/H+vUuQa4JxvwzHz4Qcfhk0PH1I3Ha0t5AOe3eVxOcxs4nJwlKppKnzgB9DSBje8D3b8sewufEmumVUDB0c5ph0LH/wB9HbBd8+GfdvL2nz/uBw7HRxmNnE5OMp19Ilw7g2w/fdw0/nQV/o44u3TvcdhZhOfg+NQzPsvcNbX4A+/gH//b5Av6YkptDTmmDG5kQ0ee9zMJrBMr6qqaiefDTvXw93/D0yfA6dfUdJmHW0e0MnMJjYHx1ic+inYsR7+v3+C6cfB4o8cdJP26S2s3fLCYSjOzCwbPlQ1FhIs/SK8bCnc/j/giR8fdJPkJsAuYoyPMDEzqxQHx1jl6uHsb8KxC+CWC6Fz1airt7e2sK+3n+17x/7gRDOzSnBwjIfGyfCBm2HqMfC9c2Db70ZctSO9l8OPHjGzicrBMV6mzIIP/jB5iu4NZ8Oe54ZdrRAcnb4k18wmKAfHeJr5UjjvJti1EW48F3r2HrBK4SbAJze/4PMcZjYhOTjG24teB3/+9eRcx48+kowiWKRtUgOzpjbx5buf4q3/cB9X3f4Ej3TucIiY2YShWvgf1qJFi2LVqtFPWo+7B74GP7kUFn8Ulv59cgVW6vk9PdyxejMrHt3E/b/bRn8+6GhtYelJs1l68rEsPK6VujqN0rmZWfYk/SYdiXVwu4MjQz/5G3jganjH38Eb//uwq+zY28Nda57l9sc28/O1W+ntD2ZPa2bJSbNZetJsFs09ipxDxMwqwMFRieDI5+GWC2DNbXD2t+CkPx919V1dvdzz+LPc/uhm7ntqKz19eWZOaWLJScew9KRjed28o6j3OB5mdpg4OCoRHJA8Sfc7Z8LGB+EvboPj31jSZru7+7j3iS385LHN/PSJLezr7adtUgPvfOVslpw0mze+ZCaN9Q4RM8tORYJD0hLgn4Ac8I2IuGrI8jcDXwFeBZwbEbek7QuArwHTgH7gyoj4frrseuBPgJ1pNxdExEOj1VHR4ADY+zx88x2wZytceCfMenlZm+/r6ednT21hxaNJiOzu7mNacz2nz5/Nu06ezZtOmElTfS6j4s2sVh324JCUA54CTgc6gZXAeUVjhyNpLkk4/DWwvCg4XgZERKyV1A78BjgxInakwfF/CuuWouLBAbD9GfjGaVDfAv/XXTB19iF109Xbzy/WPseKxzZx15pneaGrjylN9bz9xKN5x/zZzJ05iRmTmzhqcqP3SMxsTEYKjpWX9k0AAAycSURBVCwfcrgYWJeOEY6km4Azgf3BERHPpMsGPZc8Ip4qmt4oaQswC9iRYb3Zapub3F1+/buTu8vPvxUmzyi7m+aGHKfNP4bT5h9DT1+e+3/3HLc/upk712zmtoc2Dlp3WnM9M6c0MWNKIzMmp+9TmphZPD85aWttafCVXGZWkiyDowNYXzTfCbyu3E4kLQYageLneFwp6XLgHuDSiDhgNCVJy4BlAC960YvK/dhsdLwmOUl+03nwpRcnYTL7VXDsq5NnXR376uQO9BI11tfxlpcfzVtefjRX9p/Eoxt28uyubrbt6Wbb7h627e7muT3J+++27ubXz/SwfW8Pw+1k5upE26TGJFSKgmXmlCamtzQwqTFHS0OO5vTVsn++LnlvzNFcn6MhJyQHkFk1O6Ifqy7pWODfgA9HRGGv5DJgM0mYXAtcAhwwGEZEXJsuZ9GiRUfOFQAvXwIf+Sk8fR9sejh5Pb58YPnU9jRIil7T2gfdBzKc+lwdC1/UdtCP7+vPs31v70C4pMGSTHfzXBo4D2/fwbbdPezu7ivr6+XqVBQwSai0pKHS3JijpRA0DTma6utoyNXROOhdNOQG5hvT6YacaEjnC+s35DSwvLAsV0ddHdRJ1ElIhWn2zzvYzMYmy+DYABxXND8nbSuJpGnAj4HPRsQDhfaI2JROdkv6Fsn5kYmlfWHyKujaCZsfHQiSTQ/D2jugkJWTZhYFSbqH0jbvoGEynPpcHbOmNjFralNJ63f19rNjby9dvf3s6+0f/N6THzRfmN7Xk6err5+unnQ+XbZzXy/P7uynq6+fvT399PTl6e0vvA5ftheHiYpCZfigSaZzdSJXJ+rT94Zc3f75+iHTA+uIXF0ynywr9JGuk0vaC58jij67bqCWQvug2uqG2aZoHQ0JysF9AAz+/sOvl8xraH8UfvUG+ii0Ce3/tRz4/IH24v72TzPwq1y8PcXt6XoUfc7AdGHBgX0M/QwxsMFwNQz3+QfUXrysRv8IyTI4VgInSJpHEhjnAh8oZUNJjcCtwHeGngSXdGxEbFLyX+ws4LHxLbsCmqfD3Dclr4KePfDs6sFhcv8/Qz59HHvT9IEQKbxmvBTqxvfqquaGHLOnZ3/FVkTQ2x/09OfpTQOlJw2U3v48PX35omWxf3lx+PT0Bz19eSKCfAT5gHwEEZDPF88PTOeD4ddP2/rzyfL+fNCfvvf1B335PP35pOb+fDLf05dnb09/2p4s788Hvfk8/f1BXz59pct60+WFemxiGxpWA9MDQVeUWyMH4P7lg1s04szo29768Tfy4llTyvgmB5dZcEREn6SLgDtILse9LiJWS7oCWBURyyWdQhIQbcCfSvpCRLwSOAd4MzBD0gVpl4XLbm+QNIvkZ/UQ8LGsvkNFNU6G4xYnr4K+btjy+OAwWfkN6EvHMM81QUsrNE6BpinJ+/7pydA4tWh6ygjrFS3PHb4jmZJorFdyJVhpO0NVJwaF1oEhFiQ7ofmioIu0ff82+di/XaE9hqwXUdwOQXGAAvvnB2+fT9ct9F3oMwb1Vdw+ZHrotsVtaXBG8c9i/w8mWQ8GPoOiPgvT+7dPZ4avYaD/4j4G6ir0EwPTcZDlw3xm8TrD1cuQWoaedxz6d0Tx8hiy9GAXxk5tbhh9hUPgGwAnuv4+eO6pJES2rEkOe/XsgZ7d0L07eR86Xar6ljREJkN9M9Q3pu/NUN+UBFV908B8fdF8rrGovXnwssK2uQZQ3cCrLpdOF95V1FbcXlhXw7fX1R/SYTwzG6wSl+Pa4ZCrh2PmJ69S5PPQuzcNkT3Q/cII07uh54XkvXdvsrfT153s3fR1w77tRfM9A+19XdB/wEVuh59ySTDVNSTv+6frB9rq6pOA2z/dMPqy4pArBNsBbeUsH691ipfnBrehonkVzY+0TroMRl4HDfNeWMYo6xT3NcI6he0d/Ec0B0etqatLDkk1je8xz0EioL84TLoHh0pfN/TuSx45H3mIwnu+qC1GaB/yGtTenwRjvi85F9Tfm0z39yb1FKYHLesZmO7dly4rbN9TNN07UBcxfC2Fl42jEcLogDY4MIgYZprh24v7G/P0SHUPV8+QGoD9B6oGHQ0aQ9tf/Dsc9WLGk4PDxp80cGiqFsVBgmV/MKbBN2IQldlPcYAOCjjSQIsDt9vfVphnlHWKg7P4nSHrDrdO8XtxPwxeVuiruO/hPu+g2zD88mGnYfi+DzZNieuXUkMwKDyGC5RDbatvYbw5OMzGm5QcMsLPD7Pq5IcZmZlZWRwcZmZWFgeHmZmVxcFhZmZlcXCYmVlZHBxmZlYWB4eZmZXFwWFmZmWpiYccStoK/OEQN58JPDeO5WRtItXrWrMzkeqdSLXCxKp3rLUeHxEHDEtaE8ExFpJWDfd0yCPVRKrXtWZnItU7kWqFiVVvVrX6UJWZmZXFwWFmZmVxcBzctZUuoEwTqV7Xmp2JVO9EqhUmVr2Z1OpzHGZmVhbvcZiZWVkcHGZmVhYHxygkLZH0pKR1ki6tdD0jkXScpHslrZG0WtInK13TwUjKSfqtpP9T6VoORlKrpFskPSHpcUlvqHRNI5H0f6e/A49JulFSc6VrKibpOklbJD1W1HaUpLskrU3f2ypZY7ER6v1S+rvwiKRbJbVWssaC4WotWvYZSSFp5nh8loNjBJJywNXAUmA+cJ6k+ZWtakR9wGciYj7weuCvjuBaCz4JPF7pIkr0T8BPIuIVwKs5QuuW1AF8AlgUESeRDEF4bmWrOsD1wJIhbZcC90TECcA96fyR4noOrPcu4KSIeBXwFHDZ4S5qBNdzYK1IOg54B/DH8fogB8fIFgPrIuLpiOgBbgLOrHBNw4qITRHxYDr9Asn/2DoqW9XIJM0B3g18o9K1HIyk6cCbgW8CRERPROyobFWjqgdaJNUDk4CNFa5nkIj4T+D5Ic1nAt9Op78NnHVYixrFcPVGxJ0R0ZfOPgDMOeyFDWOEny3Al4H/QdEo6WPl4BhZB7C+aL6TI/h/xgWS5gILgV9VtpJRfYXkFzlf6UJKMA/YCnwrPbT2DUmTK13UcCJiA/APJH9ZbgJ2RsSdla2qJMdExKZ0ejNwTCWLKdN/BW6vdBEjkXQmsCEiHh7Pfh0cVUTSFOCHwKciYlel6xmOpPcAWyLiN5WupUT1wGuAr0XEQmAPR9ahlP3ScwNnkoRdOzBZ0vmVrao8kdwfMCHuEZD0WZLDxDdUupbhSJoE/A1w+Xj37eAY2QbguKL5OWnbEUlSA0lo3BARP6p0PaM4FThD0jMkh//eJum7lS1pVJ1AZ0QU9uBuIQmSI9FpwO8jYmtE9AI/At5Y4ZpK8aykYwHS9y0VruegJF0AvAf4YBy5N8O9hOSPiIfTf29zgAclzR5rxw6Oka0ETpA0T1IjyUnG5RWuaViSRHIM/vGI+MdK1zOaiLgsIuZExFySn+lPI+KI/as4IjYD6yW9PG16O7CmgiWN5o/A6yVNSn8n3s4ReiJ/iOXAh9PpDwO3VbCWg5K0hORQ6xkRsbfS9YwkIh6NiKMjYm76760TeE36Oz0mDo4RpCe/LgLuIPnHd3NErK5sVSM6FfgQyV/vD6Wvd1W6qCry34EbJD0CLAD+Z4XrGVa6V3QL8CDwKMm/7yPq8RiSbgR+CbxcUqekC4GrgNMlrSXZa7qqkjUWG6HefwGmAnel/9b+d0WLTI1QazafdeTuZZmZ2ZHIexxmZlYWB4eZmZXFwWFmZmVxcJiZWVkcHGZmVhYHh9kRTtJbJsJThK12ODjMzKwsDg6zcSLpfEm/Tm8KuyYdc2S3pC+nY2TcI2lWuu4CSQ8UjenQlra/VNLdkh6W9KCkl6TdTykaE+SG9M5ws4pwcJiNA0knAu8HTo2IBUA/8EFgMrAqIl4J/Az423ST7wCXpGM6PFrUfgNwdUS8muQ5U4Wnxi4EPkUyNsyLSZ4WYFYR9ZUuwKxKvB14LbAy3RloIXlYXx74frrOd4EfpWN8tEbEz9L2bwM/kDQV6IiIWwEiogsg7e/XEdGZzj8EzAV+kf3XMjuQg8NsfAj4dkQMGg1O0ueHrHeoz/jpLprux/92rYJ8qMpsfNwDnC3paNg/jvbxJP/Gzk7X+QDwi4jYCWyX9F/S9g8BP0tHb+yUdFbaR1M6poLZEcV/tZiNg4hYI+lzwJ2S6oBe4K9IBn5anC7bQnIeBJLHh//vNBieBv4ybf8QcI2kK9I+3ncYv4ZZSfx0XLMMSdodEVMqXYfZePKhKjMzK4v3OMzMrCze4zAzs7I4OMzMrCwODjMzK4uDw8zMyuLgMDOzsvz/N4HEeMowH4UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "##############\n",
    "# Functions to yield batches for the training\n",
    "##############\n",
    "\n",
    "\n",
    "def train_generator(self, batch_size):\n",
    "\n",
    "    train_gen = InputBatchGenerator(self.X_train, self.y_train, batch_size, self.codes, self.sites, self.dim_msg)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            for B in train_gen.generate_msg_count_batches():\n",
    "                yield B\n",
    "        except StopIteration:\n",
    "            logging.warning(\"start over generator loop\")          \n",
    "\n",
    "def test_generator(self, batch_size):\n",
    "\n",
    "    test_gen = InputBatchGenerator(self.X_test, self.y_test, batch_size, self.codes, self.sites, self.dim_msg)\n",
    "    for B in test_gen.generate_msg_count_batches():\n",
    "        yield B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
