{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import InputGenerator\n",
    "import base_model\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import baseline_model\n",
    "import importlib\n",
    "import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import InputBatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'InputGenerator' from '/nfshome/llayer/AIErrorLogAnalysis/training/InputGenerator.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(InputGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'InputBatchGenerator' from '/nfshome/llayer/AIErrorLogAnalysis/training/InputBatchGenerator.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(InputBatchGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = InputGenerator.InputGenerator('/nfshome/llayer/data/actionshistory_300719.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.set_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>errors</th>\n",
       "      <th>parameters</th>\n",
       "      <th>action</th>\n",
       "      <th>action_binary_encoded</th>\n",
       "      <th>splitting</th>\n",
       "      <th>splitting_encoded</th>\n",
       "      <th>xrootd</th>\n",
       "      <th>xrootd_encoded</th>\n",
       "      <th>memory</th>\n",
       "      <th>memory_encoded</th>\n",
       "      <th>action_encoded</th>\n",
       "      <th>action_split</th>\n",
       "      <th>action_split_encoded</th>\n",
       "      <th>action_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>{'good_sites': {}, 'bad_sites': {'-1': {'T3_US...</td>\n",
       "      <td>{'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...</td>\n",
       "      <td>acdc</td>\n",
       "      <td>0</td>\n",
       "      <td>1x</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>acdc_1x</td>\n",
       "      <td>2</td>\n",
       "      <td>acdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>{'good_sites': {}, 'bad_sites': {'-1': {'T3_US...</td>\n",
       "      <td>{'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...</td>\n",
       "      <td>acdc</td>\n",
       "      <td>0</td>\n",
       "      <td>1x</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>acdc_1x</td>\n",
       "      <td>2</td>\n",
       "      <td>acdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...</td>\n",
       "      <td>{'good_sites': {}, 'bad_sites': {'-1': {'T3_US...</td>\n",
       "      <td>{'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...</td>\n",
       "      <td>acdc</td>\n",
       "      <td>0</td>\n",
       "      <td>1x</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>acdc_1x</td>\n",
       "      <td>2</td>\n",
       "      <td>acdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...</td>\n",
       "      <td>{'good_sites': {'85': {'T1_UK_RAL': 1}}, 'bad_...</td>\n",
       "      <td>{'action': 'acdc', 'cores': '', 'xrootd': 'ena...</td>\n",
       "      <td>acdc</td>\n",
       "      <td>0</td>\n",
       "      <td>1x</td>\n",
       "      <td>0</td>\n",
       "      <td>enabled</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>acdc_1x</td>\n",
       "      <td>2</td>\n",
       "      <td>acdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...</td>\n",
       "      <td>{'good_sites': {'50664': {'T2_DE_RWTH': 2}, '-...</td>\n",
       "      <td>{'action': 'acdc', 'cores': '', 'sites': ['T2_...</td>\n",
       "      <td>acdc</td>\n",
       "      <td>0</td>\n",
       "      <td>1x</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>acdc_1x</td>\n",
       "      <td>2</td>\n",
       "      <td>acdc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           task_name  \\\n",
       "0  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...   \n",
       "1  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...   \n",
       "2  /amaltaro_Run2016D-v2-DoubleMuonLowMass-07Aug1...   \n",
       "3  /amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...   \n",
       "4  /amaltaro_Run2018A-v1-DoubleMuon-17Sep2018_102...   \n",
       "\n",
       "                                              errors  \\\n",
       "0  {'good_sites': {}, 'bad_sites': {'-1': {'T3_US...   \n",
       "1  {'good_sites': {}, 'bad_sites': {'-1': {'T3_US...   \n",
       "2  {'good_sites': {}, 'bad_sites': {'-1': {'T3_US...   \n",
       "3  {'good_sites': {'85': {'T1_UK_RAL': 1}}, 'bad_...   \n",
       "4  {'good_sites': {'50664': {'T2_DE_RWTH': 2}, '-...   \n",
       "\n",
       "                                          parameters action  \\\n",
       "0  {'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...   acdc   \n",
       "1  {'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...   acdc   \n",
       "2  {'action': 'acdc', 'sites': ['T1_US_FNAL'], 'm...   acdc   \n",
       "3  {'action': 'acdc', 'cores': '', 'xrootd': 'ena...   acdc   \n",
       "4  {'action': 'acdc', 'cores': '', 'sites': ['T2_...   acdc   \n",
       "\n",
       "   action_binary_encoded splitting  splitting_encoded   xrootd  \\\n",
       "0                      0        1x                  0      NaN   \n",
       "1                      0        1x                  0      NaN   \n",
       "2                      0        1x                  0      NaN   \n",
       "3                      0        1x                  0  enabled   \n",
       "4                      0        1x                  0      NaN   \n",
       "\n",
       "   xrootd_encoded memory  memory_encoded  action_encoded action_split  \\\n",
       "0               2                      3               0      acdc_1x   \n",
       "1               2                      3               0      acdc_1x   \n",
       "2               2                      3               0      acdc_1x   \n",
       "3               0                      3               0      acdc_1x   \n",
       "4               2                      3               0      acdc_1x   \n",
       "\n",
       "   action_split_encoded action_binary  \n",
       "0                     2          acdc  \n",
       "1                     2          acdc  \n",
       "2                     2          acdc  \n",
       "3                     2          acdc  \n",
       "4                     2          acdc  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.actionshistory.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.set_padded_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33586, 77, 154, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.get_input_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'test_generator' from '/nfshome/llayer/AIErrorLogAnalysis/training/test_generator.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.test_count_matrix(gen, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nlp_model' from '/nfshome/llayer/AIErrorLogAnalysis/training/nlp_model.py'>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(nlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(base_model)\n",
    "importlib.reload(baseline_model)\n",
    "importlib.reload(nlp_model) \n",
    "importlib.reload(InputBatchGenerator)\n",
    "\n",
    "class FitHandler(object):\n",
    "    \n",
    "    def __init__(self, gen):\n",
    "        \n",
    "        self.gen = gen\n",
    "        self.dim_tasks, self.dim_errors, self.dim_sites, self.dim_msg = gen.get_input_shape()\n",
    "        self.sites =  gen.sites #gen.sites_to_tiers(gen.sites) # gen.sites\n",
    "        self.codes = gen.codes\n",
    "        #print (self.sites)\n",
    "        \n",
    "        \n",
    "    def print_sample_summary(self, frame):\n",
    "        \n",
    "        print(frame.value_counts())\n",
    "        \n",
    "    \n",
    "    def split_frame(self, label, split_level):\n",
    "        \n",
    "        train, test, _, _ = train_test_split(gen.actionshistory, gen.actionshistory[label], test_size=split_level)\n",
    "        return train, test\n",
    "        \n",
    "    def k_fold_indices(self, kfold_function=KFold, kfold_splits=5, verbose=0):\n",
    "        \n",
    "        enum = enumerate(kfold_function(n_splits=kfold_splits, shuffle=True, random_state=seed).split(X,Y))\n",
    "        if verbose != 0:\n",
    "            enum = tqdm(enum, total=kfold_splits, desc='kfold', leave=False, initial=0)\n",
    "        return enum\n",
    "        \n",
    "    def fit_base_model(self, label, batch_size = 100, test_split=0.2, val_split=0.2, verbose=0):\n",
    "        \n",
    "        \n",
    "        # Split in train and test frames\n",
    "        \"\"\"\n",
    "        train, test = self.split_frame(label, test_split)\n",
    "\n",
    "        \n",
    "        #self.print_sample_summary(train[label]) \n",
    "        #self.print_sample_summary(test[label]) \n",
    "        \n",
    "        # Setup the generator of the input batches\n",
    "        \n",
    "        train_gen = InputBatchGenerator.InputBatchGenerator(train, label, self.codes, self.sites, self.dim_msg)\n",
    "        test_gen = InputBatchGenerator.InputBatchGenerator(test, label, self.codes, self.sites, self.dim_msg)        \n",
    "        X_train, y_train = train_gen.count_matrix(sum_good_bad = True)\n",
    "        X_test, y_test = test_gen.count_matrix(sum_good_bad = True)\n",
    "        \n",
    "        print(type(y_test))\n",
    "        \n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        #print(np.unique(y_train), return_counts = True)\n",
    "        #print(np.unique(y_test), return_counts = True)\n",
    "        \n",
    "        uniqueValues, occurCount = np.unique(y_test, return_counts=True)\n",
    "        print(uniqueValues, occurCount)\n",
    "        # Set the baseline model\n",
    "        n_sites = len(list(set(self.sites.values())))\n",
    "        \"\"\"\n",
    "        cv_gen = InputBatchGenerator.InputBatchGenerator(gen.actionshistory, label, self.codes, self.sites, self.dim_msg)\n",
    "        X, y = cv_gen.count_matrix(sum_good_bad = True)\n",
    "        \n",
    "        model = baseline_model.FF(2, self.dim_errors, self.dim_sites)\n",
    "        \n",
    "        #cv_score = model.kfold_val(X, y, kfold_splits=5, max_epochs = 200, batch_size = 100, early_stopping = True)\n",
    "        \n",
    "        #return cv_score\n",
    "        #history = model.train(X_train, y_train, X_test, y_test, max_epochs = 200, batch_size = 100, early_stopping = True)\n",
    "        \n",
    "        #return history\n",
    "        \n",
    "        \n",
    "        model.find_optimal_parameters(X, y, 5, max_epochs = 100, batch_size = 100, num_calls=20)\n",
    "\n",
    "        \n",
    "    def fit_nlp_model(self, label, batch_size = 1, test_split=0.2, val_split=0.2, verbose=1):\n",
    "\n",
    "        # Set the baseline model\n",
    "        model = nlp_model.NLP_Model(2, self.dim_errors, self.dim_sites, self.dim_msg)            \n",
    "        #model.print_summary()\n",
    "        \n",
    "        # Split in train and test frames\n",
    "        train, test = self.split_frame(label, test_split)\n",
    "        \n",
    "        # Setup the generator of the input batches\n",
    "        train_gen = InputBatchGenerator.InputBatchGenerator(train, label, \\\n",
    "                                                            batch_size, self.codes, self.sites, self.dim_msg)\n",
    "        test_gen = InputBatchGenerator.InputBatchGenerator(test, label, \\\n",
    "                                                            batch_size, self.codes, self.sites, self.dim_msg) \n",
    "        \n",
    "        \"\"\"\n",
    "        for x,y in train_gen.gen_inf_count_msg_batches():\n",
    "            print( x[0].shape )\n",
    "        \"\"\"\n",
    "        \n",
    "        steps_per_epoch = int(float(len(train)) / float(batch_size))\n",
    "        validation_steps = int(float(len(test)) / float(batch_size))\n",
    "        \n",
    "        model.train_on_batch(training_generator = train_gen, validation_generator = test_gen, \n",
    "                             epochs = 1, steps_per_epoch = steps_per_epoch, validation_steps = validation_steps )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = FitHandler(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t ::: 1 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.001, 'dense_units': 20, 'dense_layers': 3}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_98 (InputLayer)        (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_98 (Flatten)         (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_389 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_292 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_390 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_293 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_391 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_294 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_392 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,041\n",
      "Trainable params: 238,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 15s 565us/step - loss: 0.3555 - acc: 0.9476 - val_loss: 0.2411 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 3s 118us/step - loss: 0.2369 - acc: 0.9477 - val_loss: 0.2230 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 3s 116us/step - loss: 0.2231 - acc: 0.9474 - val_loss: 0.2191 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 3s 118us/step - loss: 0.2146 - acc: 0.9477 - val_loss: 0.2167 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 3s 123us/step - loss: 0.2102 - acc: 0.9477 - val_loss: 0.2145 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 3s 123us/step - loss: 0.2089 - acc: 0.9477 - val_loss: 0.2154 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 3s 120us/step - loss: 0.2068 - acc: 0.9477 - val_loss: 0.2139 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 3s 116us/step - loss: 0.2068 - acc: 0.9477 - val_loss: 0.2156 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 3s 119us/step - loss: 0.2051 - acc: 0.9477 - val_loss: 0.2141 - val_acc: 0.9489\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 4s 131us/step - loss: 0.2047 - acc: 0.9477 - val_loss: 0.2110 - val_acc: 0.9489\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 3s 128us/step - loss: 0.2034 - acc: 0.9478 - val_loss: 0.2173 - val_acc: 0.9489\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 3s 122us/step - loss: 0.2044 - acc: 0.9477 - val_loss: 0.2139 - val_acc: 0.9489\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 3s 120us/step - loss: 0.2024 - acc: 0.9477 - val_loss: 0.2143 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 3s 125us/step - loss: 0.2027 - acc: 0.9476 - val_loss: 0.2185 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 3s 129us/step - loss: 0.2007 - acc: 0.9476 - val_loss: 0.2213 - val_acc: 0.9489\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_99 (InputLayer)        (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_99 (Flatten)         (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_393 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_295 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_394 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_296 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_395 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_297 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_396 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,041\n",
      "Trainable params: 238,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 15s 543us/step - loss: 0.3475 - acc: 0.9481 - val_loss: 0.2495 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 120us/step - loss: 0.2336 - acc: 0.9480 - val_loss: 0.2288 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 119us/step - loss: 0.2212 - acc: 0.9480 - val_loss: 0.2238 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2139 - acc: 0.9481 - val_loss: 0.2207 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2098 - acc: 0.9480 - val_loss: 0.2175 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2076 - acc: 0.9481 - val_loss: 0.2184 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2053 - acc: 0.9481 - val_loss: 0.2187 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2035 - acc: 0.9481 - val_loss: 0.2142 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 122us/step - loss: 0.2049 - acc: 0.9480 - val_loss: 0.2169 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 121us/step - loss: 0.2043 - acc: 0.9481 - val_loss: 0.2114 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 3s 119us/step - loss: 0.2028 - acc: 0.9481 - val_loss: 0.2139 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2020 - acc: 0.9481 - val_loss: 0.2115 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2011 - acc: 0.9481 - val_loss: 0.2135 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2004 - acc: 0.9481 - val_loss: 0.2109 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 3s 118us/step - loss: 0.2003 - acc: 0.9481 - val_loss: 0.2121 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 3s 120us/step - loss: 0.1998 - acc: 0.9481 - val_loss: 0.2124 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.1995 - acc: 0.9481 - val_loss: 0.2128 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 3s 118us/step - loss: 0.2008 - acc: 0.9481 - val_loss: 0.2220 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 3s 117us/step - loss: 0.2008 - acc: 0.9481 - val_loss: 0.2181 - val_acc: 0.9476\n",
      "Epoch 00019: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_100 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_100 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_397 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_298 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_398 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_299 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_399 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_300 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_400 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,041\n",
      "Trainable params: 238,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 15s 559us/step - loss: 0.3564 - acc: 0.9485 - val_loss: 0.2708 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2357 - acc: 0.9488 - val_loss: 0.2515 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2217 - acc: 0.9488 - val_loss: 0.2513 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2124 - acc: 0.9487 - val_loss: 0.2421 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2096 - acc: 0.9487 - val_loss: 0.2453 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2056 - acc: 0.9488 - val_loss: 0.2365 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2037 - acc: 0.9487 - val_loss: 0.2352 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2365 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 119us/step - loss: 0.2035 - acc: 0.9487 - val_loss: 0.2356 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 119us/step - loss: 0.2046 - acc: 0.9488 - val_loss: 0.2359 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2009 - acc: 0.9488 - val_loss: 0.2384 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 130us/step - loss: 0.2036 - acc: 0.9488 - val_loss: 0.2453 - val_acc: 0.9449\n",
      "Epoch 00012: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_101 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_101 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_401 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_301 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_402 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_302 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_403 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_303 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_404 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,041\n",
      "Trainable params: 238,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 15s 560us/step - loss: 0.3949 - acc: 0.9403 - val_loss: 0.2409 - val_acc: 0.9528\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 112us/step - loss: 0.2513 - acc: 0.9458 - val_loss: 0.2265 - val_acc: 0.9531\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 110us/step - loss: 0.2339 - acc: 0.9466 - val_loss: 0.2222 - val_acc: 0.9531\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 113us/step - loss: 0.2238 - acc: 0.9464 - val_loss: 0.2188 - val_acc: 0.9525\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 119us/step - loss: 0.2190 - acc: 0.9467 - val_loss: 0.2156 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 109us/step - loss: 0.2156 - acc: 0.9469 - val_loss: 0.2142 - val_acc: 0.9533\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 109us/step - loss: 0.2136 - acc: 0.9469 - val_loss: 0.2104 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 108us/step - loss: 0.2103 - acc: 0.9467 - val_loss: 0.2087 - val_acc: 0.9533\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 107us/step - loss: 0.2108 - acc: 0.9465 - val_loss: 0.2060 - val_acc: 0.9533\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 117us/step - loss: 0.2082 - acc: 0.9467 - val_loss: 0.2152 - val_acc: 0.9527\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 3s 115us/step - loss: 0.2100 - acc: 0.9466 - val_loss: 0.2073 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 3s 113us/step - loss: 0.2041 - acc: 0.9467 - val_loss: 0.2048 - val_acc: 0.9533\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 3s 112us/step - loss: 0.2052 - acc: 0.9467 - val_loss: 0.2087 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 3s 115us/step - loss: 0.2028 - acc: 0.9468 - val_loss: 0.2050 - val_acc: 0.9533\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 3s 115us/step - loss: 0.2039 - acc: 0.9468 - val_loss: 0.2052 - val_acc: 0.9534\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 3s 119us/step - loss: 0.2009 - acc: 0.9467 - val_loss: 0.2083 - val_acc: 0.9533\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2020 - acc: 0.9468 - val_loss: 0.2114 - val_acc: 0.9533\n",
      "Epoch 00017: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_102 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_102 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_405 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_304 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_406 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_305 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_407 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_306 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_408 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,041\n",
      "Trainable params: 238,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 14s 537us/step - loss: 0.3556 - acc: 0.9478 - val_loss: 0.2662 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 110us/step - loss: 0.2364 - acc: 0.9485 - val_loss: 0.2480 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 112us/step - loss: 0.2214 - acc: 0.9481 - val_loss: 0.2460 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 112us/step - loss: 0.2144 - acc: 0.9486 - val_loss: 0.2454 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 115us/step - loss: 0.2097 - acc: 0.9487 - val_loss: 0.2439 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2074 - acc: 0.9487 - val_loss: 0.2471 - val_acc: 0.9448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2042 - acc: 0.9487 - val_loss: 0.2461 - val_acc: 0.9446\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 119us/step - loss: 0.2025 - acc: 0.9483 - val_loss: 0.2437 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 117us/step - loss: 0.2019 - acc: 0.9488 - val_loss: 0.2399 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 119us/step - loss: 0.2006 - acc: 0.9485 - val_loss: 0.2441 - val_acc: 0.9451\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2023 - acc: 0.9488 - val_loss: 0.2478 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 3s 109us/step - loss: 0.2000 - acc: 0.9487 - val_loss: 0.2435 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 3s 111us/step - loss: 0.2019 - acc: 0.9487 - val_loss: 0.2394 - val_acc: 0.9448\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2002 - acc: 0.9487 - val_loss: 0.2445 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.1984 - acc: 0.9488 - val_loss: 0.2439 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 3s 108us/step - loss: 0.1988 - acc: 0.9488 - val_loss: 0.2516 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 3s 118us/step - loss: 0.1983 - acc: 0.9488 - val_loss: 0.2480 - val_acc: 0.9448\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.1989 - acc: 0.9488 - val_loss: 0.2425 - val_acc: 0.9449\n",
      "Epoch 00018: early stopping\n",
      "-0.7305846726159501 0.024563345345583586\n",
      "\n",
      " \t ::: 2 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.03192360647756576, 'dense_units': 34, 'dense_layers': 6}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_103 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_103 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_409 (Dense)            (None, 34)                403206    \n",
      "_________________________________________________________________\n",
      "dropout_307 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_410 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_308 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_411 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_309 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_412 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_310 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_413 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_311 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_414 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_312 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_415 (Dense)            (None, 1)                 35        \n",
      "=================================================================\n",
      "Total params: 409,191\n",
      "Trainable params: 409,191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 16s 613us/step - loss: 0.2401 - acc: 0.9466 - val_loss: 0.2040 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 145us/step - loss: 0.2079 - acc: 0.9477 - val_loss: 0.2021 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 4s 157us/step - loss: 0.2076 - acc: 0.9477 - val_loss: 0.2019 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 142us/step - loss: 0.2076 - acc: 0.9477 - val_loss: 0.2051 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 4s 132us/step - loss: 0.2068 - acc: 0.9477 - val_loss: 0.2037 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 4s 131us/step - loss: 0.2082 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 150us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 5s 201us/step - loss: 0.2056 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2030 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 4s 148us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2013 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2055 - acc: 0.9477 - val_loss: 0.2022 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 160us/step - loss: 0.2060 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 153us/step - loss: 0.2060 - acc: 0.9477 - val_loss: 0.2018 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 4s 139us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 4s 137us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2024 - val_acc: 0.9491\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_104 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_104 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_416 (Dense)            (None, 34)                403206    \n",
      "_________________________________________________________________\n",
      "dropout_313 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_417 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_314 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_418 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_315 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_419 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_316 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_420 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_317 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_421 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_318 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_422 (Dense)            (None, 1)                 35        \n",
      "=================================================================\n",
      "Total params: 409,191\n",
      "Trainable params: 409,191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 16s 592us/step - loss: 0.2420 - acc: 0.9472 - val_loss: 0.2077 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2074 - acc: 0.9481 - val_loss: 0.2067 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2073 - acc: 0.9481 - val_loss: 0.2064 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2055 - acc: 0.9481 - val_loss: 0.2081 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2063 - acc: 0.9481 - val_loss: 0.2079 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2068 - acc: 0.9481 - val_loss: 0.2116 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2049 - acc: 0.9481 - val_loss: 0.2057 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2045 - acc: 0.9481 - val_loss: 0.2058 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2050 - acc: 0.9481 - val_loss: 0.2065 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2051 - acc: 0.9481 - val_loss: 0.2057 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2044 - acc: 0.9481 - val_loss: 0.2065 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2048 - acc: 0.9481 - val_loss: 0.2063 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2047 - acc: 0.9481 - val_loss: 0.2056 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2046 - acc: 0.9481 - val_loss: 0.2063 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2049 - acc: 0.9481 - val_loss: 0.2066 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2047 - acc: 0.9481 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2047 - acc: 0.9481 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2049 - acc: 0.9481 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 00018: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_105 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_105 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_423 (Dense)            (None, 34)                403206    \n",
      "_________________________________________________________________\n",
      "dropout_319 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_424 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_320 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_425 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_321 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_426 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_322 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_427 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_323 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_428 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_324 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_429 (Dense)            (None, 1)                 35        \n",
      "=================================================================\n",
      "Total params: 409,191\n",
      "Trainable params: 409,191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 16s 587us/step - loss: 0.2476 - acc: 0.9454 - val_loss: 0.2193 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2048 - acc: 0.9488 - val_loss: 0.2150 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2036 - acc: 0.9488 - val_loss: 0.2231 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2043 - acc: 0.9488 - val_loss: 0.2158 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2047 - acc: 0.9488 - val_loss: 0.2149 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2049 - acc: 0.9488 - val_loss: 0.2147 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2135 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2027 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2140 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2142 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2136 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2028 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2143 - val_acc: 0.9449\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_106 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_106 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_430 (Dense)            (None, 34)                403206    \n",
      "_________________________________________________________________\n",
      "dropout_325 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_431 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_326 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_432 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_327 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_433 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_328 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_434 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_329 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_435 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_330 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_436 (Dense)            (None, 1)                 35        \n",
      "=================================================================\n",
      "Total params: 409,191\n",
      "Trainable params: 409,191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 16s 598us/step - loss: 0.2421 - acc: 0.9462 - val_loss: 0.1953 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2107 - acc: 0.9466 - val_loss: 0.1895 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2113 - acc: 0.9466 - val_loss: 0.1942 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2105 - acc: 0.9466 - val_loss: 0.1891 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2103 - acc: 0.9466 - val_loss: 0.1925 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2107 - acc: 0.9466 - val_loss: 0.1891 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2089 - acc: 0.9466 - val_loss: 0.1887 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2086 - acc: 0.9466 - val_loss: 0.1894 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.1893 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.1889 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2086 - acc: 0.9466 - val_loss: 0.1900 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2090 - acc: 0.9466 - val_loss: 0.1895 - val_acc: 0.9534\n",
      "Epoch 00012: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_107 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_107 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_437 (Dense)            (None, 34)                403206    \n",
      "_________________________________________________________________\n",
      "dropout_331 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_438 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_332 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_439 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_333 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_440 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_334 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_441 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_335 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_442 (Dense)            (None, 34)                1190      \n",
      "_________________________________________________________________\n",
      "dropout_336 (Dropout)        (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_443 (Dense)            (None, 1)                 35        \n",
      "=================================================================\n",
      "Total params: 409,191\n",
      "Trainable params: 409,191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 18s 666us/step - loss: 0.2345 - acc: 0.9486 - val_loss: 0.2189 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2051 - acc: 0.9488 - val_loss: 0.2166 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2045 - acc: 0.9488 - val_loss: 0.2162 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2049 - acc: 0.9488 - val_loss: 0.2138 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2044 - acc: 0.9488 - val_loss: 0.2162 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2044 - acc: 0.9488 - val_loss: 0.2150 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2027 - acc: 0.9488 - val_loss: 0.2140 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2154 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2145 - val_acc: 0.9449\n",
      "Epoch 00009: early stopping\n",
      "-0.49996853669277874 9.415622349501919e-05\n",
      "\n",
      " \t ::: 3 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.004128933325313341, 'dense_units': 57, 'dense_layers': 3}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_108 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_108 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_444 (Dense)            (None, 57)                675963    \n",
      "_________________________________________________________________\n",
      "dropout_337 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_445 (Dense)            (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dropout_338 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_446 (Dense)            (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dropout_339 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_447 (Dense)            (None, 1)                 58        \n",
      "=================================================================\n",
      "Total params: 682,633\n",
      "Trainable params: 682,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 16s 600us/step - loss: 0.2937 - acc: 0.9457 - val_loss: 0.2378 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 3s 114us/step - loss: 0.2369 - acc: 0.9477 - val_loss: 0.2217 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 3s 117us/step - loss: 0.2293 - acc: 0.9477 - val_loss: 0.2284 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 3s 114us/step - loss: 0.2287 - acc: 0.9477 - val_loss: 0.2235 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 3s 115us/step - loss: 0.2265 - acc: 0.9477 - val_loss: 0.2238 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 3s 117us/step - loss: 0.2220 - acc: 0.9477 - val_loss: 0.2210 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 3s 125us/step - loss: 0.2209 - acc: 0.9477 - val_loss: 0.2134 - val_acc: 0.9491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 3s 117us/step - loss: 0.2154 - acc: 0.9477 - val_loss: 0.2138 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 3s 113us/step - loss: 0.2191 - acc: 0.9477 - val_loss: 0.2169 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 3s 114us/step - loss: 0.2164 - acc: 0.9477 - val_loss: 0.2149 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 3s 114us/step - loss: 0.2184 - acc: 0.9477 - val_loss: 0.2168 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 3s 112us/step - loss: 0.2147 - acc: 0.9477 - val_loss: 0.2157 - val_acc: 0.9491\n",
      "Epoch 00012: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_109 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_109 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_448 (Dense)            (None, 57)                675963    \n",
      "_________________________________________________________________\n",
      "dropout_340 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_449 (Dense)            (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dropout_341 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_450 (Dense)            (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dropout_342 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_451 (Dense)            (None, 1)                 58        \n",
      "=================================================================\n",
      "Total params: 682,633\n",
      "Trainable params: 682,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 16s 605us/step - loss: 0.2945 - acc: 0.9468 - val_loss: 0.2333 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2308 - acc: 0.9481 - val_loss: 0.2265 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2257 - acc: 0.9481 - val_loss: 0.2260 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2226 - acc: 0.9481 - val_loss: 0.2252 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2214 - acc: 0.9481 - val_loss: 0.2202 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2202 - acc: 0.9481 - val_loss: 0.2166 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2161 - acc: 0.9481 - val_loss: 0.2248 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2188 - acc: 0.9481 - val_loss: 0.2213 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2193 - acc: 0.9481 - val_loss: 0.2196 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2179 - acc: 0.9481 - val_loss: 0.2188 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2166 - acc: 0.9481 - val_loss: 0.2154 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2179 - acc: 0.9481 - val_loss: 0.2266 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2156 - acc: 0.9481 - val_loss: 0.2150 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2105 - acc: 0.9481 - val_loss: 0.2083 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2119 - acc: 0.9481 - val_loss: 0.2145 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2114 - acc: 0.9481 - val_loss: 0.2153 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2150 - acc: 0.9481 - val_loss: 0.2165 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2113 - acc: 0.9481 - val_loss: 0.2131 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2112 - acc: 0.9481 - val_loss: 0.2174 - val_acc: 0.9476\n",
      "Epoch 00019: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_110 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_110 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_452 (Dense)            (None, 57)                675963    \n",
      "_________________________________________________________________\n",
      "dropout_343 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_453 (Dense)            (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dropout_344 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_454 (Dense)            (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dropout_345 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_455 (Dense)            (None, 1)                 58        \n",
      "=================================================================\n",
      "Total params: 682,633\n",
      "Trainable params: 682,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 17s 647us/step - loss: 0.2935 - acc: 0.9472 - val_loss: 0.2518 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 180us/step - loss: 0.2328 - acc: 0.9487 - val_loss: 0.2418 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2229 - acc: 0.9488 - val_loss: 0.2350 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2220 - acc: 0.9488 - val_loss: 0.2391 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2217 - acc: 0.9488 - val_loss: 0.2347 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2203 - acc: 0.9488 - val_loss: 0.2453 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2214 - acc: 0.9488 - val_loss: 0.2349 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 130us/step - loss: 0.2183 - acc: 0.9488 - val_loss: 0.2295 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2159 - acc: 0.9488 - val_loss: 0.2308 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 130us/step - loss: 0.2157 - acc: 0.9488 - val_loss: 0.2331 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2137 - acc: 0.9488 - val_loss: 0.2281 - val_acc: 0.9449\n",
      "Epoch 12/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2122 - acc: 0.9488 - val_loss: 0.2293 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2105 - acc: 0.9488 - val_loss: 0.2320 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2130 - acc: 0.9488 - val_loss: 0.2299 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2150 - acc: 0.9488 - val_loss: 0.2341 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2130 - acc: 0.9488 - val_loss: 0.2266 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2084 - acc: 0.9488 - val_loss: 0.2277 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2112 - acc: 0.9488 - val_loss: 0.2280 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2071 - acc: 0.9488 - val_loss: 0.2258 - val_acc: 0.9449\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2073 - acc: 0.9488 - val_loss: 0.2302 - val_acc: 0.9449\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2059 - acc: 0.9488 - val_loss: 0.2236 - val_acc: 0.9449\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2114 - acc: 0.9488 - val_loss: 0.2268 - val_acc: 0.9449\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2089 - acc: 0.9487 - val_loss: 0.2274 - val_acc: 0.9449\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2096 - acc: 0.9488 - val_loss: 0.2247 - val_acc: 0.9449\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2071 - acc: 0.9488 - val_loss: 0.2235 - val_acc: 0.9449\n",
      "Epoch 26/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2050 - acc: 0.9488 - val_loss: 0.2231 - val_acc: 0.9449\n",
      "Epoch 27/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2066 - acc: 0.9488 - val_loss: 0.2300 - val_acc: 0.9449\n",
      "Epoch 28/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2087 - acc: 0.9488 - val_loss: 0.2206 - val_acc: 0.9449\n",
      "Epoch 29/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2038 - acc: 0.9488 - val_loss: 0.2172 - val_acc: 0.9449\n",
      "Epoch 30/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2043 - acc: 0.9488 - val_loss: 0.2227 - val_acc: 0.9449\n",
      "Epoch 31/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2055 - acc: 0.9488 - val_loss: 0.2211 - val_acc: 0.9449\n",
      "Epoch 32/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2269 - val_acc: 0.9449\n",
      "Epoch 33/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2234 - val_acc: 0.9449\n",
      "Epoch 34/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2013 - acc: 0.9488 - val_loss: 0.2230 - val_acc: 0.9449\n",
      "Epoch 00034: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_111 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_111 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_456 (Dense)            (None, 57)                675963    \n",
      "_________________________________________________________________\n",
      "dropout_346 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_457 (Dense)            (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dropout_347 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_458 (Dense)            (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dropout_348 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_459 (Dense)            (None, 1)                 58        \n",
      "=================================================================\n",
      "Total params: 682,633\n",
      "Trainable params: 682,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 20s 753us/step - loss: 0.2919 - acc: 0.9464 - val_loss: 0.2286 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2382 - acc: 0.9466 - val_loss: 0.2224 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 130us/step - loss: 0.2317 - acc: 0.9466 - val_loss: 0.2189 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2291 - acc: 0.9466 - val_loss: 0.2178 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2287 - acc: 0.9466 - val_loss: 0.2155 - val_acc: 0.9533\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2264 - acc: 0.9466 - val_loss: 0.2166 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2275 - acc: 0.9466 - val_loss: 0.2170 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2239 - acc: 0.9466 - val_loss: 0.2170 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2236 - acc: 0.9466 - val_loss: 0.2061 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2179 - acc: 0.9466 - val_loss: 0.2066 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2181 - acc: 0.9466 - val_loss: 0.2046 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2162 - acc: 0.9466 - val_loss: 0.2138 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2229 - acc: 0.9466 - val_loss: 0.2069 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2220 - acc: 0.9466 - val_loss: 0.2112 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2169 - acc: 0.9466 - val_loss: 0.2082 - val_acc: 0.9534\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2167 - acc: 0.9466 - val_loss: 0.2090 - val_acc: 0.9534\n",
      "Epoch 00016: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_112 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_112 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_460 (Dense)            (None, 57)                675963    \n",
      "_________________________________________________________________\n",
      "dropout_349 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_461 (Dense)            (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dropout_350 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_462 (Dense)            (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dropout_351 (Dropout)        (None, 57)                0         \n",
      "_________________________________________________________________\n",
      "dense_463 (Dense)            (None, 1)                 58        \n",
      "=================================================================\n",
      "Total params: 682,633\n",
      "Trainable params: 682,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 19s 720us/step - loss: 0.3008 - acc: 0.9474 - val_loss: 0.2518 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2367 - acc: 0.9488 - val_loss: 0.2424 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 117us/step - loss: 0.2257 - acc: 0.9486 - val_loss: 0.2448 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 116us/step - loss: 0.2274 - acc: 0.9488 - val_loss: 0.2410 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2254 - acc: 0.9488 - val_loss: 0.2404 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2254 - acc: 0.9488 - val_loss: 0.2377 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2255 - acc: 0.9487 - val_loss: 0.2449 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2199 - acc: 0.9488 - val_loss: 0.2322 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 130us/step - loss: 0.2127 - acc: 0.9488 - val_loss: 0.2274 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 122us/step - loss: 0.2129 - acc: 0.9488 - val_loss: 0.2400 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 3s 118us/step - loss: 0.2147 - acc: 0.9488 - val_loss: 0.2345 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 3s 120us/step - loss: 0.2192 - acc: 0.9488 - val_loss: 0.2332 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2132 - acc: 0.9488 - val_loss: 0.2347 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2108 - acc: 0.9488 - val_loss: 0.2294 - val_acc: 0.9449\n",
      "Epoch 00014: early stopping\n",
      "-0.6982475992980132 0.016070622253019196\n",
      "\n",
      " \t ::: 4 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.015644746533347166, 'dense_units': 47, 'dense_layers': 3}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_113 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_113 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_464 (Dense)            (None, 47)                557373    \n",
      "_________________________________________________________________\n",
      "dropout_352 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_465 (Dense)            (None, 47)                2256      \n",
      "_________________________________________________________________\n",
      "dropout_353 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_466 (Dense)            (None, 47)                2256      \n",
      "_________________________________________________________________\n",
      "dropout_354 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_467 (Dense)            (None, 1)                 48        \n",
      "=================================================================\n",
      "Total params: 561,933\n",
      "Trainable params: 561,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 19s 705us/step - loss: 0.3005 - acc: 0.9441 - val_loss: 0.2650 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 136us/step - loss: 0.2771 - acc: 0.9476 - val_loss: 0.2559 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2601 - acc: 0.9476 - val_loss: 0.2326 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 136us/step - loss: 0.2456 - acc: 0.9477 - val_loss: 0.2309 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 4s 144us/step - loss: 0.2305 - acc: 0.9477 - val_loss: 0.2146 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 4s 135us/step - loss: 0.2209 - acc: 0.9477 - val_loss: 0.2143 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 135us/step - loss: 0.2235 - acc: 0.9477 - val_loss: 0.2233 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 139us/step - loss: 0.2216 - acc: 0.9477 - val_loss: 0.2142 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 4s 154us/step - loss: 0.2146 - acc: 0.9477 - val_loss: 0.2067 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2108 - acc: 0.9477 - val_loss: 0.2058 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 3s 130us/step - loss: 0.2087 - acc: 0.9477 - val_loss: 0.2031 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 134us/step - loss: 0.2101 - acc: 0.9477 - val_loss: 0.2050 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 137us/step - loss: 0.2067 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 4s 136us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 3s 126us/step - loss: 0.2056 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 3s 126us/step - loss: 0.2056 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 3s 129us/step - loss: 0.2055 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 4s 135us/step - loss: 0.2055 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 19/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2054 - acc: 0.9477 - val_loss: 0.2013 - val_acc: 0.9491\n",
      "Epoch 20/200\n",
      "26868/26868 [==============================] - 4s 131us/step - loss: 0.2055 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 21/200\n",
      "26868/26868 [==============================] - 3s 125us/step - loss: 0.2054 - acc: 0.9477 - val_loss: 0.2013 - val_acc: 0.9491\n",
      "Epoch 22/200\n",
      "26868/26868 [==============================] - 3s 127us/step - loss: 0.2054 - acc: 0.9477 - val_loss: 0.2013 - val_acc: 0.9491\n",
      "Epoch 23/200\n",
      "26868/26868 [==============================] - 3s 123us/step - loss: 0.2054 - acc: 0.9477 - val_loss: 0.2013 - val_acc: 0.9491\n",
      "Epoch 24/200\n",
      "26868/26868 [==============================] - 3s 128us/step - loss: 0.2054 - acc: 0.9477 - val_loss: 0.2013 - val_acc: 0.9491\n",
      "Epoch 25/200\n",
      "26868/26868 [==============================] - 4s 131us/step - loss: 0.2054 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 26/200\n",
      "26868/26868 [==============================] - 4s 134us/step - loss: 0.2054 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 00026: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_114 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_114 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_468 (Dense)            (None, 47)                557373    \n",
      "_________________________________________________________________\n",
      "dropout_355 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_469 (Dense)            (None, 47)                2256      \n",
      "_________________________________________________________________\n",
      "dropout_356 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_470 (Dense)            (None, 47)                2256      \n",
      "_________________________________________________________________\n",
      "dropout_357 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_471 (Dense)            (None, 1)                 48        \n",
      "=================================================================\n",
      "Total params: 561,933\n",
      "Trainable params: 561,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 20s 738us/step - loss: 0.3018 - acc: 0.9457 - val_loss: 0.2549 - val_acc: 0.9474\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2567 - acc: 0.9479 - val_loss: 0.2594 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 130us/step - loss: 0.2378 - acc: 0.9481 - val_loss: 0.2383 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2325 - acc: 0.9481 - val_loss: 0.2600 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2406 - acc: 0.9481 - val_loss: 0.2293 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2222 - acc: 0.9481 - val_loss: 0.2256 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2292 - acc: 0.9481 - val_loss: 0.2311 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2219 - acc: 0.9481 - val_loss: 0.2185 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2171 - acc: 0.9481 - val_loss: 0.2187 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2120 - acc: 0.9481 - val_loss: 0.2112 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2104 - acc: 0.9481 - val_loss: 0.2176 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2125 - acc: 0.9481 - val_loss: 0.2141 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 3s 130us/step - loss: 0.2123 - acc: 0.9481 - val_loss: 0.2214 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2112 - acc: 0.9481 - val_loss: 0.2094 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2075 - acc: 0.9481 - val_loss: 0.2085 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2054 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2046 - acc: 0.9481 - val_loss: 0.2057 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2043 - acc: 0.9481 - val_loss: 0.2062 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2044 - acc: 0.9481 - val_loss: 0.2056 - val_acc: 0.9476\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2044 - acc: 0.9481 - val_loss: 0.2057 - val_acc: 0.9476\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2043 - acc: 0.9481 - val_loss: 0.2056 - val_acc: 0.9476\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2044 - acc: 0.9481 - val_loss: 0.2057 - val_acc: 0.9476\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2043 - acc: 0.9481 - val_loss: 0.2057 - val_acc: 0.9476\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2043 - acc: 0.9481 - val_loss: 0.2057 - val_acc: 0.9476\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2043 - acc: 0.9481 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 26/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2044 - acc: 0.9481 - val_loss: 0.2057 - val_acc: 0.9476\n",
      "Epoch 00026: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_115 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_115 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_472 (Dense)            (None, 47)                557373    \n",
      "_________________________________________________________________\n",
      "dropout_358 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_473 (Dense)            (None, 47)                2256      \n",
      "_________________________________________________________________\n",
      "dropout_359 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_474 (Dense)            (None, 47)                2256      \n",
      "_________________________________________________________________\n",
      "dropout_360 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_475 (Dense)            (None, 1)                 48        \n",
      "=================================================================\n",
      "Total params: 561,933\n",
      "Trainable params: 561,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 18s 665us/step - loss: 0.2876 - acc: 0.9470 - val_loss: 0.3051 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.2498 - acc: 0.9487 - val_loss: 0.2568 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2449 - acc: 0.9488 - val_loss: 0.2533 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2404 - acc: 0.9488 - val_loss: 0.2300 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2178 - acc: 0.9488 - val_loss: 0.2269 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2292 - acc: 0.9487 - val_loss: 0.2517 - val_acc: 0.9446\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2234 - acc: 0.9487 - val_loss: 0.2273 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2176 - acc: 0.9488 - val_loss: 0.2322 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2112 - acc: 0.9488 - val_loss: 0.2176 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2091 - acc: 0.9488 - val_loss: 0.2185 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2080 - acc: 0.9488 - val_loss: 0.2194 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.2100 - acc: 0.9488 - val_loss: 0.2247 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2130 - acc: 0.9488 - val_loss: 0.2281 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2138 - acc: 0.9488 - val_loss: 0.2160 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2075 - acc: 0.9488 - val_loss: 0.2196 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2053 - acc: 0.9488 - val_loss: 0.2152 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2027 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2135 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2138 - val_acc: 0.9449\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2136 - val_acc: 0.9449\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2023 - acc: 0.9488 - val_loss: 0.2133 - val_acc: 0.9449\n",
      "Epoch 26/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2135 - val_acc: 0.9449\n",
      "Epoch 27/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2140 - val_acc: 0.9449\n",
      "Epoch 28/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 29/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 30/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 00030: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_116 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_116 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_476 (Dense)            (None, 47)                557373    \n",
      "_________________________________________________________________\n",
      "dropout_361 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_477 (Dense)            (None, 47)                2256      \n",
      "_________________________________________________________________\n",
      "dropout_362 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_478 (Dense)            (None, 47)                2256      \n",
      "_________________________________________________________________\n",
      "dropout_363 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_479 (Dense)            (None, 1)                 48        \n",
      "=================================================================\n",
      "Total params: 561,933\n",
      "Trainable params: 561,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 19s 706us/step - loss: 0.3027 - acc: 0.9444 - val_loss: 0.2560 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2589 - acc: 0.9466 - val_loss: 0.2245 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2593 - acc: 0.9466 - val_loss: 0.2317 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2500 - acc: 0.9466 - val_loss: 0.2102 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2389 - acc: 0.9466 - val_loss: 0.2178 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2345 - acc: 0.9466 - val_loss: 0.2092 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2239 - acc: 0.9466 - val_loss: 0.2033 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2306 - acc: 0.9466 - val_loss: 0.2122 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 130us/step - loss: 0.2290 - acc: 0.9466 - val_loss: 0.2072 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2213 - acc: 0.9466 - val_loss: 0.2037 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2202 - acc: 0.9466 - val_loss: 0.2063 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2197 - acc: 0.9466 - val_loss: 0.2101 - val_acc: 0.9534\n",
      "Epoch 00012: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_117 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_117 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_480 (Dense)            (None, 47)                557373    \n",
      "_________________________________________________________________\n",
      "dropout_364 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_481 (Dense)            (None, 47)                2256      \n",
      "_________________________________________________________________\n",
      "dropout_365 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_482 (Dense)            (None, 47)                2256      \n",
      "_________________________________________________________________\n",
      "dropout_366 (Dropout)        (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "dense_483 (Dense)            (None, 1)                 48        \n",
      "=================================================================\n",
      "Total params: 561,933\n",
      "Trainable params: 561,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 19s 689us/step - loss: 0.3180 - acc: 0.9477 - val_loss: 0.2820 - val_acc: 0.9442\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2691 - acc: 0.9486 - val_loss: 0.2580 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2341 - acc: 0.9488 - val_loss: 0.2403 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2352 - acc: 0.9487 - val_loss: 0.2741 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2482 - acc: 0.9488 - val_loss: 0.2514 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2409 - acc: 0.9487 - val_loss: 0.2368 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2344 - acc: 0.9487 - val_loss: 0.2381 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2201 - acc: 0.9488 - val_loss: 0.2236 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2154 - acc: 0.9488 - val_loss: 0.2268 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2175 - acc: 0.9488 - val_loss: 0.2236 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2150 - acc: 0.9488 - val_loss: 0.2315 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2160 - acc: 0.9488 - val_loss: 0.2245 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2168 - acc: 0.9487 - val_loss: 0.2249 - val_acc: 0.9449\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5341429137463931 0.041975311582327195\n",
      "\n",
      " \t ::: 5 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.09793225376881949, 'dense_units': 32, 'dense_layers': 2}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_118 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_118 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_484 (Dense)            (None, 32)                379488    \n",
      "_________________________________________________________________\n",
      "dropout_367 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_485 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_368 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_486 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 380,577\n",
      "Trainable params: 380,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 22s 832us/step - loss: 0.9336 - acc: 0.9395 - val_loss: 0.4606 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 140us/step - loss: 0.5798 - acc: 0.9467 - val_loss: 0.5370 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 4s 141us/step - loss: 0.5545 - acc: 0.9467 - val_loss: 0.6247 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 165us/step - loss: 0.5008 - acc: 0.9470 - val_loss: 0.4917 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 4s 163us/step - loss: 0.3877 - acc: 0.9475 - val_loss: 0.3430 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 5s 201us/step - loss: 0.3432 - acc: 0.9476 - val_loss: 0.2701 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 134us/step - loss: 0.2662 - acc: 0.9477 - val_loss: 0.2286 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 141us/step - loss: 0.2358 - acc: 0.9477 - val_loss: 0.2288 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2236 - acc: 0.9477 - val_loss: 0.2140 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 5s 168us/step - loss: 0.2164 - acc: 0.9477 - val_loss: 0.2153 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 5s 194us/step - loss: 0.2204 - acc: 0.9477 - val_loss: 0.2052 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 3s 129us/step - loss: 0.2087 - acc: 0.9477 - val_loss: 0.2036 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 136us/step - loss: 0.2088 - acc: 0.9477 - val_loss: 0.2073 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 3s 126us/step - loss: 0.2145 - acc: 0.9477 - val_loss: 0.2037 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2067 - acc: 0.9477 - val_loss: 0.2025 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 4s 131us/step - loss: 0.2112 - acc: 0.9477 - val_loss: 0.2209 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 3s 123us/step - loss: 0.2097 - acc: 0.9477 - val_loss: 0.2021 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 3s 121us/step - loss: 0.2079 - acc: 0.9477 - val_loss: 0.2075 - val_acc: 0.9491\n",
      "Epoch 19/200\n",
      "26868/26868 [==============================] - 4s 131us/step - loss: 0.2118 - acc: 0.9477 - val_loss: 0.2020 - val_acc: 0.9491\n",
      "Epoch 20/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2076 - acc: 0.9477 - val_loss: 0.2055 - val_acc: 0.9491\n",
      "Epoch 21/200\n",
      "26868/26868 [==============================] - 3s 124us/step - loss: 0.2120 - acc: 0.9477 - val_loss: 0.2034 - val_acc: 0.9491\n",
      "Epoch 22/200\n",
      "26868/26868 [==============================] - 3s 122us/step - loss: 0.2072 - acc: 0.9477 - val_loss: 0.2054 - val_acc: 0.9491\n",
      "Epoch 23/200\n",
      "26868/26868 [==============================] - 3s 124us/step - loss: 0.2117 - acc: 0.9477 - val_loss: 0.2044 - val_acc: 0.9491\n",
      "Epoch 24/200\n",
      "26868/26868 [==============================] - 3s 127us/step - loss: 0.2076 - acc: 0.9477 - val_loss: 0.2064 - val_acc: 0.9491\n",
      "Epoch 00024: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_119 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_119 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_487 (Dense)            (None, 32)                379488    \n",
      "_________________________________________________________________\n",
      "dropout_369 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_488 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_370 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_489 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 380,577\n",
      "Trainable params: 380,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 18s 681us/step - loss: 0.7734 - acc: 0.9456 - val_loss: 1.2624 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.8885 - acc: 0.9467 - val_loss: 0.6657 - val_acc: 0.9424\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.6024 - acc: 0.9477 - val_loss: 0.6382 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.4928 - acc: 0.9465 - val_loss: 0.3285 - val_acc: 0.9463\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.3835 - acc: 0.9477 - val_loss: 0.3096 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2817 - acc: 0.9481 - val_loss: 0.2534 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2505 - acc: 0.9481 - val_loss: 0.2299 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2223 - acc: 0.9481 - val_loss: 0.2199 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2169 - acc: 0.9481 - val_loss: 0.2166 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2153 - acc: 0.9481 - val_loss: 0.2218 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.2218 - acc: 0.9481 - val_loss: 0.2128 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 3s 120us/step - loss: 0.2109 - acc: 0.9481 - val_loss: 0.2113 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 3s 122us/step - loss: 0.2107 - acc: 0.9481 - val_loss: 0.2149 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2164 - acc: 0.9481 - val_loss: 0.2096 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2084 - acc: 0.9481 - val_loss: 0.2114 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2122 - acc: 0.9481 - val_loss: 0.2264 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2101 - acc: 0.9481 - val_loss: 0.2084 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.2081 - acc: 0.9481 - val_loss: 0.2125 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2114 - acc: 0.9481 - val_loss: 0.2071 - val_acc: 0.9476\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2069 - acc: 0.9481 - val_loss: 0.2115 - val_acc: 0.9476\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2114 - acc: 0.9481 - val_loss: 0.2071 - val_acc: 0.9476\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2065 - acc: 0.9481 - val_loss: 0.2099 - val_acc: 0.9476\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2109 - acc: 0.9481 - val_loss: 0.2071 - val_acc: 0.9476\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2065 - acc: 0.9481 - val_loss: 0.2115 - val_acc: 0.9476\n",
      "Epoch 00024: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_120 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_120 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_490 (Dense)            (None, 32)                379488    \n",
      "_________________________________________________________________\n",
      "dropout_371 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_491 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_372 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_492 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 380,577\n",
      "Trainable params: 380,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 18s 665us/step - loss: 0.7341 - acc: 0.9465 - val_loss: 0.7294 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.5777 - acc: 0.9473 - val_loss: 0.3870 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.4482 - acc: 0.9483 - val_loss: 0.7906 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.4625 - acc: 0.9487 - val_loss: 0.3131 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.2727 - acc: 0.9487 - val_loss: 0.2607 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 122us/step - loss: 0.3099 - acc: 0.9480 - val_loss: 0.3149 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.4236 - acc: 0.9474 - val_loss: 0.3262 - val_acc: 0.9443\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2742 - acc: 0.9487 - val_loss: 0.2563 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2668 - acc: 0.9487 - val_loss: 0.2485 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2505 - acc: 0.9486 - val_loss: 0.2456 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2276 - acc: 0.9488 - val_loss: 0.2238 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2093 - acc: 0.9488 - val_loss: 0.2178 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2072 - acc: 0.9488 - val_loss: 0.2203 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2124 - acc: 0.9488 - val_loss: 0.2161 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2042 - acc: 0.9488 - val_loss: 0.2147 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2082 - acc: 0.9488 - val_loss: 0.2328 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2066 - acc: 0.9488 - val_loss: 0.2155 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2050 - acc: 0.9488 - val_loss: 0.2195 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 130us/step - loss: 0.2088 - acc: 0.9488 - val_loss: 0.2148 - val_acc: 0.9449\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2045 - acc: 0.9488 - val_loss: 0.2182 - val_acc: 0.9449\n",
      "Epoch 00020: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_121 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_121 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_493 (Dense)            (None, 32)                379488    \n",
      "_________________________________________________________________\n",
      "dropout_373 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_494 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_374 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_495 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 380,577\n",
      "Trainable params: 380,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 18s 675us/step - loss: 0.7650 - acc: 0.9432 - val_loss: 0.5615 - val_acc: 0.9509\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.6029 - acc: 0.9459 - val_loss: 0.7226 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 122us/step - loss: 0.5315 - acc: 0.9462 - val_loss: 0.4132 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.3754 - acc: 0.9466 - val_loss: 0.2700 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.3386 - acc: 0.9461 - val_loss: 0.2217 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2258 - acc: 0.9466 - val_loss: 0.1991 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2294 - acc: 0.9466 - val_loss: 0.1924 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2115 - acc: 0.9466 - val_loss: 0.1905 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.2108 - acc: 0.9466 - val_loss: 0.1924 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 122us/step - loss: 0.2112 - acc: 0.9466 - val_loss: 0.1956 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 3s 122us/step - loss: 0.2192 - acc: 0.9466 - val_loss: 0.1895 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.2096 - acc: 0.9466 - val_loss: 0.1893 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 3s 121us/step - loss: 0.2110 - acc: 0.9466 - val_loss: 0.1962 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 3s 120us/step - loss: 0.2170 - acc: 0.9466 - val_loss: 0.1900 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 3s 121us/step - loss: 0.2096 - acc: 0.9466 - val_loss: 0.1897 - val_acc: 0.9534\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2141 - acc: 0.9466 - val_loss: 0.2083 - val_acc: 0.9534\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2122 - acc: 0.9466 - val_loss: 0.1895 - val_acc: 0.9534\n",
      "Epoch 00017: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_122 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_122 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_496 (Dense)            (None, 32)                379488    \n",
      "_________________________________________________________________\n",
      "dropout_375 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_497 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_376 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_498 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 380,577\n",
      "Trainable params: 380,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 18s 658us/step - loss: 0.8047 - acc: 0.9437 - val_loss: 0.5827 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.5693 - acc: 0.9478 - val_loss: 0.5567 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.4728 - acc: 0.9482 - val_loss: 0.3439 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 122us/step - loss: 0.3941 - acc: 0.9484 - val_loss: 0.2962 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2636 - acc: 0.9487 - val_loss: 0.2490 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2327 - acc: 0.9487 - val_loss: 0.2312 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2476 - acc: 0.9488 - val_loss: 0.2230 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 123us/step - loss: 0.2091 - acc: 0.9488 - val_loss: 0.2184 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 124us/step - loss: 0.2069 - acc: 0.9488 - val_loss: 0.2182 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2065 - acc: 0.9488 - val_loss: 0.2211 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2143 - acc: 0.9488 - val_loss: 0.2155 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2047 - acc: 0.9488 - val_loss: 0.2151 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2055 - acc: 0.9488 - val_loss: 0.2210 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2117 - acc: 0.9488 - val_loss: 0.2146 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2042 - acc: 0.9488 - val_loss: 0.2150 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2083 - acc: 0.9488 - val_loss: 0.2327 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2067 - acc: 0.9488 - val_loss: 0.2147 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2053 - acc: 0.9488 - val_loss: 0.2198 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2092 - acc: 0.9488 - val_loss: 0.2144 - val_acc: 0.9449\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2044 - acc: 0.9488 - val_loss: 0.2185 - val_acc: 0.9449\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2087 - acc: 0.9488 - val_loss: 0.2153 - val_acc: 0.9449\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2043 - acc: 0.9488 - val_loss: 0.2217 - val_acc: 0.9449\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2085 - acc: 0.9488 - val_loss: 0.2177 - val_acc: 0.9449\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2046 - acc: 0.9488 - val_loss: 0.2183 - val_acc: 0.9449\n",
      "Epoch 00024: early stopping\n",
      "-0.4999373062118293 7.697287216252554e-05\n",
      "\n",
      " \t ::: 6 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.0038818138307821935, 'dense_units': 74, 'dense_layers': 6}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_123 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_123 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_499 (Dense)            (None, 74)                877566    \n",
      "_________________________________________________________________\n",
      "dropout_377 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_500 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_378 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_501 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_379 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_502 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_380 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_503 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_381 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_504 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_382 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_505 (Dense)            (None, 1)                 75        \n",
      "=================================================================\n",
      "Total params: 905,391\n",
      "Trainable params: 905,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 19s 707us/step - loss: 0.3151 - acc: 0.9474 - val_loss: 0.2175 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 156us/step - loss: 0.2217 - acc: 0.9477 - val_loss: 0.2106 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 5s 171us/step - loss: 0.2153 - acc: 0.9477 - val_loss: 0.2102 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 154us/step - loss: 0.2100 - acc: 0.9477 - val_loss: 0.2037 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 4s 148us/step - loss: 0.2083 - acc: 0.9477 - val_loss: 0.2030 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2079 - acc: 0.9477 - val_loss: 0.2027 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2073 - acc: 0.9477 - val_loss: 0.2025 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 152us/step - loss: 0.2073 - acc: 0.9477 - val_loss: 0.2033 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 6s 229us/step - loss: 0.2071 - acc: 0.9477 - val_loss: 0.2031 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 5s 174us/step - loss: 0.2069 - acc: 0.9477 - val_loss: 0.2051 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 4s 162us/step - loss: 0.2066 - acc: 0.9477 - val_loss: 0.2020 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 156us/step - loss: 0.2065 - acc: 0.9477 - val_loss: 0.2020 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 157us/step - loss: 0.2066 - acc: 0.9477 - val_loss: 0.2020 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 4s 161us/step - loss: 0.2064 - acc: 0.9477 - val_loss: 0.2021 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 4s 155us/step - loss: 0.2065 - acc: 0.9477 - val_loss: 0.2019 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 4s 148us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2018 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 4s 151us/step - loss: 0.2063 - acc: 0.9477 - val_loss: 0.2021 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 4s 149us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2022 - val_acc: 0.9491\n",
      "Epoch 19/200\n",
      "26868/26868 [==============================] - 4s 153us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 20/200\n",
      "26868/26868 [==============================] - 5s 183us/step - loss: 0.2063 - acc: 0.9477 - val_loss: 0.2018 - val_acc: 0.9491\n",
      "Epoch 21/200\n",
      "26868/26868 [==============================] - 5s 196us/step - loss: 0.2067 - acc: 0.9477 - val_loss: 0.2025 - val_acc: 0.9491\n",
      "Epoch 22/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2064 - acc: 0.9477 - val_loss: 0.2020 - val_acc: 0.9491\n",
      "Epoch 23/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2065 - acc: 0.9477 - val_loss: 0.2023 - val_acc: 0.9491\n",
      "Epoch 24/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 25/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2060 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 26/200\n",
      "26868/26868 [==============================] - 4s 149us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2070 - val_acc: 0.9491\n",
      "Epoch 27/200\n",
      "26868/26868 [==============================] - 4s 152us/step - loss: 0.2063 - acc: 0.9477 - val_loss: 0.2023 - val_acc: 0.9491\n",
      "Epoch 28/200\n",
      "26868/26868 [==============================] - 4s 151us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2018 - val_acc: 0.9491\n",
      "Epoch 29/200\n",
      "26868/26868 [==============================] - 4s 144us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 30/200\n",
      "26868/26868 [==============================] - 4s 145us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2022 - val_acc: 0.9491\n",
      "Epoch 31/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 32/200\n",
      "26868/26868 [==============================] - 4s 143us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 33/200\n",
      "26868/26868 [==============================] - 4s 145us/step - loss: 0.2063 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 34/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 35/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2060 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 36/200\n",
      "26868/26868 [==============================] - 4s 155us/step - loss: 0.2063 - acc: 0.9477 - val_loss: 0.2023 - val_acc: 0.9491\n",
      "Epoch 37/200\n",
      "26868/26868 [==============================] - 4s 154us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2062 - val_acc: 0.9491\n",
      "Epoch 38/200\n",
      "26868/26868 [==============================] - 4s 151us/step - loss: 0.2060 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 39/200\n",
      "26868/26868 [==============================] - 4s 151us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 40/200\n",
      "26868/26868 [==============================] - 4s 156us/step - loss: 0.2059 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 41/200\n",
      "26868/26868 [==============================] - 4s 149us/step - loss: 0.2060 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 42/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2059 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 43/200\n",
      "26868/26868 [==============================] - 4s 149us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 44/200\n",
      "26868/26868 [==============================] - 4s 152us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 45/200\n",
      "26868/26868 [==============================] - 4s 157us/step - loss: 0.2060 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 46/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 47/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2064 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 48/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2063 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 49/200\n",
      "26868/26868 [==============================] - 4s 150us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 50/200\n",
      "26868/26868 [==============================] - 4s 152us/step - loss: 0.2059 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 00050: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_124 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_124 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_506 (Dense)            (None, 74)                877566    \n",
      "_________________________________________________________________\n",
      "dropout_383 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_507 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_384 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_508 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_385 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_509 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_386 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_510 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_387 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_511 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_388 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_512 (Dense)            (None, 1)                 75        \n",
      "=================================================================\n",
      "Total params: 905,391\n",
      "Trainable params: 905,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 19s 708us/step - loss: 0.3149 - acc: 0.9474 - val_loss: 0.2263 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2180 - acc: 0.9481 - val_loss: 0.2172 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2104 - acc: 0.9481 - val_loss: 0.2090 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2080 - acc: 0.9481 - val_loss: 0.2086 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2070 - acc: 0.9481 - val_loss: 0.2105 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2064 - acc: 0.9481 - val_loss: 0.2073 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2058 - acc: 0.9481 - val_loss: 0.2076 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2061 - acc: 0.9481 - val_loss: 0.2068 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2060 - acc: 0.9481 - val_loss: 0.2068 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2055 - acc: 0.9481 - val_loss: 0.2065 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2057 - acc: 0.9481 - val_loss: 0.2067 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2055 - acc: 0.9481 - val_loss: 0.2066 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2055 - acc: 0.9481 - val_loss: 0.2078 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2054 - acc: 0.9481 - val_loss: 0.2064 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2054 - acc: 0.9481 - val_loss: 0.2065 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2056 - acc: 0.9481 - val_loss: 0.2062 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2052 - acc: 0.9481 - val_loss: 0.2063 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2055 - acc: 0.9481 - val_loss: 0.2076 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2052 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2053 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2052 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2053 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2056 - acc: 0.9481 - val_loss: 0.2072 - val_acc: 0.9476\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2054 - acc: 0.9481 - val_loss: 0.2068 - val_acc: 0.9476\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2050 - acc: 0.9481 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 26/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2051 - acc: 0.9481 - val_loss: 0.2064 - val_acc: 0.9476\n",
      "Epoch 27/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2053 - acc: 0.9481 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 28/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2052 - acc: 0.9481 - val_loss: 0.2065 - val_acc: 0.9476\n",
      "Epoch 29/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2048 - acc: 0.9481 - val_loss: 0.2059 - val_acc: 0.9476\n",
      "Epoch 30/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2052 - acc: 0.9481 - val_loss: 0.2059 - val_acc: 0.9476\n",
      "Epoch 31/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2050 - acc: 0.9481 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 32/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2051 - acc: 0.9481 - val_loss: 0.2065 - val_acc: 0.9476\n",
      "Epoch 33/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2049 - acc: 0.9481 - val_loss: 0.2059 - val_acc: 0.9476\n",
      "Epoch 34/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2052 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 35/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2049 - acc: 0.9481 - val_loss: 0.2069 - val_acc: 0.9476\n",
      "Epoch 36/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2050 - acc: 0.9481 - val_loss: 0.2059 - val_acc: 0.9476\n",
      "Epoch 37/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2048 - acc: 0.9481 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 38/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2049 - acc: 0.9481 - val_loss: 0.2064 - val_acc: 0.9476\n",
      "Epoch 00038: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_125 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_125 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_513 (Dense)            (None, 74)                877566    \n",
      "_________________________________________________________________\n",
      "dropout_389 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_514 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_390 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_515 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_391 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_516 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_392 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_517 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_393 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_518 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_394 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_519 (Dense)            (None, 1)                 75        \n",
      "=================================================================\n",
      "Total params: 905,391\n",
      "Trainable params: 905,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 20s 751us/step - loss: 0.3344 - acc: 0.9456 - val_loss: 0.2361 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2147 - acc: 0.9488 - val_loss: 0.2200 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2071 - acc: 0.9488 - val_loss: 0.2163 - val_acc: 0.9449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2052 - acc: 0.9488 - val_loss: 0.2160 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2052 - acc: 0.9488 - val_loss: 0.2149 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2045 - acc: 0.9488 - val_loss: 0.2151 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2046 - acc: 0.9488 - val_loss: 0.2144 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2042 - acc: 0.9488 - val_loss: 0.2148 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2039 - acc: 0.9488 - val_loss: 0.2142 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2039 - acc: 0.9488 - val_loss: 0.2143 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2038 - acc: 0.9488 - val_loss: 0.2140 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2147 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2034 - acc: 0.9488 - val_loss: 0.2155 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2039 - acc: 0.9488 - val_loss: 0.2145 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2033 - acc: 0.9488 - val_loss: 0.2152 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2033 - acc: 0.9488 - val_loss: 0.2142 - val_acc: 0.9449\n",
      "Epoch 00016: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_126 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_126 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_520 (Dense)            (None, 74)                877566    \n",
      "_________________________________________________________________\n",
      "dropout_395 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_521 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_396 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_522 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_397 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_523 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_398 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_524 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_399 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_525 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_400 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_526 (Dense)            (None, 1)                 75        \n",
      "=================================================================\n",
      "Total params: 905,391\n",
      "Trainable params: 905,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 19s 723us/step - loss: 0.3161 - acc: 0.9465 - val_loss: 0.2155 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2321 - acc: 0.9466 - val_loss: 0.2000 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2201 - acc: 0.9466 - val_loss: 0.2000 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2153 - acc: 0.9466 - val_loss: 0.1927 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2116 - acc: 0.9466 - val_loss: 0.1914 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2109 - acc: 0.9466 - val_loss: 0.1911 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2107 - acc: 0.9466 - val_loss: 0.1907 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2106 - acc: 0.9466 - val_loss: 0.1958 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2106 - acc: 0.9466 - val_loss: 0.1897 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2098 - acc: 0.9466 - val_loss: 0.1903 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2097 - acc: 0.9466 - val_loss: 0.1895 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2096 - acc: 0.9466 - val_loss: 0.1893 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2099 - acc: 0.9466 - val_loss: 0.1896 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2096 - acc: 0.9466 - val_loss: 0.1901 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2096 - acc: 0.9466 - val_loss: 0.1891 - val_acc: 0.9534\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2095 - acc: 0.9466 - val_loss: 0.1894 - val_acc: 0.9534\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2093 - acc: 0.9466 - val_loss: 0.1905 - val_acc: 0.9534\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2094 - acc: 0.9466 - val_loss: 0.1891 - val_acc: 0.9534\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2096 - acc: 0.9466 - val_loss: 0.1902 - val_acc: 0.9534\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2095 - acc: 0.9466 - val_loss: 0.1894 - val_acc: 0.9534\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2093 - acc: 0.9466 - val_loss: 0.1890 - val_acc: 0.9534\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2093 - acc: 0.9466 - val_loss: 0.1893 - val_acc: 0.9534\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.1889 - val_acc: 0.9534\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2094 - acc: 0.9466 - val_loss: 0.1897 - val_acc: 0.9534\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2094 - acc: 0.9466 - val_loss: 0.1902 - val_acc: 0.9534\n",
      "Epoch 26/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2093 - acc: 0.9466 - val_loss: 0.1891 - val_acc: 0.9534\n",
      "Epoch 27/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.1888 - val_acc: 0.9534\n",
      "Epoch 28/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2094 - acc: 0.9466 - val_loss: 0.1889 - val_acc: 0.9534\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.1890 - val_acc: 0.9534\n",
      "Epoch 30/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2091 - acc: 0.9466 - val_loss: 0.1891 - val_acc: 0.9534\n",
      "Epoch 31/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.1908 - val_acc: 0.9534\n",
      "Epoch 32/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2093 - acc: 0.9466 - val_loss: 0.1888 - val_acc: 0.9534\n",
      "Epoch 33/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.1888 - val_acc: 0.9534\n",
      "Epoch 34/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2089 - acc: 0.9466 - val_loss: 0.1891 - val_acc: 0.9534\n",
      "Epoch 35/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2091 - acc: 0.9466 - val_loss: 0.1895 - val_acc: 0.9534\n",
      "Epoch 36/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2093 - acc: 0.9466 - val_loss: 0.1889 - val_acc: 0.9534\n",
      "Epoch 37/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2090 - acc: 0.9466 - val_loss: 0.1897 - val_acc: 0.9534\n",
      "Epoch 38/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.1900 - val_acc: 0.9534\n",
      "Epoch 00038: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_127 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_127 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_527 (Dense)            (None, 74)                877566    \n",
      "_________________________________________________________________\n",
      "dropout_401 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_528 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_402 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_529 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_403 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_530 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_404 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_531 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_405 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_532 (Dense)            (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_406 (Dropout)        (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_533 (Dense)            (None, 1)                 75        \n",
      "=================================================================\n",
      "Total params: 905,391\n",
      "Trainable params: 905,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 20s 729us/step - loss: 0.3181 - acc: 0.9487 - val_loss: 0.2397 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2176 - acc: 0.9488 - val_loss: 0.2217 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2075 - acc: 0.9488 - val_loss: 0.2165 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2059 - acc: 0.9488 - val_loss: 0.2160 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2050 - acc: 0.9488 - val_loss: 0.2149 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2045 - acc: 0.9488 - val_loss: 0.2176 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2041 - acc: 0.9488 - val_loss: 0.2144 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2039 - acc: 0.9488 - val_loss: 0.2143 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2037 - acc: 0.9488 - val_loss: 0.2149 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2037 - acc: 0.9488 - val_loss: 0.2143 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2042 - acc: 0.9488 - val_loss: 0.2140 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2033 - acc: 0.9488 - val_loss: 0.2147 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2034 - acc: 0.9488 - val_loss: 0.2139 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2140 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2138 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2034 - acc: 0.9488 - val_loss: 0.2138 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2140 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2033 - acc: 0.9488 - val_loss: 0.2141 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2144 - val_acc: 0.9449\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2165 - val_acc: 0.9449\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2033 - acc: 0.9488 - val_loss: 0.2139 - val_acc: 0.9449\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2143 - val_acc: 0.9449\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2149 - val_acc: 0.9449\n",
      "Epoch 00025: early stopping\n",
      "-0.49998434020358645 5.873065052841389e-05\n",
      "\n",
      " \t ::: 7 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.07617796242239587, 'dense_units': 78, 'dense_layers': 5}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_128 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_128 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_534 (Dense)            (None, 78)                925002    \n",
      "_________________________________________________________________\n",
      "dropout_407 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_535 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_408 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_536 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_409 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_537 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_410 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_538 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_411 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_539 (Dense)            (None, 1)                 79        \n",
      "=================================================================\n",
      "Total params: 949,729\n",
      "Trainable params: 949,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 19s 719us/step - loss: 0.7298 - acc: 0.9434 - val_loss: 0.2788 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 136us/step - loss: 0.2353 - acc: 0.9477 - val_loss: 0.2096 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 4s 138us/step - loss: 0.2095 - acc: 0.9477 - val_loss: 0.2027 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 155us/step - loss: 0.2073 - acc: 0.9477 - val_loss: 0.2028 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2071 - acc: 0.9477 - val_loss: 0.2034 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 4s 144us/step - loss: 0.2106 - acc: 0.9477 - val_loss: 0.2164 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 135us/step - loss: 0.2268 - acc: 0.9477 - val_loss: 0.2020 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 134us/step - loss: 0.2064 - acc: 0.9477 - val_loss: 0.2022 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 4s 134us/step - loss: 0.2070 - acc: 0.9477 - val_loss: 0.2029 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2119 - acc: 0.9477 - val_loss: 0.2156 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 4s 137us/step - loss: 0.2184 - acc: 0.9477 - val_loss: 0.2032 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 145us/step - loss: 0.2067 - acc: 0.9477 - val_loss: 0.2038 - val_acc: 0.9491\n",
      "Epoch 00012: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_129 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_129 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_540 (Dense)            (None, 78)                925002    \n",
      "_________________________________________________________________\n",
      "dropout_412 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_541 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_413 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_542 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_414 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_543 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_415 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_544 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_416 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_545 (Dense)            (None, 1)                 79        \n",
      "=================================================================\n",
      "Total params: 949,729\n",
      "Trainable params: 949,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 21s 774us/step - loss: 0.5332 - acc: 0.9443 - val_loss: 0.3053 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2203 - acc: 0.9481 - val_loss: 0.2062 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2047 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2055 - acc: 0.9481 - val_loss: 0.2068 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2057 - acc: 0.9481 - val_loss: 0.2069 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2096 - acc: 0.9481 - val_loss: 0.2210 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2255 - acc: 0.9481 - val_loss: 0.2064 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2052 - acc: 0.9481 - val_loss: 0.2068 - val_acc: 0.9476\n",
      "Epoch 00008: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_130 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_130 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_546 (Dense)            (None, 78)                925002    \n",
      "_________________________________________________________________\n",
      "dropout_417 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_547 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_418 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_548 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_419 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_549 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_420 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_550 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_421 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_551 (Dense)            (None, 1)                 79        \n",
      "=================================================================\n",
      "Total params: 949,729\n",
      "Trainable params: 949,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 20s 752us/step - loss: 0.9561 - acc: 0.9462 - val_loss: 0.3555 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2507 - acc: 0.9488 - val_loss: 0.2256 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2102 - acc: 0.9488 - val_loss: 0.2165 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2058 - acc: 0.9488 - val_loss: 0.2163 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2047 - acc: 0.9488 - val_loss: 0.2157 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2082 - acc: 0.9488 - val_loss: 0.2291 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2234 - acc: 0.9488 - val_loss: 0.2144 - val_acc: 0.9449\n",
      "Epoch 8/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2034 - acc: 0.9488 - val_loss: 0.2145 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2041 - acc: 0.9488 - val_loss: 0.2158 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2088 - acc: 0.9488 - val_loss: 0.2279 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2152 - acc: 0.9488 - val_loss: 0.2149 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2039 - acc: 0.9488 - val_loss: 0.2161 - val_acc: 0.9449\n",
      "Epoch 00012: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_131 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_131 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_552 (Dense)            (None, 78)                925002    \n",
      "_________________________________________________________________\n",
      "dropout_422 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_553 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_423 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_554 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_424 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_555 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_425 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_556 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_426 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_557 (Dense)            (None, 1)                 79        \n",
      "=================================================================\n",
      "Total params: 949,729\n",
      "Trainable params: 949,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 20s 757us/step - loss: 0.9035 - acc: 0.9399 - val_loss: 0.2743 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2331 - acc: 0.9466 - val_loss: 0.1941 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2118 - acc: 0.9466 - val_loss: 0.1897 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2100 - acc: 0.9466 - val_loss: 0.1898 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2102 - acc: 0.9466 - val_loss: 0.1917 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2135 - acc: 0.9466 - val_loss: 0.2040 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2293 - acc: 0.9466 - val_loss: 0.1915 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2097 - acc: 0.9466 - val_loss: 0.1895 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2102 - acc: 0.9466 - val_loss: 0.1908 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2149 - acc: 0.9466 - val_loss: 0.2026 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2214 - acc: 0.9466 - val_loss: 0.1908 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2100 - acc: 0.9466 - val_loss: 0.1898 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2143 - acc: 0.9466 - val_loss: 0.2012 - val_acc: 0.9534\n",
      "Epoch 00013: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_132 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_132 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_558 (Dense)            (None, 78)                925002    \n",
      "_________________________________________________________________\n",
      "dropout_427 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_559 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_428 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_560 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_429 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_561 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_430 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_562 (Dense)            (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout_431 (Dropout)        (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_563 (Dense)            (None, 1)                 79        \n",
      "=================================================================\n",
      "Total params: 949,729\n",
      "Trainable params: 949,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 20s 762us/step - loss: 0.9758 - acc: 0.9464 - val_loss: 0.6413 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.3550 - acc: 0.9485 - val_loss: 0.2281 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2105 - acc: 0.9488 - val_loss: 0.2170 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2057 - acc: 0.9488 - val_loss: 0.2157 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2046 - acc: 0.9488 - val_loss: 0.2152 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2079 - acc: 0.9488 - val_loss: 0.2283 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2235 - acc: 0.9488 - val_loss: 0.2142 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2146 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2040 - acc: 0.9488 - val_loss: 0.2162 - val_acc: 0.9449\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2085 - acc: 0.9488 - val_loss: 0.2279 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2154 - acc: 0.9488 - val_loss: 0.2174 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2041 - acc: 0.9488 - val_loss: 0.2147 - val_acc: 0.9449\n",
      "Epoch 00012: early stopping\n",
      "-0.4999373062118293 7.697287216252554e-05\n",
      "\n",
      " \t ::: 8 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.015320204284174802, 'dense_units': 20, 'dense_layers': 4}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_133 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_133 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_564 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_432 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_565 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_433 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_566 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_434 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_567 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_435 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_568 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,461\n",
      "Trainable params: 238,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 20s 748us/step - loss: 0.2607 - acc: 0.9475 - val_loss: 0.2169 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 3s 128us/step - loss: 0.2163 - acc: 0.9477 - val_loss: 0.2051 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 3s 127us/step - loss: 0.2077 - acc: 0.9477 - val_loss: 0.2024 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 3s 124us/step - loss: 0.2071 - acc: 0.9477 - val_loss: 0.2024 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 3s 125us/step - loss: 0.2071 - acc: 0.9477 - val_loss: 0.2028 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 3s 128us/step - loss: 0.2070 - acc: 0.9477 - val_loss: 0.2022 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 3s 124us/step - loss: 0.2066 - acc: 0.9477 - val_loss: 0.2021 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 3s 125us/step - loss: 0.2064 - acc: 0.9477 - val_loss: 0.2021 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 3s 125us/step - loss: 0.2069 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 3s 126us/step - loss: 0.2064 - acc: 0.9477 - val_loss: 0.2016 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 3s 130us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 134us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2018 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 3s 129us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2019 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 3s 124us/step - loss: 0.2059 - acc: 0.9477 - val_loss: 0.2021 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 3s 124us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_134 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_134 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_569 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_436 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_570 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_437 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_571 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_438 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_572 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_439 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_573 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,461\n",
      "Trainable params: 238,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 20s 759us/step - loss: 0.2533 - acc: 0.9479 - val_loss: 0.2227 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2141 - acc: 0.9481 - val_loss: 0.2106 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2066 - acc: 0.9481 - val_loss: 0.2066 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2065 - acc: 0.9481 - val_loss: 0.2064 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2059 - acc: 0.9481 - val_loss: 0.2064 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2059 - acc: 0.9481 - val_loss: 0.2070 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2060 - acc: 0.9481 - val_loss: 0.2068 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2054 - acc: 0.9481 - val_loss: 0.2064 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2056 - acc: 0.9481 - val_loss: 0.2062 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2051 - acc: 0.9481 - val_loss: 0.2098 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2051 - acc: 0.9481 - val_loss: 0.2106 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2051 - acc: 0.9481 - val_loss: 0.2087 - val_acc: 0.9476\n",
      "Epoch 13/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2052 - acc: 0.9481 - val_loss: 0.2059 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2050 - acc: 0.9481 - val_loss: 0.2058 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2048 - acc: 0.9481 - val_loss: 0.2057 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2046 - acc: 0.9481 - val_loss: 0.2057 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2048 - acc: 0.9481 - val_loss: 0.2070 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2045 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2044 - acc: 0.9481 - val_loss: 0.2058 - val_acc: 0.9476\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2043 - acc: 0.9481 - val_loss: 0.2058 - val_acc: 0.9476\n",
      "Epoch 00020: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_135 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_135 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_574 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_440 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_575 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_441 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_576 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_442 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_577 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_443 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_578 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,461\n",
      "Trainable params: 238,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 20s 745us/step - loss: 0.2619 - acc: 0.9488 - val_loss: 0.2382 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2176 - acc: 0.9488 - val_loss: 0.2235 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2118 - acc: 0.9488 - val_loss: 0.2194 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2082 - acc: 0.9488 - val_loss: 0.2175 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2059 - acc: 0.9488 - val_loss: 0.2143 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2038 - acc: 0.9488 - val_loss: 0.2141 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2039 - acc: 0.9488 - val_loss: 0.2147 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2033 - acc: 0.9488 - val_loss: 0.2139 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2136 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2037 - acc: 0.9488 - val_loss: 0.2149 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2033 - acc: 0.9488 - val_loss: 0.2141 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2143 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2136 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2032 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 00014: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_136 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_136 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_579 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_444 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_580 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_445 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_581 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_446 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_582 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_447 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_583 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,461\n",
      "Trainable params: 238,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 21s 771us/step - loss: 0.2653 - acc: 0.9464 - val_loss: 0.2035 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2161 - acc: 0.9466 - val_loss: 0.1914 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2108 - acc: 0.9466 - val_loss: 0.1902 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2103 - acc: 0.9466 - val_loss: 0.1903 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2098 - acc: 0.9466 - val_loss: 0.1892 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2097 - acc: 0.9466 - val_loss: 0.1892 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2096 - acc: 0.9466 - val_loss: 0.1932 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2100 - acc: 0.9466 - val_loss: 0.1899 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2101 - acc: 0.9466 - val_loss: 0.1891 - val_acc: 0.9534\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2095 - acc: 0.9466 - val_loss: 0.1899 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2098 - acc: 0.9466 - val_loss: 0.1890 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 133us/step - loss: 0.2090 - acc: 0.9466 - val_loss: 0.1890 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2093 - acc: 0.9466 - val_loss: 0.1887 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2088 - acc: 0.9466 - val_loss: 0.1917 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2089 - acc: 0.9466 - val_loss: 0.1889 - val_acc: 0.9534\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2090 - acc: 0.9466 - val_loss: 0.1893 - val_acc: 0.9534\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 3s 130us/step - loss: 0.2089 - acc: 0.9466 - val_loss: 0.1894 - val_acc: 0.9534\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 3s 130us/step - loss: 0.2087 - acc: 0.9466 - val_loss: 0.1892 - val_acc: 0.9534\n",
      "Epoch 00018: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_137 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_137 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_584 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_448 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_585 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_449 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_586 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_450 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_587 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_451 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_588 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,461\n",
      "Trainable params: 238,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 20s 744us/step - loss: 0.2706 - acc: 0.9451 - val_loss: 0.2359 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2152 - acc: 0.9488 - val_loss: 0.2194 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2083 - acc: 0.9488 - val_loss: 0.2200 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2058 - acc: 0.9488 - val_loss: 0.2181 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2039 - acc: 0.9488 - val_loss: 0.2152 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2037 - acc: 0.9488 - val_loss: 0.2142 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2040 - acc: 0.9488 - val_loss: 0.2138 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2034 - acc: 0.9488 - val_loss: 0.2159 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2033 - acc: 0.9488 - val_loss: 0.2136 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2138 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2032 - acc: 0.9488 - val_loss: 0.2142 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2028 - acc: 0.9488 - val_loss: 0.2138 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2141 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2028 - acc: 0.9488 - val_loss: 0.2148 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2136 - val_acc: 0.9449\n",
      "Epoch 00019: early stopping\n",
      "-0.5000000414690439 9.927402683714072e-05\n",
      "\n",
      " \t ::: 9 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.03414158832856008, 'dense_units': 22, 'dense_layers': 5}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_138 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_138 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_589 (Dense)            (None, 22)                260898    \n",
      "_________________________________________________________________\n",
      "dropout_452 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_590 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_453 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_591 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_454 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_592 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_455 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_593 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_456 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_594 (Dense)            (None, 1)                 23        \n",
      "=================================================================\n",
      "Total params: 262,945\n",
      "Trainable params: 262,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26868/26868 [==============================] - 21s 783us/step - loss: 0.2474 - acc: 0.9453 - val_loss: 0.2047 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 134us/step - loss: 0.2095 - acc: 0.9477 - val_loss: 0.2024 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 4s 138us/step - loss: 0.2070 - acc: 0.9477 - val_loss: 0.2028 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2070 - acc: 0.9477 - val_loss: 0.2027 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 3s 130us/step - loss: 0.2072 - acc: 0.9477 - val_loss: 0.2034 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 4s 135us/step - loss: 0.2071 - acc: 0.9477 - val_loss: 0.2018 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 136us/step - loss: 0.2057 - acc: 0.9477 - val_loss: 0.2020 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 134us/step - loss: 0.2056 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 4s 145us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2038 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2056 - acc: 0.9477 - val_loss: 0.2020 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 145us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2018 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 138us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2012 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 4s 142us/step - loss: 0.2055 - acc: 0.9477 - val_loss: 0.2018 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2021 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 4s 135us/step - loss: 0.2057 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 4s 136us/step - loss: 0.2057 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 4s 145us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 00018: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_139 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_139 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_595 (Dense)            (None, 22)                260898    \n",
      "_________________________________________________________________\n",
      "dropout_457 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_596 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_458 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_597 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_459 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_598 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_460 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_599 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_461 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_600 (Dense)            (None, 1)                 23        \n",
      "=================================================================\n",
      "Total params: 262,945\n",
      "Trainable params: 262,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 21s 785us/step - loss: 0.2364 - acc: 0.9454 - val_loss: 0.2068 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2073 - acc: 0.9481 - val_loss: 0.2065 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2061 - acc: 0.9481 - val_loss: 0.2073 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 6s 215us/step - loss: 0.2058 - acc: 0.9481 - val_loss: 0.2077 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2057 - acc: 0.9481 - val_loss: 0.2070 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2066 - acc: 0.9481 - val_loss: 0.2082 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2048 - acc: 0.9481 - val_loss: 0.2056 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2045 - acc: 0.9481 - val_loss: 0.2056 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2047 - acc: 0.9481 - val_loss: 0.2062 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2049 - acc: 0.9481 - val_loss: 0.2056 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2044 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2047 - acc: 0.9481 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 00012: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_140 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_140 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_601 (Dense)            (None, 22)                260898    \n",
      "_________________________________________________________________\n",
      "dropout_462 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_602 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_463 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_603 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_464 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_604 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_465 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_605 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_466 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_606 (Dense)            (None, 1)                 23        \n",
      "=================================================================\n",
      "Total params: 262,945\n",
      "Trainable params: 262,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 24s 895us/step - loss: 0.2404 - acc: 0.9458 - val_loss: 0.2161 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2047 - acc: 0.9488 - val_loss: 0.2149 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2049 - acc: 0.9488 - val_loss: 0.2151 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2038 - acc: 0.9488 - val_loss: 0.2149 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2037 - acc: 0.9488 - val_loss: 0.2157 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2044 - acc: 0.9488 - val_loss: 0.2151 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2135 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2142 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2144 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2028 - acc: 0.9488 - val_loss: 0.2139 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2028 - acc: 0.9488 - val_loss: 0.2142 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.2027 - acc: 0.9488 - val_loss: 0.2143 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2028 - acc: 0.9488 - val_loss: 0.2155 - val_acc: 0.9449\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_141 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_141 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_607 (Dense)            (None, 22)                260898    \n",
      "_________________________________________________________________\n",
      "dropout_467 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_608 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_468 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_609 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_469 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_610 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_470 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_611 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_471 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_612 (Dense)            (None, 1)                 23        \n",
      "=================================================================\n",
      "Total params: 262,945\n",
      "Trainable params: 262,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 26s 957us/step - loss: 0.2599 - acc: 0.9455 - val_loss: 0.1932 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2113 - acc: 0.9466 - val_loss: 0.1896 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 7s 254us/step - loss: 0.2107 - acc: 0.9466 - val_loss: 0.1908 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2101 - acc: 0.9466 - val_loss: 0.1889 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2098 - acc: 0.9466 - val_loss: 0.1899 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2100 - acc: 0.9466 - val_loss: 0.1889 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2091 - acc: 0.9466 - val_loss: 0.1886 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.2088 - acc: 0.9466 - val_loss: 0.1890 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2090 - acc: 0.9466 - val_loss: 0.1889 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2090 - acc: 0.9466 - val_loss: 0.1887 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2086 - acc: 0.9466 - val_loss: 0.1896 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2089 - acc: 0.9466 - val_loss: 0.1888 - val_acc: 0.9534\n",
      "Epoch 00012: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_142 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_142 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_613 (Dense)            (None, 22)                260898    \n",
      "_________________________________________________________________\n",
      "dropout_472 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_614 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_473 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_615 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_474 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_616 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_475 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_617 (Dense)            (None, 22)                506       \n",
      "_________________________________________________________________\n",
      "dropout_476 (Dropout)        (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_618 (Dense)            (None, 1)                 23        \n",
      "=================================================================\n",
      "Total params: 262,945\n",
      "Trainable params: 262,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 22s 802us/step - loss: 0.2417 - acc: 0.9469 - val_loss: 0.2171 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2045 - acc: 0.9488 - val_loss: 0.2153 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2038 - acc: 0.9488 - val_loss: 0.2141 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2043 - acc: 0.9488 - val_loss: 0.2137 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2041 - acc: 0.9488 - val_loss: 0.2146 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2040 - acc: 0.9488 - val_loss: 0.2135 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2144 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2138 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2135 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2140 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2027 - acc: 0.9488 - val_loss: 0.2139 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2028 - acc: 0.9488 - val_loss: 0.2139 - val_acc: 0.9449\n",
      "Epoch 00013: early stopping\n",
      "-0.49999995853095613 9.9274026837167e-05\n",
      "\n",
      " \t ::: 10 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.0028125415783072654, 'dense_units': 77, 'dense_layers': 3}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_143 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_143 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_619 (Dense)            (None, 77)                913143    \n",
      "_________________________________________________________________\n",
      "dropout_477 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_620 (Dense)            (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dropout_478 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_621 (Dense)            (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dropout_479 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_622 (Dense)            (None, 1)                 78        \n",
      "=================================================================\n",
      "Total params: 925,233\n",
      "Trainable params: 925,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 22s 833us/step - loss: 0.3333 - acc: 0.9448 - val_loss: 0.2673 - val_acc: 0.9489\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2505 - acc: 0.9476 - val_loss: 0.2392 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 4s 148us/step - loss: 0.2333 - acc: 0.9474 - val_loss: 0.2277 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2291 - acc: 0.9477 - val_loss: 0.2196 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 4s 145us/step - loss: 0.2224 - acc: 0.9477 - val_loss: 0.2234 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 4s 145us/step - loss: 0.2254 - acc: 0.9475 - val_loss: 0.2206 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 143us/step - loss: 0.2236 - acc: 0.9477 - val_loss: 0.2180 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2225 - acc: 0.9477 - val_loss: 0.2161 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 4s 160us/step - loss: 0.2165 - acc: 0.9477 - val_loss: 0.2111 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 4s 149us/step - loss: 0.2143 - acc: 0.9477 - val_loss: 0.2137 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2144 - acc: 0.9477 - val_loss: 0.2144 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 148us/step - loss: 0.2120 - acc: 0.9477 - val_loss: 0.2129 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2165 - acc: 0.9477 - val_loss: 0.2147 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 4s 151us/step - loss: 0.2190 - acc: 0.9477 - val_loss: 0.2153 - val_acc: 0.9491\n",
      "Epoch 00014: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_144 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_144 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_623 (Dense)            (None, 77)                913143    \n",
      "_________________________________________________________________\n",
      "dropout_480 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_624 (Dense)            (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dropout_481 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_625 (Dense)            (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dropout_482 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_626 (Dense)            (None, 1)                 78        \n",
      "=================================================================\n",
      "Total params: 925,233\n",
      "Trainable params: 925,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 22s 819us/step - loss: 0.3299 - acc: 0.9452 - val_loss: 0.2571 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2387 - acc: 0.9481 - val_loss: 0.2358 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2283 - acc: 0.9480 - val_loss: 0.2296 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2241 - acc: 0.9480 - val_loss: 0.2253 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2195 - acc: 0.9480 - val_loss: 0.2215 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2160 - acc: 0.9479 - val_loss: 0.2213 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2169 - acc: 0.9481 - val_loss: 0.2173 - val_acc: 0.9476\n",
      "Epoch 8/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2162 - acc: 0.9481 - val_loss: 0.2238 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2149 - acc: 0.9480 - val_loss: 0.2261 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2138 - acc: 0.9481 - val_loss: 0.2133 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2115 - acc: 0.9481 - val_loss: 0.2177 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2145 - acc: 0.9481 - val_loss: 0.2223 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2174 - acc: 0.9481 - val_loss: 0.2156 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2141 - acc: 0.9481 - val_loss: 0.2222 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2143 - acc: 0.9481 - val_loss: 0.2142 - val_acc: 0.9476\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_145 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_145 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_627 (Dense)            (None, 77)                913143    \n",
      "_________________________________________________________________\n",
      "dropout_483 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_628 (Dense)            (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dropout_484 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_629 (Dense)            (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dropout_485 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_630 (Dense)            (None, 1)                 78        \n",
      "=================================================================\n",
      "Total params: 925,233\n",
      "Trainable params: 925,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 23s 839us/step - loss: 0.3006 - acc: 0.9476 - val_loss: 0.2500 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2317 - acc: 0.9487 - val_loss: 0.2400 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2235 - acc: 0.9487 - val_loss: 0.2381 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2181 - acc: 0.9488 - val_loss: 0.2321 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2171 - acc: 0.9488 - val_loss: 0.2372 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2181 - acc: 0.9488 - val_loss: 0.2317 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2205 - acc: 0.9488 - val_loss: 0.2375 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2184 - acc: 0.9488 - val_loss: 0.2329 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2149 - acc: 0.9488 - val_loss: 0.2369 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2174 - acc: 0.9488 - val_loss: 0.2295 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2119 - acc: 0.9488 - val_loss: 0.2382 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2171 - acc: 0.9488 - val_loss: 0.2327 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2091 - acc: 0.9488 - val_loss: 0.2264 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2062 - acc: 0.9488 - val_loss: 0.2307 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2099 - acc: 0.9488 - val_loss: 0.2314 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2093 - acc: 0.9488 - val_loss: 0.2291 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2059 - acc: 0.9488 - val_loss: 0.2240 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2058 - acc: 0.9488 - val_loss: 0.2258 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2069 - acc: 0.9488 - val_loss: 0.2216 - val_acc: 0.9449\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2027 - acc: 0.9488 - val_loss: 0.2243 - val_acc: 0.9449\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2210 - val_acc: 0.9449\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2234 - val_acc: 0.9449\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2237 - val_acc: 0.9449\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2039 - acc: 0.9488 - val_loss: 0.2284 - val_acc: 0.9449\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2050 - acc: 0.9488 - val_loss: 0.2338 - val_acc: 0.9449\n",
      "Epoch 26/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2018 - acc: 0.9488 - val_loss: 0.2198 - val_acc: 0.9449\n",
      "Epoch 27/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2250 - val_acc: 0.9449\n",
      "Epoch 28/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2211 - val_acc: 0.9449\n",
      "Epoch 29/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2014 - acc: 0.9488 - val_loss: 0.2223 - val_acc: 0.9449\n",
      "Epoch 30/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2239 - val_acc: 0.9449\n",
      "Epoch 31/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2011 - acc: 0.9488 - val_loss: 0.2223 - val_acc: 0.9449\n",
      "Epoch 00031: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_146 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_146 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_631 (Dense)            (None, 77)                913143    \n",
      "_________________________________________________________________\n",
      "dropout_486 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_632 (Dense)            (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dropout_487 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_633 (Dense)            (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dropout_488 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_634 (Dense)            (None, 1)                 78        \n",
      "=================================================================\n",
      "Total params: 925,233\n",
      "Trainable params: 925,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 23s 846us/step - loss: 0.3129 - acc: 0.9431 - val_loss: 0.2302 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2389 - acc: 0.9464 - val_loss: 0.2203 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2303 - acc: 0.9466 - val_loss: 0.2170 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2242 - acc: 0.9466 - val_loss: 0.2189 - val_acc: 0.9533\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2236 - acc: 0.9466 - val_loss: 0.2183 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2220 - acc: 0.9466 - val_loss: 0.2116 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2214 - acc: 0.9466 - val_loss: 0.2095 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2200 - acc: 0.9466 - val_loss: 0.2018 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2145 - acc: 0.9466 - val_loss: 0.2100 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2172 - acc: 0.9466 - val_loss: 0.2092 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2179 - acc: 0.9466 - val_loss: 0.2058 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2119 - acc: 0.9466 - val_loss: 0.2033 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2126 - acc: 0.9466 - val_loss: 0.2048 - val_acc: 0.9534\n",
      "Epoch 00013: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_147 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_147 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_635 (Dense)            (None, 77)                913143    \n",
      "_________________________________________________________________\n",
      "dropout_489 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_636 (Dense)            (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dropout_490 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_637 (Dense)            (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dropout_491 (Dropout)        (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_638 (Dense)            (None, 1)                 78        \n",
      "=================================================================\n",
      "Total params: 925,233\n",
      "Trainable params: 925,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 23s 843us/step - loss: 0.3273 - acc: 0.9472 - val_loss: 0.2547 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2368 - acc: 0.9487 - val_loss: 0.2458 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2310 - acc: 0.9486 - val_loss: 0.2376 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2191 - acc: 0.9488 - val_loss: 0.2350 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2167 - acc: 0.9488 - val_loss: 0.2413 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2197 - acc: 0.9488 - val_loss: 0.2348 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2171 - acc: 0.9488 - val_loss: 0.2368 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2163 - acc: 0.9488 - val_loss: 0.2360 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2153 - acc: 0.9488 - val_loss: 0.2341 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2135 - acc: 0.9488 - val_loss: 0.2366 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2123 - acc: 0.9488 - val_loss: 0.2285 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2073 - acc: 0.9488 - val_loss: 0.2285 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 186us/step - loss: 0.2106 - acc: 0.9488 - val_loss: 0.2358 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2097 - acc: 0.9488 - val_loss: 0.2320 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2121 - acc: 0.9488 - val_loss: 0.2246 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2113 - acc: 0.9488 - val_loss: 0.2283 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2129 - acc: 0.9488 - val_loss: 0.2310 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2122 - acc: 0.9488 - val_loss: 0.2317 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2092 - acc: 0.9488 - val_loss: 0.2283 - val_acc: 0.9449\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2082 - acc: 0.9488 - val_loss: 0.2330 - val_acc: 0.9449\n",
      "Epoch 00020: early stopping\n",
      "-0.7156176579638517 0.017833673711552138\n",
      "\n",
      " \t ::: 11 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.02207447397983681, 'dense_units': 10, 'dense_layers': 4}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_148 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_148 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_639 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_492 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_640 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_493 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_641 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_494 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_642 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_495 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_643 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 118,931\n",
      "Trainable params: 118,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 23s 874us/step - loss: 0.2537 - acc: 0.9477 - val_loss: 0.2152 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 144us/step - loss: 0.2105 - acc: 0.9477 - val_loss: 0.2041 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2076 - acc: 0.9477 - val_loss: 0.2021 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 144us/step - loss: 0.2068 - acc: 0.9477 - val_loss: 0.2025 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2066 - acc: 0.9477 - val_loss: 0.2026 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2067 - acc: 0.9477 - val_loss: 0.2020 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 146us/step - loss: 0.2063 - acc: 0.9477 - val_loss: 0.2015 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 144us/step - loss: 0.2061 - acc: 0.9477 - val_loss: 0.2021 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 4s 155us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2032 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 5s 182us/step - loss: 0.2057 - acc: 0.9477 - val_loss: 0.2013 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 5s 175us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 157us/step - loss: 0.2057 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 5s 189us/step - loss: 0.2055 - acc: 0.9477 - val_loss: 0.2012 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 6s 218us/step - loss: 0.2055 - acc: 0.9477 - val_loss: 0.2017 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 4s 167us/step - loss: 0.2053 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 5s 177us/step - loss: 0.2055 - acc: 0.9477 - val_loss: 0.2013 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 4s 155us/step - loss: 0.2054 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 5s 185us/step - loss: 0.2054 - acc: 0.9477 - val_loss: 0.2014 - val_acc: 0.9491\n",
      "Epoch 00018: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_149 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_149 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_644 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_496 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_645 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_497 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_646 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_498 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_647 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_499 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_648 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 118,931\n",
      "Trainable params: 118,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 25s 937us/step - loss: 0.2517 - acc: 0.9480 - val_loss: 0.2274 - val_acc: 0.9474\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2119 - acc: 0.9481 - val_loss: 0.2076 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2067 - acc: 0.9481 - val_loss: 0.2072 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2063 - acc: 0.9481 - val_loss: 0.2062 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2060 - acc: 0.9481 - val_loss: 0.2066 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2054 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2055 - acc: 0.9481 - val_loss: 0.2059 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2051 - acc: 0.9481 - val_loss: 0.2065 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2047 - acc: 0.9481 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2047 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2045 - acc: 0.9481 - val_loss: 0.2059 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2045 - acc: 0.9481 - val_loss: 0.2059 - val_acc: 0.9476\n",
      "Epoch 00012: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_150 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_150 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_649 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_500 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_650 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_501 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_651 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_502 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_652 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_503 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_653 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 118,931\n",
      "Trainable params: 118,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 24s 888us/step - loss: 0.2529 - acc: 0.9473 - val_loss: 0.2358 - val_acc: 0.9449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2152 - acc: 0.9488 - val_loss: 0.2215 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2058 - acc: 0.9488 - val_loss: 0.2194 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2045 - acc: 0.9488 - val_loss: 0.2140 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2161 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2036 - acc: 0.9488 - val_loss: 0.2135 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2032 - acc: 0.9488 - val_loss: 0.2142 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2136 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2029 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2144 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2027 - acc: 0.9488 - val_loss: 0.2139 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2133 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2133 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2143 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2139 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2133 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2135 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 143us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2135 - val_acc: 0.9449\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2136 - val_acc: 0.9449\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2133 - val_acc: 0.9449\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2022 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 00022: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_151 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_151 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_654 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_504 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_655 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_505 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_656 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_506 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_657 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_507 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_658 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 118,931\n",
      "Trainable params: 118,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 28s 1ms/step - loss: 0.2531 - acc: 0.9464 - val_loss: 0.2055 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2210 - acc: 0.9466 - val_loss: 0.1971 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2130 - acc: 0.9466 - val_loss: 0.1914 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2113 - acc: 0.9466 - val_loss: 0.1921 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2106 - acc: 0.9466 - val_loss: 0.1932 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2098 - acc: 0.9466 - val_loss: 0.1900 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2097 - acc: 0.9466 - val_loss: 0.1888 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 181us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.1891 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 183us/step - loss: 0.2093 - acc: 0.9466 - val_loss: 0.1890 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2089 - acc: 0.9466 - val_loss: 0.1886 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2088 - acc: 0.9466 - val_loss: 0.1909 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2091 - acc: 0.9466 - val_loss: 0.1886 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2085 - acc: 0.9466 - val_loss: 0.1906 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2085 - acc: 0.9466 - val_loss: 0.1896 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2085 - acc: 0.9466 - val_loss: 0.1893 - val_acc: 0.9534\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_152 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_152 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_659 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_508 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_660 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_509 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_661 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_510 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_662 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_511 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_663 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 118,931\n",
      "Trainable params: 118,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 24s 877us/step - loss: 0.2640 - acc: 0.9481 - val_loss: 0.2309 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2140 - acc: 0.9488 - val_loss: 0.2180 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 183us/step - loss: 0.2053 - acc: 0.9488 - val_loss: 0.2144 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 191us/step - loss: 0.2040 - acc: 0.9488 - val_loss: 0.2138 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2037 - acc: 0.9488 - val_loss: 0.2164 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2160 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2154 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2136 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2156 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2028 - acc: 0.9488 - val_loss: 0.2133 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2140 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2142 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 185us/step - loss: 0.2023 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 180us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2135 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 193us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2133 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2023 - acc: 0.9488 - val_loss: 0.2133 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2134 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2136 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2025 - acc: 0.9488 - val_loss: 0.2133 - val_acc: 0.9449\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2023 - acc: 0.9488 - val_loss: 0.2138 - val_acc: 0.9449\n",
      "Epoch 00020: early stopping\n",
      "-0.4999370257427236 7.674357982675126e-05\n",
      "\n",
      " \t ::: 12 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.0016196486691479346, 'dense_units': 10, 'dense_layers': 2}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_153 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_153 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_664 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_512 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_665 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_513 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_666 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 118,711\n",
      "Trainable params: 118,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 26s 982us/step - loss: 0.5107 - acc: 0.9300 - val_loss: 0.2773 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 167us/step - loss: 0.2367 - acc: 0.9473 - val_loss: 0.2092 - val_acc: 0.9489\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 5s 169us/step - loss: 0.2156 - acc: 0.9476 - val_loss: 0.2114 - val_acc: 0.9489\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 5s 172us/step - loss: 0.2135 - acc: 0.9476 - val_loss: 0.2111 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 5s 171us/step - loss: 0.2099 - acc: 0.9476 - val_loss: 0.2125 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 5s 169us/step - loss: 0.2086 - acc: 0.9477 - val_loss: 0.2112 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 161us/step - loss: 0.2071 - acc: 0.9477 - val_loss: 0.2112 - val_acc: 0.9491\n",
      "Epoch 00007: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_154 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_154 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_667 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_514 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_668 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_515 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_669 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 118,711\n",
      "Trainable params: 118,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 27s 1ms/step - loss: 0.3829 - acc: 0.9421 - val_loss: 0.2455 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2259 - acc: 0.9479 - val_loss: 0.2219 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2165 - acc: 0.9473 - val_loss: 0.2217 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2083 - acc: 0.9479 - val_loss: 0.2160 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2036 - acc: 0.9481 - val_loss: 0.2177 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 144us/step - loss: 0.2034 - acc: 0.9480 - val_loss: 0.2157 - val_acc: 0.9474\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2012 - acc: 0.9481 - val_loss: 0.2145 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 196us/step - loss: 0.2012 - acc: 0.9477 - val_loss: 0.2130 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 190us/step - loss: 0.1998 - acc: 0.9479 - val_loss: 0.2114 - val_acc: 0.9471\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.1984 - acc: 0.9480 - val_loss: 0.2124 - val_acc: 0.9476\n",
      "Epoch 11/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.1973 - acc: 0.9481 - val_loss: 0.2101 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.1984 - acc: 0.9481 - val_loss: 0.2126 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.1966 - acc: 0.9482 - val_loss: 0.2117 - val_acc: 0.9477\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.1975 - acc: 0.9481 - val_loss: 0.2103 - val_acc: 0.9477\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.1969 - acc: 0.9481 - val_loss: 0.2077 - val_acc: 0.9474\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 5s 184us/step - loss: 0.1976 - acc: 0.9480 - val_loss: 0.2093 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.1970 - acc: 0.9482 - val_loss: 0.2108 - val_acc: 0.9479\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 187us/step - loss: 0.1965 - acc: 0.9483 - val_loss: 0.2065 - val_acc: 0.9480\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.1964 - acc: 0.9485 - val_loss: 0.2077 - val_acc: 0.9479\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.1954 - acc: 0.9483 - val_loss: 0.2119 - val_acc: 0.9469\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 5s 187us/step - loss: 0.1958 - acc: 0.9482 - val_loss: 0.2103 - val_acc: 0.9474\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.1962 - acc: 0.9483 - val_loss: 0.2103 - val_acc: 0.9476\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 5s 181us/step - loss: 0.1951 - acc: 0.9480 - val_loss: 0.2113 - val_acc: 0.9473\n",
      "Epoch 00023: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_155 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_155 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_670 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_516 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_671 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_517 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_672 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 118,711\n",
      "Trainable params: 118,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 27s 998us/step - loss: 0.3177 - acc: 0.9482 - val_loss: 0.2425 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2174 - acc: 0.9487 - val_loss: 0.2371 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2109 - acc: 0.9487 - val_loss: 0.2387 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2054 - acc: 0.9486 - val_loss: 0.2276 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2030 - acc: 0.9488 - val_loss: 0.2363 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2007 - acc: 0.9487 - val_loss: 0.2261 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 141us/step - loss: 0.2027 - acc: 0.9486 - val_loss: 0.2274 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2017 - acc: 0.9486 - val_loss: 0.2283 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2005 - acc: 0.9488 - val_loss: 0.2286 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2003 - acc: 0.9488 - val_loss: 0.2342 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2005 - acc: 0.9488 - val_loss: 0.2320 - val_acc: 0.9449\n",
      "Epoch 00011: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_156 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_156 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_673 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_518 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_674 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_519 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_675 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 118,711\n",
      "Trainable params: 118,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 25s 945us/step - loss: 0.3396 - acc: 0.9459 - val_loss: 0.2159 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2274 - acc: 0.9465 - val_loss: 0.2081 - val_acc: 0.9524\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2166 - acc: 0.9463 - val_loss: 0.2020 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2118 - acc: 0.9463 - val_loss: 0.2040 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 137us/step - loss: 0.2099 - acc: 0.9466 - val_loss: 0.2029 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 3s 130us/step - loss: 0.2075 - acc: 0.9465 - val_loss: 0.2002 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 130us/step - loss: 0.2069 - acc: 0.9466 - val_loss: 0.2033 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 3s 129us/step - loss: 0.2068 - acc: 0.9465 - val_loss: 0.2030 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2054 - acc: 0.9466 - val_loss: 0.2001 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2028 - acc: 0.9468 - val_loss: 0.2014 - val_acc: 0.9531\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2071 - acc: 0.9463 - val_loss: 0.2046 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2068 - acc: 0.9466 - val_loss: 0.1966 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2020 - acc: 0.9466 - val_loss: 0.1994 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 131us/step - loss: 0.2036 - acc: 0.9466 - val_loss: 0.1990 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2041 - acc: 0.9466 - val_loss: 0.2069 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2064 - acc: 0.9466 - val_loss: 0.2064 - val_acc: 0.9534\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2052 - acc: 0.9466 - val_loss: 0.2031 - val_acc: 0.9534\n",
      "Epoch 00017: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_157 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_157 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_676 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_520 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_677 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_521 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_678 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 118,711\n",
      "Trainable params: 118,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 26s 973us/step - loss: 0.4051 - acc: 0.9407 - val_loss: 0.2527 - val_acc: 0.9448\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 134us/step - loss: 0.2229 - acc: 0.9486 - val_loss: 0.2390 - val_acc: 0.9448\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 3s 127us/step - loss: 0.2123 - acc: 0.9488 - val_loss: 0.2386 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2069 - acc: 0.9486 - val_loss: 0.2316 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 135us/step - loss: 0.2054 - acc: 0.9487 - val_loss: 0.2333 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2053 - acc: 0.9486 - val_loss: 0.2338 - val_acc: 0.9446\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 3s 125us/step - loss: 0.2046 - acc: 0.9487 - val_loss: 0.2305 - val_acc: 0.9448\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 132us/step - loss: 0.2031 - acc: 0.9485 - val_loss: 0.2326 - val_acc: 0.9448\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2026 - acc: 0.9488 - val_loss: 0.2339 - val_acc: 0.9448\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 3s 126us/step - loss: 0.2012 - acc: 0.9489 - val_loss: 0.2362 - val_acc: 0.9448\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 3s 128us/step - loss: 0.2004 - acc: 0.9486 - val_loss: 0.2349 - val_acc: 0.9445\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 136us/step - loss: 0.2006 - acc: 0.9486 - val_loss: 0.2364 - val_acc: 0.9445\n",
      "Epoch 00012: early stopping\n",
      "-0.7193966414238501 0.01847169349741113\n",
      "\n",
      " \t ::: 13 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.0033633112608423047, 'dense_units': 88, 'dense_layers': 2}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_158 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_158 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_679 (Dense)            (None, 88)                1043592   \n",
      "_________________________________________________________________\n",
      "dropout_522 (Dropout)        (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_680 (Dense)            (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dropout_523 (Dropout)        (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_681 (Dense)            (None, 1)                 89        \n",
      "=================================================================\n",
      "Total params: 1,051,513\n",
      "Trainable params: 1,051,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 24s 889us/step - loss: 0.3100 - acc: 0.9447 - val_loss: 0.2431 - val_acc: 0.9494\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 143us/step - loss: 0.2475 - acc: 0.9473 - val_loss: 0.2526 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 4s 135us/step - loss: 0.2450 - acc: 0.9476 - val_loss: 0.2393 - val_acc: 0.9489\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 134us/step - loss: 0.2353 - acc: 0.9476 - val_loss: 0.2309 - val_acc: 0.9488\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 4s 134us/step - loss: 0.2288 - acc: 0.9477 - val_loss: 0.2235 - val_acc: 0.9489\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2232 - acc: 0.9477 - val_loss: 0.2244 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2200 - acc: 0.9477 - val_loss: 0.2262 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2207 - acc: 0.9477 - val_loss: 0.2199 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 4s 144us/step - loss: 0.2201 - acc: 0.9477 - val_loss: 0.2259 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 4s 145us/step - loss: 0.2194 - acc: 0.9477 - val_loss: 0.2223 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 4s 143us/step - loss: 0.2174 - acc: 0.9477 - val_loss: 0.2164 - val_acc: 0.9489\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 135us/step - loss: 0.2147 - acc: 0.9477 - val_loss: 0.2090 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2181 - acc: 0.9477 - val_loss: 0.2145 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 4s 133us/step - loss: 0.2166 - acc: 0.9476 - val_loss: 0.2120 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 4s 147us/step - loss: 0.2108 - acc: 0.9477 - val_loss: 0.2167 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 4s 136us/step - loss: 0.2114 - acc: 0.9477 - val_loss: 0.2071 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 4s 142us/step - loss: 0.2127 - acc: 0.9477 - val_loss: 0.2293 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 4s 141us/step - loss: 0.2188 - acc: 0.9477 - val_loss: 0.2239 - val_acc: 0.9491\n",
      "Epoch 19/200\n",
      "26868/26868 [==============================] - 4s 139us/step - loss: 0.2162 - acc: 0.9477 - val_loss: 0.2149 - val_acc: 0.9488\n",
      "Epoch 20/200\n",
      "26868/26868 [==============================] - 4s 142us/step - loss: 0.2133 - acc: 0.9477 - val_loss: 0.2133 - val_acc: 0.9491\n",
      "Epoch 21/200\n",
      "26868/26868 [==============================] - 4s 153us/step - loss: 0.2112 - acc: 0.9476 - val_loss: 0.2151 - val_acc: 0.9491\n",
      "Epoch 00021: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_159 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_159 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_682 (Dense)            (None, 88)                1043592   \n",
      "_________________________________________________________________\n",
      "dropout_524 (Dropout)        (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_683 (Dense)            (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dropout_525 (Dropout)        (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_684 (Dense)            (None, 1)                 89        \n",
      "=================================================================\n",
      "Total params: 1,051,513\n",
      "Trainable params: 1,051,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 29s 1ms/step - loss: 0.3040 - acc: 0.9464 - val_loss: 0.2491 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2441 - acc: 0.9480 - val_loss: 0.2357 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2371 - acc: 0.9481 - val_loss: 0.2375 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2326 - acc: 0.9480 - val_loss: 0.2372 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2286 - acc: 0.9480 - val_loss: 0.2375 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2277 - acc: 0.9481 - val_loss: 0.2265 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2218 - acc: 0.9481 - val_loss: 0.2281 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 188us/step - loss: 0.2213 - acc: 0.9479 - val_loss: 0.2298 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2208 - acc: 0.9480 - val_loss: 0.2251 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2173 - acc: 0.9481 - val_loss: 0.2258 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2208 - acc: 0.9479 - val_loss: 0.2258 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2238 - acc: 0.9481 - val_loss: 0.2219 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 183us/step - loss: 0.2148 - acc: 0.9481 - val_loss: 0.2200 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 193us/step - loss: 0.2160 - acc: 0.9481 - val_loss: 0.2171 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2148 - acc: 0.9481 - val_loss: 0.2153 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2139 - acc: 0.9481 - val_loss: 0.2231 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2147 - acc: 0.9481 - val_loss: 0.2154 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2128 - acc: 0.9481 - val_loss: 0.2184 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2127 - acc: 0.9481 - val_loss: 0.2198 - val_acc: 0.9476\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2123 - acc: 0.9481 - val_loss: 0.2169 - val_acc: 0.9476\n",
      "Epoch 00020: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_160 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_160 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_685 (Dense)            (None, 88)                1043592   \n",
      "_________________________________________________________________\n",
      "dropout_526 (Dropout)        (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_686 (Dense)            (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dropout_527 (Dropout)        (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_687 (Dense)            (None, 1)                 89        \n",
      "=================================================================\n",
      "Total params: 1,051,513\n",
      "Trainable params: 1,051,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 25s 947us/step - loss: 0.3019 - acc: 0.9483 - val_loss: 0.2551 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 142us/step - loss: 0.2401 - acc: 0.9488 - val_loss: 0.2481 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2336 - acc: 0.9488 - val_loss: 0.2502 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2260 - acc: 0.9488 - val_loss: 0.2428 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2227 - acc: 0.9488 - val_loss: 0.2478 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2240 - acc: 0.9486 - val_loss: 0.2355 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 191us/step - loss: 0.2194 - acc: 0.9487 - val_loss: 0.2408 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2155 - acc: 0.9488 - val_loss: 0.2377 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2134 - acc: 0.9488 - val_loss: 0.2343 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2160 - acc: 0.9488 - val_loss: 0.2372 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2137 - acc: 0.9488 - val_loss: 0.2302 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2106 - acc: 0.9488 - val_loss: 0.2298 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2126 - acc: 0.9488 - val_loss: 0.2371 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2093 - acc: 0.9487 - val_loss: 0.2324 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2132 - acc: 0.9488 - val_loss: 0.2322 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2103 - acc: 0.9486 - val_loss: 0.2312 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2085 - acc: 0.9487 - val_loss: 0.2293 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2120 - acc: 0.9487 - val_loss: 0.2370 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2097 - acc: 0.9488 - val_loss: 0.2299 - val_acc: 0.9449\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 146us/step - loss: 0.2095 - acc: 0.9487 - val_loss: 0.2247 - val_acc: 0.9449\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2051 - acc: 0.9488 - val_loss: 0.2283 - val_acc: 0.9449\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 5s 183us/step - loss: 0.2059 - acc: 0.9487 - val_loss: 0.2266 - val_acc: 0.9449\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2043 - acc: 0.9486 - val_loss: 0.2293 - val_acc: 0.9449\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2061 - acc: 0.9488 - val_loss: 0.2254 - val_acc: 0.9449\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 6s 216us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2214 - val_acc: 0.9449\n",
      "Epoch 26/200\n",
      "26869/26869 [==============================] - 5s 199us/step - loss: 0.2047 - acc: 0.9488 - val_loss: 0.2198 - val_acc: 0.9449\n",
      "Epoch 27/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2026 - acc: 0.9487 - val_loss: 0.2214 - val_acc: 0.9449\n",
      "Epoch 28/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.2018 - acc: 0.9488 - val_loss: 0.2282 - val_acc: 0.9449\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2252 - val_acc: 0.9449\n",
      "Epoch 30/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2034 - acc: 0.9487 - val_loss: 0.2262 - val_acc: 0.9449\n",
      "Epoch 31/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2058 - acc: 0.9487 - val_loss: 0.2254 - val_acc: 0.9449\n",
      "Epoch 00031: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_161 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_161 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_688 (Dense)            (None, 88)                1043592   \n",
      "_________________________________________________________________\n",
      "dropout_528 (Dropout)        (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_689 (Dense)            (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dropout_529 (Dropout)        (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_690 (Dense)            (None, 1)                 89        \n",
      "=================================================================\n",
      "Total params: 1,051,513\n",
      "Trainable params: 1,051,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 26s 975us/step - loss: 0.3043 - acc: 0.9440 - val_loss: 0.2402 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 6s 226us/step - loss: 0.2504 - acc: 0.9466 - val_loss: 0.2307 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2408 - acc: 0.9466 - val_loss: 0.2209 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2354 - acc: 0.9464 - val_loss: 0.2246 - val_acc: 0.9522\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 197us/step - loss: 0.2351 - acc: 0.9466 - val_loss: 0.2210 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 189us/step - loss: 0.2334 - acc: 0.9466 - val_loss: 0.2228 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2301 - acc: 0.9466 - val_loss: 0.2251 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 181us/step - loss: 0.2246 - acc: 0.9466 - val_loss: 0.2167 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 194us/step - loss: 0.2223 - acc: 0.9466 - val_loss: 0.2071 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2209 - acc: 0.9466 - val_loss: 0.2126 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2215 - acc: 0.9466 - val_loss: 0.2100 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.2153 - acc: 0.9466 - val_loss: 0.2101 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2145 - acc: 0.9466 - val_loss: 0.2086 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2149 - acc: 0.9466 - val_loss: 0.2037 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2116 - acc: 0.9467 - val_loss: 0.2077 - val_acc: 0.9534\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2157 - acc: 0.9466 - val_loss: 0.2050 - val_acc: 0.9534\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.2171 - acc: 0.9466 - val_loss: 0.2066 - val_acc: 0.9534\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2204 - acc: 0.9465 - val_loss: 0.2082 - val_acc: 0.9534\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 5s 186us/step - loss: 0.2161 - acc: 0.9466 - val_loss: 0.2083 - val_acc: 0.9534\n",
      "Epoch 00019: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_162 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_162 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_691 (Dense)            (None, 88)                1043592   \n",
      "_________________________________________________________________\n",
      "dropout_530 (Dropout)        (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_692 (Dense)            (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dropout_531 (Dropout)        (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_693 (Dense)            (None, 1)                 89        \n",
      "=================================================================\n",
      "Total params: 1,051,513\n",
      "Trainable params: 1,051,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 27s 989us/step - loss: 0.2956 - acc: 0.9467 - val_loss: 0.2584 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 188us/step - loss: 0.2379 - acc: 0.9487 - val_loss: 0.2521 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2323 - acc: 0.9487 - val_loss: 0.2456 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2273 - acc: 0.9488 - val_loss: 0.2486 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2315 - acc: 0.9488 - val_loss: 0.2508 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 197us/step - loss: 0.2256 - acc: 0.9488 - val_loss: 0.2435 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2156 - acc: 0.9488 - val_loss: 0.2421 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2172 - acc: 0.9488 - val_loss: 0.2412 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2162 - acc: 0.9488 - val_loss: 0.2417 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2158 - acc: 0.9488 - val_loss: 0.2422 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2173 - acc: 0.9488 - val_loss: 0.2436 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2189 - acc: 0.9486 - val_loss: 0.2444 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 191us/step - loss: 0.2150 - acc: 0.9487 - val_loss: 0.2417 - val_acc: 0.9448\n",
      "Epoch 00013: early stopping\n",
      "-0.7091104950862109 0.025178326739822593\n",
      "\n",
      " \t ::: 14 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.001, 'dense_units': 100, 'dense_layers': 2}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_163 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_163 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_694 (Dense)            (None, 100)               1185900   \n",
      "_________________________________________________________________\n",
      "dropout_532 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_695 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_533 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_696 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,196,101\n",
      "Trainable params: 1,196,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 29s 1ms/step - loss: 0.3608 - acc: 0.9464 - val_loss: 0.2420 - val_acc: 0.9486\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 5s 185us/step - loss: 0.2356 - acc: 0.9477 - val_loss: 0.2186 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 5s 182us/step - loss: 0.2232 - acc: 0.9476 - val_loss: 0.2191 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 5s 176us/step - loss: 0.2157 - acc: 0.9477 - val_loss: 0.2175 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 5s 198us/step - loss: 0.2165 - acc: 0.9468 - val_loss: 0.2209 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 5s 202us/step - loss: 0.2158 - acc: 0.9474 - val_loss: 0.2211 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 5s 180us/step - loss: 0.2130 - acc: 0.9477 - val_loss: 0.2115 - val_acc: 0.9485\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 5s 182us/step - loss: 0.2142 - acc: 0.9472 - val_loss: 0.2155 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 5s 200us/step - loss: 0.2085 - acc: 0.9477 - val_loss: 0.2164 - val_acc: 0.9489\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 5s 175us/step - loss: 0.2072 - acc: 0.9477 - val_loss: 0.2159 - val_acc: 0.9488\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 5s 168us/step - loss: 0.2063 - acc: 0.9477 - val_loss: 0.2125 - val_acc: 0.9473\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 5s 174us/step - loss: 0.2070 - acc: 0.9476 - val_loss: 0.2107 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 157us/step - loss: 0.2072 - acc: 0.9476 - val_loss: 0.2133 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 4s 157us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2127 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 5s 169us/step - loss: 0.2045 - acc: 0.9477 - val_loss: 0.2151 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 4s 160us/step - loss: 0.2040 - acc: 0.9477 - val_loss: 0.2158 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 4s 167us/step - loss: 0.2025 - acc: 0.9477 - val_loss: 0.2146 - val_acc: 0.9491\n",
      "Epoch 00017: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_164 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_164 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_697 (Dense)            (None, 100)               1185900   \n",
      "_________________________________________________________________\n",
      "dropout_534 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_698 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_535 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_699 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,196,101\n",
      "Trainable params: 1,196,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 28s 1ms/step - loss: 0.3557 - acc: 0.9474 - val_loss: 0.2539 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2321 - acc: 0.9481 - val_loss: 0.2297 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 183us/step - loss: 0.2215 - acc: 0.9482 - val_loss: 0.2280 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 190us/step - loss: 0.2157 - acc: 0.9478 - val_loss: 0.2267 - val_acc: 0.9466\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2132 - acc: 0.9480 - val_loss: 0.2231 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2090 - acc: 0.9479 - val_loss: 0.2176 - val_acc: 0.9471\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2060 - acc: 0.9480 - val_loss: 0.2155 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2067 - acc: 0.9480 - val_loss: 0.2194 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2053 - acc: 0.9481 - val_loss: 0.2186 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2048 - acc: 0.9482 - val_loss: 0.2155 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2036 - acc: 0.9480 - val_loss: 0.2149 - val_acc: 0.9473\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2021 - acc: 0.9481 - val_loss: 0.2134 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2013 - acc: 0.9480 - val_loss: 0.2179 - val_acc: 0.9474\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 149us/step - loss: 0.2013 - acc: 0.9479 - val_loss: 0.2144 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.1990 - acc: 0.9481 - val_loss: 0.2131 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.1997 - acc: 0.9481 - val_loss: 0.2168 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2002 - acc: 0.9481 - val_loss: 0.2145 - val_acc: 0.9474\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2023 - acc: 0.9481 - val_loss: 0.2119 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.1994 - acc: 0.9481 - val_loss: 0.2151 - val_acc: 0.9476\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.1993 - acc: 0.9481 - val_loss: 0.2120 - val_acc: 0.9476\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.1988 - acc: 0.9481 - val_loss: 0.2162 - val_acc: 0.9470\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.1999 - acc: 0.9480 - val_loss: 0.2127 - val_acc: 0.9476\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.1986 - acc: 0.9481 - val_loss: 0.2142 - val_acc: 0.9476\n",
      "Epoch 00023: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_165 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_165 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_700 (Dense)            (None, 100)               1185900   \n",
      "_________________________________________________________________\n",
      "dropout_536 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_701 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_537 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_702 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,196,101\n",
      "Trainable params: 1,196,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 27s 1ms/step - loss: 0.3522 - acc: 0.9470 - val_loss: 0.2534 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 188us/step - loss: 0.2263 - acc: 0.9486 - val_loss: 0.2443 - val_acc: 0.9443\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 190us/step - loss: 0.2182 - acc: 0.9484 - val_loss: 0.2386 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2122 - acc: 0.9487 - val_loss: 0.2471 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2101 - acc: 0.9486 - val_loss: 0.2420 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2095 - acc: 0.9487 - val_loss: 0.2420 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2088 - acc: 0.9486 - val_loss: 0.2385 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 6s 207us/step - loss: 0.2081 - acc: 0.9488 - val_loss: 0.2411 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2067 - acc: 0.9488 - val_loss: 0.2397 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2049 - acc: 0.9488 - val_loss: 0.2368 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2064 - acc: 0.9487 - val_loss: 0.2417 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2010 - acc: 0.9488 - val_loss: 0.2485 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2059 - acc: 0.9486 - val_loss: 0.2456 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 7s 252us/step - loss: 0.2011 - acc: 0.9487 - val_loss: 0.2440 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.1993 - acc: 0.9487 - val_loss: 0.2433 - val_acc: 0.9449\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_166 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_166 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_703 (Dense)            (None, 100)               1185900   \n",
      "_________________________________________________________________\n",
      "dropout_538 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_704 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_539 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_705 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,196,101\n",
      "Trainable params: 1,196,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 26s 952us/step - loss: 0.3555 - acc: 0.9434 - val_loss: 0.2347 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2361 - acc: 0.9464 - val_loss: 0.2147 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 138us/step - loss: 0.2243 - acc: 0.9466 - val_loss: 0.2116 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2213 - acc: 0.9467 - val_loss: 0.2093 - val_acc: 0.9533\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 200us/step - loss: 0.2152 - acc: 0.9469 - val_loss: 0.2056 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2150 - acc: 0.9464 - val_loss: 0.2098 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 139us/step - loss: 0.2126 - acc: 0.9464 - val_loss: 0.2113 - val_acc: 0.9533\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2105 - acc: 0.9464 - val_loss: 0.2128 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 140us/step - loss: 0.2108 - acc: 0.9466 - val_loss: 0.2108 - val_acc: 0.9531\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 145us/step - loss: 0.2103 - acc: 0.9464 - val_loss: 0.2061 - val_acc: 0.9534\n",
      "Epoch 00010: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_167 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_167 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_706 (Dense)            (None, 100)               1185900   \n",
      "_________________________________________________________________\n",
      "dropout_540 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_707 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_541 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_708 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,196,101\n",
      "Trainable params: 1,196,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 27s 994us/step - loss: 0.3512 - acc: 0.9463 - val_loss: 0.2662 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 6s 220us/step - loss: 0.2300 - acc: 0.9484 - val_loss: 0.2560 - val_acc: 0.9434\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2216 - acc: 0.9485 - val_loss: 0.2450 - val_acc: 0.9448\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2166 - acc: 0.9489 - val_loss: 0.2409 - val_acc: 0.9448\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2146 - acc: 0.9484 - val_loss: 0.2483 - val_acc: 0.9446\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2137 - acc: 0.9488 - val_loss: 0.2446 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2090 - acc: 0.9492 - val_loss: 0.2459 - val_acc: 0.9446\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2105 - acc: 0.9492 - val_loss: 0.2473 - val_acc: 0.9445\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2073 - acc: 0.9492 - val_loss: 0.2508 - val_acc: 0.9448\n",
      "Epoch 00009: early stopping\n",
      "-0.7258315869351473 0.025535153942532765\n",
      "\n",
      " \t ::: 15 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.001, 'dense_units': 69, 'dense_layers': 4}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_168 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_168 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_709 (Dense)            (None, 69)                818271    \n",
      "_________________________________________________________________\n",
      "dropout_542 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_710 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_543 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_711 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_544 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_712 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_545 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_713 (Dense)            (None, 1)                 70        \n",
      "=================================================================\n",
      "Total params: 832,831\n",
      "Trainable params: 832,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 26s 982us/step - loss: 0.4328 - acc: 0.9437 - val_loss: 0.2763 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 5s 183us/step - loss: 0.2574 - acc: 0.9476 - val_loss: 0.2341 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 5s 202us/step - loss: 0.2323 - acc: 0.9477 - val_loss: 0.2239 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 166us/step - loss: 0.2231 - acc: 0.9477 - val_loss: 0.2166 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 4s 153us/step - loss: 0.2185 - acc: 0.9477 - val_loss: 0.2167 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 5s 170us/step - loss: 0.2143 - acc: 0.9477 - val_loss: 0.2186 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 152us/step - loss: 0.2123 - acc: 0.9477 - val_loss: 0.2164 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 156us/step - loss: 0.2111 - acc: 0.9477 - val_loss: 0.2184 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 5s 187us/step - loss: 0.2116 - acc: 0.9477 - val_loss: 0.2158 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 4s 159us/step - loss: 0.2072 - acc: 0.9477 - val_loss: 0.2141 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 5s 178us/step - loss: 0.2078 - acc: 0.9477 - val_loss: 0.2185 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 5s 171us/step - loss: 0.2076 - acc: 0.9477 - val_loss: 0.2177 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 152us/step - loss: 0.2073 - acc: 0.9477 - val_loss: 0.2113 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 5s 172us/step - loss: 0.2042 - acc: 0.9477 - val_loss: 0.2101 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 4s 162us/step - loss: 0.2082 - acc: 0.9477 - val_loss: 0.2191 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 5s 186us/step - loss: 0.2079 - acc: 0.9477 - val_loss: 0.2089 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 5s 193us/step - loss: 0.2019 - acc: 0.9477 - val_loss: 0.2084 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 5s 181us/step - loss: 0.1998 - acc: 0.9477 - val_loss: 0.2068 - val_acc: 0.9491\n",
      "Epoch 19/200\n",
      "26868/26868 [==============================] - 6s 208us/step - loss: 0.1976 - acc: 0.9477 - val_loss: 0.2112 - val_acc: 0.9491\n",
      "Epoch 20/200\n",
      "26868/26868 [==============================] - 6s 208us/step - loss: 0.1967 - acc: 0.9477 - val_loss: 0.2085 - val_acc: 0.9491\n",
      "Epoch 21/200\n",
      "26868/26868 [==============================] - 5s 180us/step - loss: 0.1973 - acc: 0.9477 - val_loss: 0.2111 - val_acc: 0.9491\n",
      "Epoch 22/200\n",
      "26868/26868 [==============================] - 4s 167us/step - loss: 0.1985 - acc: 0.9477 - val_loss: 0.2104 - val_acc: 0.9491\n",
      "Epoch 23/200\n",
      "26868/26868 [==============================] - 5s 169us/step - loss: 0.1952 - acc: 0.9477 - val_loss: 0.2103 - val_acc: 0.9491\n",
      "Epoch 00023: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_169 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_169 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_714 (Dense)            (None, 69)                818271    \n",
      "_________________________________________________________________\n",
      "dropout_546 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_715 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_547 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_716 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_548 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_717 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_549 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_718 (Dense)            (None, 1)                 70        \n",
      "=================================================================\n",
      "Total params: 832,831\n",
      "Trainable params: 832,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 30s 1ms/step - loss: 0.4301 - acc: 0.9443 - val_loss: 0.2816 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 200us/step - loss: 0.2525 - acc: 0.9481 - val_loss: 0.2413 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.2264 - acc: 0.9480 - val_loss: 0.2247 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2177 - acc: 0.9481 - val_loss: 0.2191 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 196us/step - loss: 0.2127 - acc: 0.9481 - val_loss: 0.2200 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2099 - acc: 0.9481 - val_loss: 0.2148 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2045 - acc: 0.9481 - val_loss: 0.2114 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2046 - acc: 0.9481 - val_loss: 0.2138 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2005 - acc: 0.9481 - val_loss: 0.2117 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.2040 - acc: 0.9481 - val_loss: 0.2135 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 147us/step - loss: 0.2056 - acc: 0.9481 - val_loss: 0.2113 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2031 - acc: 0.9481 - val_loss: 0.2117 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2015 - acc: 0.9481 - val_loss: 0.2081 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2016 - acc: 0.9481 - val_loss: 0.2068 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2008 - acc: 0.9481 - val_loss: 0.2081 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.1975 - acc: 0.9481 - val_loss: 0.2061 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 150us/step - loss: 0.1969 - acc: 0.9481 - val_loss: 0.2122 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.1962 - acc: 0.9481 - val_loss: 0.2059 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 148us/step - loss: 0.1964 - acc: 0.9481 - val_loss: 0.2100 - val_acc: 0.9476\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.1965 - acc: 0.9481 - val_loss: 0.2103 - val_acc: 0.9476\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 5s 190us/step - loss: 0.1959 - acc: 0.9481 - val_loss: 0.2114 - val_acc: 0.9476\n",
      "Epoch 22/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.1943 - acc: 0.9481 - val_loss: 0.2097 - val_acc: 0.9476\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.1935 - acc: 0.9481 - val_loss: 0.2097 - val_acc: 0.9476\n",
      "Epoch 00023: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_170 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_170 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_719 (Dense)            (None, 69)                818271    \n",
      "_________________________________________________________________\n",
      "dropout_550 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_720 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_551 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_721 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_552 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_722 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_553 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_723 (Dense)            (None, 1)                 70        \n",
      "=================================================================\n",
      "Total params: 832,831\n",
      "Trainable params: 832,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 32s 1ms/step - loss: 0.4256 - acc: 0.9450 - val_loss: 0.3006 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2589 - acc: 0.9487 - val_loss: 0.2734 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2317 - acc: 0.9486 - val_loss: 0.2533 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 195us/step - loss: 0.2179 - acc: 0.9488 - val_loss: 0.2427 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 185us/step - loss: 0.2118 - acc: 0.9488 - val_loss: 0.2429 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2123 - acc: 0.9488 - val_loss: 0.2362 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2102 - acc: 0.9488 - val_loss: 0.2408 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 203us/step - loss: 0.2079 - acc: 0.9488 - val_loss: 0.2392 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2078 - acc: 0.9488 - val_loss: 0.2404 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2055 - acc: 0.9488 - val_loss: 0.2387 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 6s 208us/step - loss: 0.2048 - acc: 0.9488 - val_loss: 0.2388 - val_acc: 0.9449\n",
      "Epoch 00011: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_171 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_171 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_724 (Dense)            (None, 69)                818271    \n",
      "_________________________________________________________________\n",
      "dropout_554 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_725 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_555 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_726 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_556 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_727 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_557 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_728 (Dense)            (None, 1)                 70        \n",
      "=================================================================\n",
      "Total params: 832,831\n",
      "Trainable params: 832,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 31s 1ms/step - loss: 0.4281 - acc: 0.9434 - val_loss: 0.2603 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 181us/step - loss: 0.2573 - acc: 0.9466 - val_loss: 0.2276 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2328 - acc: 0.9466 - val_loss: 0.2186 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2199 - acc: 0.9466 - val_loss: 0.2130 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2145 - acc: 0.9466 - val_loss: 0.2132 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 193us/step - loss: 0.2120 - acc: 0.9466 - val_loss: 0.2081 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 6s 221us/step - loss: 0.2090 - acc: 0.9466 - val_loss: 0.2041 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2083 - acc: 0.9466 - val_loss: 0.2041 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2061 - acc: 0.9466 - val_loss: 0.2058 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 191us/step - loss: 0.2064 - acc: 0.9466 - val_loss: 0.2048 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2047 - acc: 0.9466 - val_loss: 0.2095 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2014 - acc: 0.9466 - val_loss: 0.2052 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2007 - acc: 0.9466 - val_loss: 0.2056 - val_acc: 0.9534\n",
      "Epoch 00013: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_172 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_172 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_729 (Dense)            (None, 69)                818271    \n",
      "_________________________________________________________________\n",
      "dropout_558 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_730 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_559 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_731 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_560 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_732 (Dense)            (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_561 (Dropout)        (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_733 (Dense)            (None, 1)                 70        \n",
      "=================================================================\n",
      "Total params: 832,831\n",
      "Trainable params: 832,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 29s 1ms/step - loss: 0.3922 - acc: 0.9464 - val_loss: 0.2862 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 6s 208us/step - loss: 0.2486 - acc: 0.9483 - val_loss: 0.2574 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 197us/step - loss: 0.2278 - acc: 0.9488 - val_loss: 0.2576 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2211 - acc: 0.9484 - val_loss: 0.2464 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 6s 208us/step - loss: 0.2140 - acc: 0.9488 - val_loss: 0.2411 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 199us/step - loss: 0.2096 - acc: 0.9488 - val_loss: 0.2459 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2083 - acc: 0.9488 - val_loss: 0.2452 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2079 - acc: 0.9488 - val_loss: 0.2424 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 6s 214us/step - loss: 0.2046 - acc: 0.9488 - val_loss: 0.2418 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2053 - acc: 0.9488 - val_loss: 0.2432 - val_acc: 0.9449\n",
      "Epoch 00010: early stopping\n",
      "-0.7459571217000438 0.032705303461070555\n",
      "\n",
      " \t ::: 16 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.001, 'dense_units': 82, 'dense_layers': 5}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_173 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_173 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_734 (Dense)            (None, 82)                972438    \n",
      "_________________________________________________________________\n",
      "dropout_562 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_735 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_563 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_736 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_564 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_737 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_565 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_738 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_566 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_739 (Dense)            (None, 1)                 83        \n",
      "=================================================================\n",
      "Total params: 999,745\n",
      "Trainable params: 999,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 32s 1ms/step - loss: 0.4662 - acc: 0.9444 - val_loss: 0.2722 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 5s 190us/step - loss: 0.2531 - acc: 0.9477 - val_loss: 0.2341 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 6s 222us/step - loss: 0.2277 - acc: 0.9477 - val_loss: 0.2178 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 5s 172us/step - loss: 0.2215 - acc: 0.9477 - val_loss: 0.2168 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 5s 179us/step - loss: 0.2163 - acc: 0.9477 - val_loss: 0.2140 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 5s 197us/step - loss: 0.2157 - acc: 0.9477 - val_loss: 0.2122 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 5s 179us/step - loss: 0.2122 - acc: 0.9477 - val_loss: 0.2130 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 5s 186us/step - loss: 0.2092 - acc: 0.9477 - val_loss: 0.2079 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 6s 231us/step - loss: 0.2070 - acc: 0.9477 - val_loss: 0.2056 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 5s 201us/step - loss: 0.2056 - acc: 0.9477 - val_loss: 0.2072 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 5s 189us/step - loss: 0.2046 - acc: 0.9477 - val_loss: 0.2063 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 6s 212us/step - loss: 0.2021 - acc: 0.9477 - val_loss: 0.2094 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 6s 207us/step - loss: 0.2014 - acc: 0.9477 - val_loss: 0.2051 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 5s 181us/step - loss: 0.2004 - acc: 0.9477 - val_loss: 0.2080 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 5s 187us/step - loss: 0.1994 - acc: 0.9477 - val_loss: 0.2116 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 5s 188us/step - loss: 0.1983 - acc: 0.9477 - val_loss: 0.2117 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 6s 212us/step - loss: 0.1985 - acc: 0.9477 - val_loss: 0.2105 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 6s 227us/step - loss: 0.1976 - acc: 0.9477 - val_loss: 0.2073 - val_acc: 0.9491\n",
      "Epoch 00018: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_174 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_174 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_740 (Dense)            (None, 82)                972438    \n",
      "_________________________________________________________________\n",
      "dropout_567 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_741 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_568 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_742 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_569 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_743 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_570 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_744 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_571 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_745 (Dense)            (None, 1)                 83        \n",
      "=================================================================\n",
      "Total params: 999,745\n",
      "Trainable params: 999,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 30s 1ms/step - loss: 0.4849 - acc: 0.9449 - val_loss: 0.3058 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2678 - acc: 0.9480 - val_loss: 0.2575 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 204us/step - loss: 0.2361 - acc: 0.9481 - val_loss: 0.2275 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 203us/step - loss: 0.2191 - acc: 0.9481 - val_loss: 0.2188 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2122 - acc: 0.9481 - val_loss: 0.2128 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2085 - acc: 0.9481 - val_loss: 0.2183 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2069 - acc: 0.9481 - val_loss: 0.2174 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2050 - acc: 0.9481 - val_loss: 0.2097 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 6s 221us/step - loss: 0.2035 - acc: 0.9481 - val_loss: 0.2086 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2025 - acc: 0.9481 - val_loss: 0.2070 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.1996 - acc: 0.9481 - val_loss: 0.2100 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2031 - acc: 0.9481 - val_loss: 0.2118 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2000 - acc: 0.9481 - val_loss: 0.2071 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 184us/step - loss: 0.2002 - acc: 0.9481 - val_loss: 0.2147 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.1979 - acc: 0.9481 - val_loss: 0.2104 - val_acc: 0.9476\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_175 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_175 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_746 (Dense)            (None, 82)                972438    \n",
      "_________________________________________________________________\n",
      "dropout_572 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_747 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_573 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_748 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_574 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_749 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_575 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_750 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_576 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_751 (Dense)            (None, 1)                 83        \n",
      "=================================================================\n",
      "Total params: 999,745\n",
      "Trainable params: 999,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 31s 1ms/step - loss: 0.4569 - acc: 0.9484 - val_loss: 0.3033 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 7s 250us/step - loss: 0.2554 - acc: 0.9488 - val_loss: 0.2614 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2273 - acc: 0.9488 - val_loss: 0.2420 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2165 - acc: 0.9488 - val_loss: 0.2367 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2101 - acc: 0.9488 - val_loss: 0.2345 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2071 - acc: 0.9488 - val_loss: 0.2337 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 196us/step - loss: 0.2039 - acc: 0.9488 - val_loss: 0.2313 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2057 - acc: 0.9488 - val_loss: 0.2363 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2033 - acc: 0.9488 - val_loss: 0.2320 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2013 - acc: 0.9488 - val_loss: 0.2281 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 6s 207us/step - loss: 0.1998 - acc: 0.9488 - val_loss: 0.2316 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 7s 247us/step - loss: 0.1975 - acc: 0.9488 - val_loss: 0.2275 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 195us/step - loss: 0.1976 - acc: 0.9488 - val_loss: 0.2284 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 6s 208us/step - loss: 0.1986 - acc: 0.9488 - val_loss: 0.2277 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.1967 - acc: 0.9488 - val_loss: 0.2299 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 5s 197us/step - loss: 0.1959 - acc: 0.9488 - val_loss: 0.2286 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.1942 - acc: 0.9488 - val_loss: 0.2296 - val_acc: 0.9449\n",
      "Epoch 00017: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_176 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_176 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_752 (Dense)            (None, 82)                972438    \n",
      "_________________________________________________________________\n",
      "dropout_577 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_753 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_578 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_754 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_579 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_755 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_580 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_756 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_581 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_757 (Dense)            (None, 1)                 83        \n",
      "=================================================================\n",
      "Total params: 999,745\n",
      "Trainable params: 999,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 33s 1ms/step - loss: 0.4482 - acc: 0.9464 - val_loss: 0.2589 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 6s 230us/step - loss: 0.2542 - acc: 0.9466 - val_loss: 0.2290 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2281 - acc: 0.9466 - val_loss: 0.2141 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2176 - acc: 0.9466 - val_loss: 0.2122 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2159 - acc: 0.9466 - val_loss: 0.2153 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2160 - acc: 0.9466 - val_loss: 0.2075 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2117 - acc: 0.9466 - val_loss: 0.2040 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2128 - acc: 0.9466 - val_loss: 0.2070 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2093 - acc: 0.9466 - val_loss: 0.2035 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2084 - acc: 0.9466 - val_loss: 0.2049 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2046 - acc: 0.9466 - val_loss: 0.2043 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 191us/step - loss: 0.2026 - acc: 0.9466 - val_loss: 0.2029 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2016 - acc: 0.9466 - val_loss: 0.2033 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2026 - acc: 0.9466 - val_loss: 0.2022 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2013 - acc: 0.9466 - val_loss: 0.2021 - val_acc: 0.9534\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.1997 - acc: 0.9466 - val_loss: 0.2009 - val_acc: 0.9534\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.1980 - acc: 0.9466 - val_loss: 0.2038 - val_acc: 0.9534\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 193us/step - loss: 0.1992 - acc: 0.9466 - val_loss: 0.2012 - val_acc: 0.9534\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2004 - acc: 0.9466 - val_loss: 0.2023 - val_acc: 0.9534\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.1955 - acc: 0.9466 - val_loss: 0.2034 - val_acc: 0.9534\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.1964 - acc: 0.9466 - val_loss: 0.1999 - val_acc: 0.9534\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.1937 - acc: 0.9466 - val_loss: 0.2017 - val_acc: 0.9534\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.1946 - acc: 0.9466 - val_loss: 0.2044 - val_acc: 0.9534\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.1948 - acc: 0.9466 - val_loss: 0.2041 - val_acc: 0.9534\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.1938 - acc: 0.9466 - val_loss: 0.2051 - val_acc: 0.9534\n",
      "Epoch 26/200\n",
      "26869/26869 [==============================] - 5s 189us/step - loss: 0.1949 - acc: 0.9466 - val_loss: 0.2038 - val_acc: 0.9534\n",
      "Epoch 00026: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_177 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_177 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_758 (Dense)            (None, 82)                972438    \n",
      "_________________________________________________________________\n",
      "dropout_582 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_759 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_583 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_760 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_584 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_761 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_585 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_762 (Dense)            (None, 82)                6806      \n",
      "_________________________________________________________________\n",
      "dropout_586 (Dropout)        (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "dense_763 (Dense)            (None, 1)                 83        \n",
      "=================================================================\n",
      "Total params: 999,745\n",
      "Trainable params: 999,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 28s 1ms/step - loss: 0.4931 - acc: 0.9463 - val_loss: 0.3307 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2779 - acc: 0.9487 - val_loss: 0.2729 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2376 - acc: 0.9488 - val_loss: 0.2581 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2237 - acc: 0.9488 - val_loss: 0.2556 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2185 - acc: 0.9488 - val_loss: 0.2415 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2111 - acc: 0.9488 - val_loss: 0.2380 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2080 - acc: 0.9488 - val_loss: 0.2366 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2060 - acc: 0.9488 - val_loss: 0.2429 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2066 - acc: 0.9488 - val_loss: 0.2372 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2052 - acc: 0.9488 - val_loss: 0.2329 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2383 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2018 - acc: 0.9488 - val_loss: 0.2391 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2016 - acc: 0.9488 - val_loss: 0.2379 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.2015 - acc: 0.9488 - val_loss: 0.2365 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2015 - acc: 0.9488 - val_loss: 0.2333 - val_acc: 0.9449\n",
      "Epoch 00015: early stopping\n",
      "-0.7574439437155631 0.02489163681171442\n",
      "\n",
      " \t ::: 17 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.001, 'dense_units': 10, 'dense_layers': 6}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_178 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_178 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_764 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_587 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_765 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_588 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_766 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_589 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_767 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_590 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_768 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_591 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_769 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_592 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_770 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 119,151\n",
      "Trainable params: 119,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 31s 1ms/step - loss: 0.4357 - acc: 0.9378 - val_loss: 0.2550 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 5s 172us/step - loss: 0.2525 - acc: 0.9476 - val_loss: 0.2333 - val_acc: 0.9489\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 5s 175us/step - loss: 0.2357 - acc: 0.9476 - val_loss: 0.2286 - val_acc: 0.9489\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 5s 187us/step - loss: 0.2270 - acc: 0.9476 - val_loss: 0.2226 - val_acc: 0.9489\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 5s 176us/step - loss: 0.2191 - acc: 0.9476 - val_loss: 0.2178 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 5s 185us/step - loss: 0.2160 - acc: 0.9476 - val_loss: 0.2155 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 5s 182us/step - loss: 0.2117 - acc: 0.9477 - val_loss: 0.2193 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 5s 177us/step - loss: 0.2115 - acc: 0.9477 - val_loss: 0.2140 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 5s 168us/step - loss: 0.2070 - acc: 0.9477 - val_loss: 0.2144 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 5s 172us/step - loss: 0.2055 - acc: 0.9477 - val_loss: 0.2118 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 5s 196us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2151 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 5s 186us/step - loss: 0.2051 - acc: 0.9477 - val_loss: 0.2148 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 5s 200us/step - loss: 0.2046 - acc: 0.9477 - val_loss: 0.2134 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 5s 188us/step - loss: 0.2025 - acc: 0.9477 - val_loss: 0.2123 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 5s 184us/step - loss: 0.2013 - acc: 0.9477 - val_loss: 0.2113 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 5s 184us/step - loss: 0.2009 - acc: 0.9477 - val_loss: 0.2124 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 5s 189us/step - loss: 0.1991 - acc: 0.9477 - val_loss: 0.2156 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 6s 209us/step - loss: 0.1992 - acc: 0.9477 - val_loss: 0.2154 - val_acc: 0.9491\n",
      "Epoch 19/200\n",
      "26868/26868 [==============================] - 5s 173us/step - loss: 0.1983 - acc: 0.9477 - val_loss: 0.2180 - val_acc: 0.9491\n",
      "Epoch 20/200\n",
      "26868/26868 [==============================] - 5s 191us/step - loss: 0.1978 - acc: 0.9477 - val_loss: 0.2166 - val_acc: 0.9491\n",
      "Epoch 00020: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_179 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_179 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_771 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_593 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_772 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_594 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_773 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_595 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_774 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_596 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_775 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_597 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_776 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_598 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_777 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 119,151\n",
      "Trainable params: 119,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 34s 1ms/step - loss: 0.4201 - acc: 0.9402 - val_loss: 0.2577 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 186us/step - loss: 0.2418 - acc: 0.9481 - val_loss: 0.2330 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 188us/step - loss: 0.2257 - acc: 0.9481 - val_loss: 0.2240 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2187 - acc: 0.9481 - val_loss: 0.2237 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2144 - acc: 0.9481 - val_loss: 0.2209 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 188us/step - loss: 0.2104 - acc: 0.9481 - val_loss: 0.2151 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2082 - acc: 0.9481 - val_loss: 0.2168 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 184us/step - loss: 0.2056 - acc: 0.9481 - val_loss: 0.2151 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 188us/step - loss: 0.2037 - acc: 0.9481 - val_loss: 0.2143 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 196us/step - loss: 0.2023 - acc: 0.9481 - val_loss: 0.2152 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 191us/step - loss: 0.2013 - acc: 0.9481 - val_loss: 0.2120 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 186us/step - loss: 0.2014 - acc: 0.9481 - val_loss: 0.2122 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 6s 216us/step - loss: 0.2001 - acc: 0.9481 - val_loss: 0.2110 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 181us/step - loss: 0.2000 - acc: 0.9481 - val_loss: 0.2106 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 198us/step - loss: 0.1990 - acc: 0.9481 - val_loss: 0.2107 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 5s 180us/step - loss: 0.1995 - acc: 0.9481 - val_loss: 0.2090 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 7s 258us/step - loss: 0.1981 - acc: 0.9481 - val_loss: 0.2147 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.1988 - acc: 0.9481 - val_loss: 0.2104 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 5s 197us/step - loss: 0.1972 - acc: 0.9481 - val_loss: 0.2114 - val_acc: 0.9476\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 5s 192us/step - loss: 0.1954 - acc: 0.9481 - val_loss: 0.2076 - val_acc: 0.9476\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.1958 - acc: 0.9481 - val_loss: 0.2072 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 5s 192us/step - loss: 0.1943 - acc: 0.9481 - val_loss: 0.2142 - val_acc: 0.9476\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.1958 - acc: 0.9481 - val_loss: 0.2181 - val_acc: 0.9476\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 5s 187us/step - loss: 0.2002 - acc: 0.9481 - val_loss: 0.2084 - val_acc: 0.9476\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.1938 - acc: 0.9481 - val_loss: 0.2098 - val_acc: 0.9476\n",
      "Epoch 26/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.1962 - acc: 0.9481 - val_loss: 0.2110 - val_acc: 0.9476\n",
      "Epoch 00026: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_180 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_180 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_778 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_599 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_779 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_600 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_780 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_601 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_781 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_602 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_782 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_603 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_783 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_604 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_784 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 119,151\n",
      "Trainable params: 119,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 29s 1ms/step - loss: 0.4125 - acc: 0.9457 - val_loss: 0.2790 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2522 - acc: 0.9488 - val_loss: 0.2606 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2350 - acc: 0.9488 - val_loss: 0.2507 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2282 - acc: 0.9488 - val_loss: 0.2497 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2201 - acc: 0.9488 - val_loss: 0.2449 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.2185 - acc: 0.9488 - val_loss: 0.2419 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2140 - acc: 0.9488 - val_loss: 0.2427 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2108 - acc: 0.9488 - val_loss: 0.2404 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2111 - acc: 0.9488 - val_loss: 0.2359 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 197us/step - loss: 0.2084 - acc: 0.9488 - val_loss: 0.2386 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2067 - acc: 0.9488 - val_loss: 0.2394 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2061 - acc: 0.9488 - val_loss: 0.2370 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2038 - acc: 0.9488 - val_loss: 0.2376 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2042 - acc: 0.9488 - val_loss: 0.2354 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2035 - acc: 0.9488 - val_loss: 0.2366 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2013 - acc: 0.9488 - val_loss: 0.2362 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2038 - acc: 0.9488 - val_loss: 0.2415 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2021 - acc: 0.9488 - val_loss: 0.2388 - val_acc: 0.9449\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2028 - acc: 0.9488 - val_loss: 0.2389 - val_acc: 0.9449\n",
      "Epoch 00019: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_181 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_181 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_785 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_605 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_786 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_606 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_787 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_607 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_788 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_608 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_789 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_609 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_790 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_610 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_791 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 119,151\n",
      "Trainable params: 119,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26869/26869 [==============================] - 30s 1ms/step - loss: 0.4183 - acc: 0.9390 - val_loss: 0.2417 - val_acc: 0.9533\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2499 - acc: 0.9466 - val_loss: 0.2211 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2307 - acc: 0.9466 - val_loss: 0.2116 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 188us/step - loss: 0.2214 - acc: 0.9466 - val_loss: 0.2090 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 195us/step - loss: 0.2174 - acc: 0.9466 - val_loss: 0.2056 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2140 - acc: 0.9466 - val_loss: 0.2054 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2126 - acc: 0.9466 - val_loss: 0.2045 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.2021 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2086 - acc: 0.9466 - val_loss: 0.2047 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2060 - acc: 0.9466 - val_loss: 0.2044 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2064 - acc: 0.9466 - val_loss: 0.2016 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2053 - acc: 0.9466 - val_loss: 0.2029 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2039 - acc: 0.9466 - val_loss: 0.1986 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2050 - acc: 0.9466 - val_loss: 0.2034 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2049 - acc: 0.9466 - val_loss: 0.1997 - val_acc: 0.9534\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2021 - acc: 0.9466 - val_loss: 0.2004 - val_acc: 0.9534\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2006 - acc: 0.9466 - val_loss: 0.2009 - val_acc: 0.9534\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2009 - acc: 0.9466 - val_loss: 0.2009 - val_acc: 0.9534\n",
      "Epoch 00018: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_182 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_182 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_792 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_611 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_793 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_612 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_794 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_613 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_795 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_614 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_796 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_615 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_797 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_616 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_798 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 119,151\n",
      "Trainable params: 119,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 31s 1ms/step - loss: 0.3799 - acc: 0.9484 - val_loss: 0.2799 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2453 - acc: 0.9487 - val_loss: 0.2548 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2277 - acc: 0.9488 - val_loss: 0.2462 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2177 - acc: 0.9488 - val_loss: 0.2439 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2134 - acc: 0.9488 - val_loss: 0.2424 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2128 - acc: 0.9488 - val_loss: 0.2438 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2065 - acc: 0.9488 - val_loss: 0.2434 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2066 - acc: 0.9488 - val_loss: 0.2420 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2052 - acc: 0.9488 - val_loss: 0.2370 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2031 - acc: 0.9488 - val_loss: 0.2376 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2016 - acc: 0.9488 - val_loss: 0.2395 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.1989 - acc: 0.9488 - val_loss: 0.2414 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.1993 - acc: 0.9488 - val_loss: 0.2403 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.1993 - acc: 0.9488 - val_loss: 0.2408 - val_acc: 0.9449\n",
      "Epoch 00014: early stopping\n",
      "-0.7416331194490738 0.026159762951437977\n",
      "\n",
      " \t ::: 18 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.001331919684888523, 'dense_units': 10, 'dense_layers': 5}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_183 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_183 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_799 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_617 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_800 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_618 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_801 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_619 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_802 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_620 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_803 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_621 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_804 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 119,041\n",
      "Trainable params: 119,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 28s 1ms/step - loss: 0.4240 - acc: 0.9407 - val_loss: 0.2319 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 5s 171us/step - loss: 0.2361 - acc: 0.9476 - val_loss: 0.2222 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 5s 181us/step - loss: 0.2218 - acc: 0.9477 - val_loss: 0.2189 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 5s 172us/step - loss: 0.2168 - acc: 0.9477 - val_loss: 0.2144 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 5s 168us/step - loss: 0.2152 - acc: 0.9477 - val_loss: 0.2123 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 4s 162us/step - loss: 0.2116 - acc: 0.9477 - val_loss: 0.2114 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 157us/step - loss: 0.2080 - acc: 0.9477 - val_loss: 0.2112 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 157us/step - loss: 0.2083 - acc: 0.9477 - val_loss: 0.2103 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 4s 160us/step - loss: 0.2055 - acc: 0.9477 - val_loss: 0.2088 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 4s 161us/step - loss: 0.2072 - acc: 0.9477 - val_loss: 0.2088 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 4s 153us/step - loss: 0.2044 - acc: 0.9477 - val_loss: 0.2089 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 151us/step - loss: 0.2042 - acc: 0.9477 - val_loss: 0.2088 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 163us/step - loss: 0.2029 - acc: 0.9477 - val_loss: 0.2058 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 4s 160us/step - loss: 0.2015 - acc: 0.9477 - val_loss: 0.2084 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 4s 161us/step - loss: 0.2058 - acc: 0.9477 - val_loss: 0.2096 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 4s 161us/step - loss: 0.2019 - acc: 0.9477 - val_loss: 0.2075 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 5s 176us/step - loss: 0.2009 - acc: 0.9477 - val_loss: 0.2050 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 4s 165us/step - loss: 0.2007 - acc: 0.9477 - val_loss: 0.2072 - val_acc: 0.9491\n",
      "Epoch 19/200\n",
      "26868/26868 [==============================] - 4s 161us/step - loss: 0.2008 - acc: 0.9477 - val_loss: 0.2084 - val_acc: 0.9491\n",
      "Epoch 20/200\n",
      "26868/26868 [==============================] - 4s 158us/step - loss: 0.2013 - acc: 0.9477 - val_loss: 0.2098 - val_acc: 0.9491\n",
      "Epoch 21/200\n",
      "26868/26868 [==============================] - 4s 164us/step - loss: 0.1995 - acc: 0.9477 - val_loss: 0.2098 - val_acc: 0.9491\n",
      "Epoch 22/200\n",
      "26868/26868 [==============================] - 4s 163us/step - loss: 0.1990 - acc: 0.9477 - val_loss: 0.2076 - val_acc: 0.9491\n",
      "Epoch 00022: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_184 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_184 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_805 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_622 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_806 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_623 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_807 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_624 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_808 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_625 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_809 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_626 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_810 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 119,041\n",
      "Trainable params: 119,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 31s 1ms/step - loss: 0.3381 - acc: 0.9470 - val_loss: 0.2430 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2323 - acc: 0.9481 - val_loss: 0.2225 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2216 - acc: 0.9481 - val_loss: 0.2243 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2148 - acc: 0.9481 - val_loss: 0.2227 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 153us/step - loss: 0.2132 - acc: 0.9481 - val_loss: 0.2196 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2110 - acc: 0.9481 - val_loss: 0.2152 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2090 - acc: 0.9481 - val_loss: 0.2200 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2100 - acc: 0.9481 - val_loss: 0.2200 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 155us/step - loss: 0.2078 - acc: 0.9481 - val_loss: 0.2211 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2071 - acc: 0.9481 - val_loss: 0.2173 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2073 - acc: 0.9481 - val_loss: 0.2199 - val_acc: 0.9476\n",
      "Epoch 00011: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_185 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_185 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_811 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_627 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_812 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_628 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_813 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_629 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_814 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_630 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_815 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_631 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_816 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 119,041\n",
      "Trainable params: 119,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 33s 1ms/step - loss: 0.4595 - acc: 0.9335 - val_loss: 0.2649 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2398 - acc: 0.9487 - val_loss: 0.2427 - val_acc: 0.9448\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2242 - acc: 0.9487 - val_loss: 0.2340 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2175 - acc: 0.9488 - val_loss: 0.2321 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2108 - acc: 0.9488 - val_loss: 0.2306 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2067 - acc: 0.9488 - val_loss: 0.2245 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2045 - acc: 0.9488 - val_loss: 0.2273 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2032 - acc: 0.9488 - val_loss: 0.2318 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2017 - acc: 0.9488 - val_loss: 0.2263 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2003 - acc: 0.9488 - val_loss: 0.2263 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.1997 - acc: 0.9488 - val_loss: 0.2265 - val_acc: 0.9449\n",
      "Epoch 00011: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_186 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_186 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_817 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_632 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_818 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_633 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_819 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_634 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_820 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_635 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_821 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_636 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_822 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 119,041\n",
      "Trainable params: 119,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 30s 1ms/step - loss: 0.3506 - acc: 0.9441 - val_loss: 0.2232 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2344 - acc: 0.9466 - val_loss: 0.2096 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.2230 - acc: 0.9466 - val_loss: 0.2102 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 152us/step - loss: 0.2202 - acc: 0.9466 - val_loss: 0.2092 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2170 - acc: 0.9466 - val_loss: 0.2040 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2132 - acc: 0.9466 - val_loss: 0.2041 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 166us/step - loss: 0.2103 - acc: 0.9466 - val_loss: 0.2061 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 191us/step - loss: 0.2123 - acc: 0.9466 - val_loss: 0.2038 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2117 - acc: 0.9466 - val_loss: 0.2083 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2116 - acc: 0.9466 - val_loss: 0.2049 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2093 - acc: 0.9466 - val_loss: 0.2064 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2103 - acc: 0.9466 - val_loss: 0.2027 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2059 - acc: 0.9466 - val_loss: 0.2009 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2076 - acc: 0.9466 - val_loss: 0.2020 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 158us/step - loss: 0.2058 - acc: 0.9466 - val_loss: 0.1985 - val_acc: 0.9534\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 156us/step - loss: 0.2054 - acc: 0.9466 - val_loss: 0.2021 - val_acc: 0.9534\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 5s 181us/step - loss: 0.2053 - acc: 0.9466 - val_loss: 0.2122 - val_acc: 0.9534\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2041 - acc: 0.9466 - val_loss: 0.2031 - val_acc: 0.9534\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2036 - acc: 0.9466 - val_loss: 0.2041 - val_acc: 0.9534\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 6s 205us/step - loss: 0.2034 - acc: 0.9466 - val_loss: 0.2060 - val_acc: 0.9534\n",
      "Epoch 00020: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_187 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_187 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_823 (Dense)            (None, 10)                118590    \n",
      "_________________________________________________________________\n",
      "dropout_637 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_824 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_638 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_825 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_639 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_826 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_640 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_827 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_641 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_828 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 119,041\n",
      "Trainable params: 119,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 36s 1ms/step - loss: 0.3685 - acc: 0.9445 - val_loss: 0.2589 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 6s 224us/step - loss: 0.2364 - acc: 0.9487 - val_loss: 0.2472 - val_acc: 0.9446\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 198us/step - loss: 0.2228 - acc: 0.9485 - val_loss: 0.2426 - val_acc: 0.9443\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 6s 241us/step - loss: 0.2154 - acc: 0.9487 - val_loss: 0.2377 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2116 - acc: 0.9487 - val_loss: 0.2392 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2086 - acc: 0.9488 - val_loss: 0.2371 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 180us/step - loss: 0.2048 - acc: 0.9487 - val_loss: 0.2369 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 161us/step - loss: 0.2040 - acc: 0.9488 - val_loss: 0.2421 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2053 - acc: 0.9488 - val_loss: 0.2463 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 188us/step - loss: 0.2047 - acc: 0.9488 - val_loss: 0.2418 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2022 - acc: 0.9488 - val_loss: 0.2427 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2021 - acc: 0.9488 - val_loss: 0.2449 - val_acc: 0.9449\n",
      "Epoch 00012: early stopping\n",
      "-0.7294169069012426 0.024650525514116745\n",
      "\n",
      " \t ::: 19 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.001, 'dense_units': 20, 'dense_layers': 5}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_188 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_188 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_829 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_642 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_830 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_643 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_831 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_644 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_832 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_645 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_833 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_646 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_834 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,881\n",
      "Trainable params: 238,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n",
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 35s 1ms/step - loss: 0.4012 - acc: 0.9471 - val_loss: 0.2516 - val_acc: 0.9491\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 4s 166us/step - loss: 0.2484 - acc: 0.9477 - val_loss: 0.2348 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 4s 163us/step - loss: 0.2295 - acc: 0.9477 - val_loss: 0.2196 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 4s 156us/step - loss: 0.2215 - acc: 0.9477 - val_loss: 0.2180 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 4s 162us/step - loss: 0.2182 - acc: 0.9477 - val_loss: 0.2176 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 4s 159us/step - loss: 0.2154 - acc: 0.9477 - val_loss: 0.2142 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 4s 160us/step - loss: 0.2120 - acc: 0.9477 - val_loss: 0.2130 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 4s 159us/step - loss: 0.2088 - acc: 0.9477 - val_loss: 0.2131 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 4s 159us/step - loss: 0.2062 - acc: 0.9477 - val_loss: 0.2124 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 4s 157us/step - loss: 0.2063 - acc: 0.9477 - val_loss: 0.2081 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 4s 157us/step - loss: 0.2054 - acc: 0.9477 - val_loss: 0.2107 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 4s 163us/step - loss: 0.2037 - acc: 0.9477 - val_loss: 0.2125 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 4s 159us/step - loss: 0.2022 - acc: 0.9477 - val_loss: 0.2088 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 4s 165us/step - loss: 0.2003 - acc: 0.9477 - val_loss: 0.2095 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 4s 166us/step - loss: 0.1994 - acc: 0.9477 - val_loss: 0.2116 - val_acc: 0.9491\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_189 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_189 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_835 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_647 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_836 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_648 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_837 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_649 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_838 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_650 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_839 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_651 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_840 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,881\n",
      "Trainable params: 238,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 38s 1ms/step - loss: 0.4136 - acc: 0.9456 - val_loss: 0.2705 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2567 - acc: 0.9480 - val_loss: 0.2472 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 196us/step - loss: 0.2368 - acc: 0.9481 - val_loss: 0.2350 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 191us/step - loss: 0.2280 - acc: 0.9480 - val_loss: 0.2256 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2194 - acc: 0.9481 - val_loss: 0.2259 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 196us/step - loss: 0.2140 - acc: 0.9481 - val_loss: 0.2181 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 186us/step - loss: 0.2117 - acc: 0.9481 - val_loss: 0.2207 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2088 - acc: 0.9481 - val_loss: 0.2196 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 6s 236us/step - loss: 0.2071 - acc: 0.9481 - val_loss: 0.2178 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 6s 205us/step - loss: 0.2055 - acc: 0.9481 - val_loss: 0.2163 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 194us/step - loss: 0.2064 - acc: 0.9481 - val_loss: 0.2213 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 199us/step - loss: 0.2062 - acc: 0.9481 - val_loss: 0.2122 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2036 - acc: 0.9481 - val_loss: 0.2161 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 192us/step - loss: 0.2026 - acc: 0.9481 - val_loss: 0.2119 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2039 - acc: 0.9481 - val_loss: 0.2133 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2034 - acc: 0.9481 - val_loss: 0.2147 - val_acc: 0.9476\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 5s 200us/step - loss: 0.2037 - acc: 0.9481 - val_loss: 0.2131 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2026 - acc: 0.9481 - val_loss: 0.2119 - val_acc: 0.9476\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.1986 - acc: 0.9481 - val_loss: 0.2135 - val_acc: 0.9476\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 5s 185us/step - loss: 0.1985 - acc: 0.9481 - val_loss: 0.2100 - val_acc: 0.9476\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.1998 - acc: 0.9481 - val_loss: 0.2129 - val_acc: 0.9476\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.1966 - acc: 0.9481 - val_loss: 0.2133 - val_acc: 0.9476\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.1951 - acc: 0.9481 - val_loss: 0.2105 - val_acc: 0.9476\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 5s 183us/step - loss: 0.1960 - acc: 0.9481 - val_loss: 0.2116 - val_acc: 0.9476\n",
      "Epoch 25/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.1969 - acc: 0.9481 - val_loss: 0.2114 - val_acc: 0.9476\n",
      "Epoch 00025: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_190 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_190 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_841 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_652 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_842 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_653 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_843 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_654 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_844 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_655 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_845 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_656 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_846 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,881\n",
      "Trainable params: 238,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 34s 1ms/step - loss: 0.3625 - acc: 0.9480 - val_loss: 0.2814 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2460 - acc: 0.9488 - val_loss: 0.2543 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 191us/step - loss: 0.2262 - acc: 0.9488 - val_loss: 0.2466 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2203 - acc: 0.9488 - val_loss: 0.2470 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2146 - acc: 0.9488 - val_loss: 0.2508 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 185us/step - loss: 0.2110 - acc: 0.9488 - val_loss: 0.2441 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 196us/step - loss: 0.2093 - acc: 0.9488 - val_loss: 0.2410 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 187us/step - loss: 0.2074 - acc: 0.9488 - val_loss: 0.2381 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2053 - acc: 0.9488 - val_loss: 0.2405 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 189us/step - loss: 0.2061 - acc: 0.9488 - val_loss: 0.2378 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 6s 220us/step - loss: 0.2024 - acc: 0.9488 - val_loss: 0.2429 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 180us/step - loss: 0.2032 - acc: 0.9488 - val_loss: 0.2365 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 192us/step - loss: 0.2021 - acc: 0.9488 - val_loss: 0.2376 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 188us/step - loss: 0.2045 - acc: 0.9488 - val_loss: 0.2330 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2034 - acc: 0.9488 - val_loss: 0.2456 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 5s 192us/step - loss: 0.2028 - acc: 0.9488 - val_loss: 0.2418 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 5s 180us/step - loss: 0.2001 - acc: 0.9488 - val_loss: 0.2426 - val_acc: 0.9449\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.1966 - acc: 0.9488 - val_loss: 0.2419 - val_acc: 0.9449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 5s 183us/step - loss: 0.1989 - acc: 0.9488 - val_loss: 0.2446 - val_acc: 0.9449\n",
      "Epoch 00019: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_191 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_191 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_847 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_657 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_848 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_658 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_849 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_659 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_850 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_660 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_851 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_661 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_852 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,881\n",
      "Trainable params: 238,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 37s 1ms/step - loss: 0.3753 - acc: 0.9466 - val_loss: 0.2475 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 180us/step - loss: 0.2524 - acc: 0.9466 - val_loss: 0.2263 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 196us/step - loss: 0.2318 - acc: 0.9466 - val_loss: 0.2167 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2219 - acc: 0.9466 - val_loss: 0.2104 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2184 - acc: 0.9466 - val_loss: 0.2080 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 184us/step - loss: 0.2154 - acc: 0.9466 - val_loss: 0.2132 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 188us/step - loss: 0.2130 - acc: 0.9466 - val_loss: 0.2093 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 180us/step - loss: 0.2116 - acc: 0.9466 - val_loss: 0.2098 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2095 - acc: 0.9466 - val_loss: 0.2094 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2083 - acc: 0.9466 - val_loss: 0.2102 - val_acc: 0.9534\n",
      "Epoch 00010: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_192 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_192 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_853 (Dense)            (None, 20)                237180    \n",
      "_________________________________________________________________\n",
      "dropout_662 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_854 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_663 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_855 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_664 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_856 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_665 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_857 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_666 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_858 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 238,881\n",
      "Trainable params: 238,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 33s 1ms/step - loss: 0.4387 - acc: 0.9382 - val_loss: 0.3038 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2711 - acc: 0.9486 - val_loss: 0.2787 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2480 - acc: 0.9486 - val_loss: 0.2694 - val_acc: 0.9427\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2362 - acc: 0.9485 - val_loss: 0.2587 - val_acc: 0.9443\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2253 - acc: 0.9486 - val_loss: 0.2558 - val_acc: 0.9446\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2169 - acc: 0.9488 - val_loss: 0.2548 - val_acc: 0.9442\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2152 - acc: 0.9488 - val_loss: 0.2453 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2104 - acc: 0.9488 - val_loss: 0.2411 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2076 - acc: 0.9488 - val_loss: 0.2444 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2053 - acc: 0.9488 - val_loss: 0.2435 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2043 - acc: 0.9488 - val_loss: 0.2441 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2003 - acc: 0.9488 - val_loss: 0.2438 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 185us/step - loss: 0.2043 - acc: 0.9488 - val_loss: 0.2439 - val_acc: 0.9449\n",
      "Epoch 00013: early stopping\n",
      "-0.7394184098511483 0.03148689505040682\n",
      "\n",
      " \t ::: 20 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.0014105186310148523, 'dense_units': 93, 'dense_layers': 4}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_193 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_193 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_859 (Dense)            (None, 93)                1102887   \n",
      "_________________________________________________________________\n",
      "dropout_667 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_860 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_668 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_861 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_669 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_862 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_670 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_863 (Dense)            (None, 1)                 94        \n",
      "=================================================================\n",
      "Total params: 1,129,207\n",
      "Trainable params: 1,129,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26868, 77, 154)\n",
      "(6718, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26868 samples, validate on 6718 samples\n",
      "Epoch 1/200\n",
      "26868/26868 [==============================] - 30s 1ms/step - loss: 0.3828 - acc: 0.9477 - val_loss: 0.2446 - val_acc: 0.9482\n",
      "Epoch 2/200\n",
      "26868/26868 [==============================] - 5s 178us/step - loss: 0.2391 - acc: 0.9476 - val_loss: 0.2264 - val_acc: 0.9491\n",
      "Epoch 3/200\n",
      "26868/26868 [==============================] - 5s 174us/step - loss: 0.2274 - acc: 0.9477 - val_loss: 0.2163 - val_acc: 0.9491\n",
      "Epoch 4/200\n",
      "26868/26868 [==============================] - 5s 177us/step - loss: 0.2209 - acc: 0.9477 - val_loss: 0.2154 - val_acc: 0.9491\n",
      "Epoch 5/200\n",
      "26868/26868 [==============================] - 5s 176us/step - loss: 0.2161 - acc: 0.9477 - val_loss: 0.2124 - val_acc: 0.9491\n",
      "Epoch 6/200\n",
      "26868/26868 [==============================] - 5s 173us/step - loss: 0.2134 - acc: 0.9477 - val_loss: 0.2163 - val_acc: 0.9491\n",
      "Epoch 7/200\n",
      "26868/26868 [==============================] - 5s 177us/step - loss: 0.2119 - acc: 0.9477 - val_loss: 0.2166 - val_acc: 0.9491\n",
      "Epoch 8/200\n",
      "26868/26868 [==============================] - 5s 170us/step - loss: 0.2114 - acc: 0.9477 - val_loss: 0.2092 - val_acc: 0.9491\n",
      "Epoch 9/200\n",
      "26868/26868 [==============================] - 5s 175us/step - loss: 0.2096 - acc: 0.9477 - val_loss: 0.2094 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "26868/26868 [==============================] - 5s 171us/step - loss: 0.2118 - acc: 0.9477 - val_loss: 0.2090 - val_acc: 0.9491\n",
      "Epoch 11/200\n",
      "26868/26868 [==============================] - 5s 176us/step - loss: 0.2063 - acc: 0.9477 - val_loss: 0.2104 - val_acc: 0.9491\n",
      "Epoch 12/200\n",
      "26868/26868 [==============================] - 5s 170us/step - loss: 0.2076 - acc: 0.9477 - val_loss: 0.2082 - val_acc: 0.9491\n",
      "Epoch 13/200\n",
      "26868/26868 [==============================] - 5s 179us/step - loss: 0.2046 - acc: 0.9477 - val_loss: 0.2084 - val_acc: 0.9491\n",
      "Epoch 14/200\n",
      "26868/26868 [==============================] - 5s 191us/step - loss: 0.2033 - acc: 0.9477 - val_loss: 0.2094 - val_acc: 0.9491\n",
      "Epoch 15/200\n",
      "26868/26868 [==============================] - 5s 180us/step - loss: 0.2043 - acc: 0.9477 - val_loss: 0.2100 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "26868/26868 [==============================] - 5s 182us/step - loss: 0.2033 - acc: 0.9477 - val_loss: 0.2064 - val_acc: 0.9491\n",
      "Epoch 17/200\n",
      "26868/26868 [==============================] - 5s 183us/step - loss: 0.2027 - acc: 0.9477 - val_loss: 0.2087 - val_acc: 0.9491\n",
      "Epoch 18/200\n",
      "26868/26868 [==============================] - 5s 176us/step - loss: 0.2009 - acc: 0.9477 - val_loss: 0.2070 - val_acc: 0.9491\n",
      "Epoch 19/200\n",
      "26868/26868 [==============================] - 5s 177us/step - loss: 0.2012 - acc: 0.9477 - val_loss: 0.2083 - val_acc: 0.9491\n",
      "Epoch 20/200\n",
      "26868/26868 [==============================] - 5s 192us/step - loss: 0.2000 - acc: 0.9477 - val_loss: 0.2092 - val_acc: 0.9491\n",
      "Epoch 21/200\n",
      "26868/26868 [==============================] - 5s 188us/step - loss: 0.2011 - acc: 0.9477 - val_loss: 0.2078 - val_acc: 0.9491\n",
      "Epoch 00021: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_194 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_194 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_864 (Dense)            (None, 93)                1102887   \n",
      "_________________________________________________________________\n",
      "dropout_671 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_865 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_672 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_866 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_673 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_867 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_674 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_868 (Dense)            (None, 1)                 94        \n",
      "=================================================================\n",
      "Total params: 1,129,207\n",
      "Trainable params: 1,129,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 35s 1ms/step - loss: 0.3941 - acc: 0.9471 - val_loss: 0.2655 - val_acc: 0.9476\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2406 - acc: 0.9481 - val_loss: 0.2316 - val_acc: 0.9476\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2239 - acc: 0.9481 - val_loss: 0.2239 - val_acc: 0.9476\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2167 - acc: 0.9481 - val_loss: 0.2211 - val_acc: 0.9476\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 157us/step - loss: 0.2128 - acc: 0.9481 - val_loss: 0.2174 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2120 - acc: 0.9481 - val_loss: 0.2180 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2114 - acc: 0.9481 - val_loss: 0.2137 - val_acc: 0.9476\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2090 - acc: 0.9481 - val_loss: 0.2109 - val_acc: 0.9476\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2069 - acc: 0.9481 - val_loss: 0.2091 - val_acc: 0.9476\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 4s 151us/step - loss: 0.2085 - acc: 0.9481 - val_loss: 0.2082 - val_acc: 0.9476\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2072 - acc: 0.9481 - val_loss: 0.2138 - val_acc: 0.9476\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2074 - acc: 0.9481 - val_loss: 0.2138 - val_acc: 0.9476\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2059 - acc: 0.9481 - val_loss: 0.2090 - val_acc: 0.9476\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2060 - acc: 0.9481 - val_loss: 0.2109 - val_acc: 0.9476\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2060 - acc: 0.9481 - val_loss: 0.2094 - val_acc: 0.9476\n",
      "Epoch 00015: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_195 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_195 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_869 (Dense)            (None, 93)                1102887   \n",
      "_________________________________________________________________\n",
      "dropout_675 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_870 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_676 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_871 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_677 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_872 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_678 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_873 (Dense)            (None, 1)                 94        \n",
      "=================================================================\n",
      "Total params: 1,129,207\n",
      "Trainable params: 1,129,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 31s 1ms/step - loss: 0.3956 - acc: 0.9478 - val_loss: 0.2688 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2392 - acc: 0.9488 - val_loss: 0.2425 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2233 - acc: 0.9488 - val_loss: 0.2440 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 4s 154us/step - loss: 0.2176 - acc: 0.9488 - val_loss: 0.2386 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2119 - acc: 0.9488 - val_loss: 0.2364 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 4s 167us/step - loss: 0.2104 - acc: 0.9488 - val_loss: 0.2271 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 4s 162us/step - loss: 0.2110 - acc: 0.9488 - val_loss: 0.2284 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2074 - acc: 0.9488 - val_loss: 0.2319 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.2079 - acc: 0.9488 - val_loss: 0.2298 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2073 - acc: 0.9488 - val_loss: 0.2324 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2060 - acc: 0.9488 - val_loss: 0.2253 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 4s 165us/step - loss: 0.2023 - acc: 0.9488 - val_loss: 0.2213 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 4s 159us/step - loss: 0.2022 - acc: 0.9488 - val_loss: 0.2247 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 4s 163us/step - loss: 0.1993 - acc: 0.9488 - val_loss: 0.2222 - val_acc: 0.9449\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.1970 - acc: 0.9488 - val_loss: 0.2271 - val_acc: 0.9449\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 4s 164us/step - loss: 0.2000 - acc: 0.9488 - val_loss: 0.2234 - val_acc: 0.9449\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 4s 160us/step - loss: 0.1971 - acc: 0.9488 - val_loss: 0.2231 - val_acc: 0.9449\n",
      "Epoch 00017: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_196 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_196 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_874 (Dense)            (None, 93)                1102887   \n",
      "_________________________________________________________________\n",
      "dropout_679 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_875 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_680 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_876 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_681 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_877 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_682 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_878 (Dense)            (None, 1)                 94        \n",
      "=================================================================\n",
      "Total params: 1,129,207\n",
      "Trainable params: 1,129,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n",
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 30s 1ms/step - loss: 0.3826 - acc: 0.9441 - val_loss: 0.2404 - val_acc: 0.9534\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2391 - acc: 0.9466 - val_loss: 0.2203 - val_acc: 0.9534\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 179us/step - loss: 0.2252 - acc: 0.9466 - val_loss: 0.2107 - val_acc: 0.9534\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2177 - acc: 0.9466 - val_loss: 0.2051 - val_acc: 0.9534\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 194us/step - loss: 0.2158 - acc: 0.9466 - val_loss: 0.2046 - val_acc: 0.9534\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2154 - acc: 0.9466 - val_loss: 0.2065 - val_acc: 0.9534\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 170us/step - loss: 0.2111 - acc: 0.9466 - val_loss: 0.2113 - val_acc: 0.9534\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2118 - acc: 0.9466 - val_loss: 0.2047 - val_acc: 0.9534\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 6s 217us/step - loss: 0.2118 - acc: 0.9466 - val_loss: 0.2034 - val_acc: 0.9534\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2099 - acc: 0.9466 - val_loss: 0.2035 - val_acc: 0.9534\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 189us/step - loss: 0.2092 - acc: 0.9466 - val_loss: 0.2068 - val_acc: 0.9534\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 191us/step - loss: 0.2105 - acc: 0.9466 - val_loss: 0.2040 - val_acc: 0.9534\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 180us/step - loss: 0.2075 - acc: 0.9466 - val_loss: 0.2028 - val_acc: 0.9534\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 194us/step - loss: 0.2096 - acc: 0.9466 - val_loss: 0.2024 - val_acc: 0.9534\n",
      "Epoch 15/200\n",
      "26869/26869 [==============================] - 5s 198us/step - loss: 0.2060 - acc: 0.9466 - val_loss: 0.2001 - val_acc: 0.9534\n",
      "Epoch 16/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2069 - acc: 0.9466 - val_loss: 0.2037 - val_acc: 0.9534\n",
      "Epoch 17/200\n",
      "26869/26869 [==============================] - 5s 202us/step - loss: 0.2059 - acc: 0.9466 - val_loss: 0.1997 - val_acc: 0.9534\n",
      "Epoch 18/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2041 - acc: 0.9466 - val_loss: 0.2018 - val_acc: 0.9534\n",
      "Epoch 19/200\n",
      "26869/26869 [==============================] - 5s 173us/step - loss: 0.2036 - acc: 0.9466 - val_loss: 0.1988 - val_acc: 0.9534\n",
      "Epoch 20/200\n",
      "26869/26869 [==============================] - 6s 210us/step - loss: 0.2021 - acc: 0.9466 - val_loss: 0.2053 - val_acc: 0.9534\n",
      "Epoch 21/200\n",
      "26869/26869 [==============================] - 5s 172us/step - loss: 0.2029 - acc: 0.9466 - val_loss: 0.2024 - val_acc: 0.9534\n",
      "Epoch 22/200\n",
      "26869/26869 [==============================] - 5s 168us/step - loss: 0.2032 - acc: 0.9466 - val_loss: 0.2031 - val_acc: 0.9534\n",
      "Epoch 23/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2029 - acc: 0.9466 - val_loss: 0.2037 - val_acc: 0.9534\n",
      "Epoch 24/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2009 - acc: 0.9466 - val_loss: 0.2024 - val_acc: 0.9534\n",
      "Epoch 00024: early stopping\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_197 (InputLayer)       (None, 77, 154)           0         \n",
      "_________________________________________________________________\n",
      "flatten_197 (Flatten)        (None, 11858)             0         \n",
      "_________________________________________________________________\n",
      "dense_879 (Dense)            (None, 93)                1102887   \n",
      "_________________________________________________________________\n",
      "dropout_683 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_880 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_684 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_881 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_685 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_882 (Dense)            (None, 93)                8742      \n",
      "_________________________________________________________________\n",
      "dropout_686 (Dropout)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_883 (Dense)            (None, 1)                 94        \n",
      "=================================================================\n",
      "Total params: 1,129,207\n",
      "Trainable params: 1,129,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(26869, 77, 154)\n",
      "(6717, 77, 154)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26869 samples, validate on 6717 samples\n",
      "Epoch 1/200\n",
      "26869/26869 [==============================] - 30s 1ms/step - loss: 0.4220 - acc: 0.9452 - val_loss: 0.2874 - val_acc: 0.9449\n",
      "Epoch 2/200\n",
      "26869/26869 [==============================] - 5s 169us/step - loss: 0.2479 - acc: 0.9485 - val_loss: 0.2540 - val_acc: 0.9449\n",
      "Epoch 3/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2247 - acc: 0.9487 - val_loss: 0.2407 - val_acc: 0.9449\n",
      "Epoch 4/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2152 - acc: 0.9488 - val_loss: 0.2389 - val_acc: 0.9449\n",
      "Epoch 5/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2132 - acc: 0.9488 - val_loss: 0.2366 - val_acc: 0.9449\n",
      "Epoch 6/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2131 - acc: 0.9488 - val_loss: 0.2371 - val_acc: 0.9449\n",
      "Epoch 7/200\n",
      "26869/26869 [==============================] - 5s 182us/step - loss: 0.2104 - acc: 0.9488 - val_loss: 0.2395 - val_acc: 0.9449\n",
      "Epoch 8/200\n",
      "26869/26869 [==============================] - 5s 177us/step - loss: 0.2095 - acc: 0.9488 - val_loss: 0.2334 - val_acc: 0.9449\n",
      "Epoch 9/200\n",
      "26869/26869 [==============================] - 5s 171us/step - loss: 0.2094 - acc: 0.9488 - val_loss: 0.2271 - val_acc: 0.9449\n",
      "Epoch 10/200\n",
      "26869/26869 [==============================] - 5s 176us/step - loss: 0.2078 - acc: 0.9488 - val_loss: 0.2334 - val_acc: 0.9449\n",
      "Epoch 11/200\n",
      "26869/26869 [==============================] - 5s 178us/step - loss: 0.2074 - acc: 0.9488 - val_loss: 0.2430 - val_acc: 0.9449\n",
      "Epoch 12/200\n",
      "26869/26869 [==============================] - 5s 180us/step - loss: 0.2108 - acc: 0.9488 - val_loss: 0.2363 - val_acc: 0.9449\n",
      "Epoch 13/200\n",
      "26869/26869 [==============================] - 5s 175us/step - loss: 0.2060 - acc: 0.9488 - val_loss: 0.2351 - val_acc: 0.9449\n",
      "Epoch 14/200\n",
      "26869/26869 [==============================] - 5s 174us/step - loss: 0.2043 - acc: 0.9488 - val_loss: 0.2364 - val_acc: 0.9449\n",
      "Epoch 00014: early stopping\n",
      "-0.7506114009435871 0.025942705060722035\n",
      "  ::: ALL PARAMETERS :::   learning_rate dense_units dense_layers \n",
      "result value: -0.757 on step: 15.     params: 0.001      82         5          \n",
      "result value: -0.751 on step: 19.     params: 0.001410   93         4          \n",
      "result value: -0.746 on step: 14.     params: 0.001      69         4          \n",
      "result value: -0.742 on step: 16.     params: 0.001      10         6          \n",
      "result value: -0.739 on step: 18.     params: 0.001      20         5          \n",
      "result value: -0.731 on step:  0.     params: 0.001      20         3          \n",
      "result value: -0.729 on step: 17.     params: 0.001331   10         5          \n",
      "result value: -0.726 on step: 13.     params: 0.001      100        2          \n",
      "result value: -0.719 on step: 11.     params: 0.001619   10         2          \n",
      "result value: -0.716 on step:  9.     params: 0.002812   77         3          \n",
      "result value: -0.709 on step: 12.     params: 0.003363   88         2          \n",
      "result value: -0.698 on step:  2.     params: 0.004128   57         3          \n",
      "result value: -0.534 on step:  3.     params: 0.015644   47         3          \n",
      "result value: -0.5 on step:  7.     params: 0.015320   20         4          \n",
      "result value: -0.5 on step:  8.     params: 0.034141   22         5          \n",
      "result value: -0.5 on step:  5.     params: 0.003881   74         6          \n",
      "result value: -0.5 on step:  1.     params: 0.031923   34         6          \n",
      "result value: -0.5 on step:  4.     params: 0.097932   32         2          \n",
      "result value: -0.5 on step:  6.     params: 0.076177   78         5          \n",
      "result value: -0.5 on step: 10.     params: 0.022074   10         4          \n",
      "  ::: BEST SCORE :::\n",
      "-0.7574439437155631\n",
      "  ::: BEST PARAM :::\n",
      "{\n",
      "    \"learning_rate\": \"0.001\",\n",
      "    \"dense_units\": \"82\",\n",
      "    \"dense_layers\": \"5\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "history = handler.fit_base_model('action_binary_encoded', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023997187714071073\n"
     ]
    }
   ],
   "source": [
    "print( np.std(history) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-0dc589e4ae49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_nlp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'action_binary_encoded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-4ef265cfeb5f>\u001b[0m in \u001b[0;36mfit_nlp_model\u001b[0;34m(self, label, batch_size, test_split, val_split, verbose)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         model.train_on_batch(training_generator = train_gen, validation_generator = test_gen, \n\u001b[0;32m---> 90\u001b[0;31m                              epochs = 1, steps_per_epoch = steps_per_epoch, validation_steps = validation_steps )\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, training_generator, validation_generator, epochs, steps_per_epoch, validation_steps, early_stopping_callback, early_stopping)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#self.class_weights = get_class_weights(data.train.y, self.num_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/nlp_model.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Encode the words of the sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0msent_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_encoder_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_input_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Shape back to concat the matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/nlp_model.py\u001b[0m in \u001b[0;36mword_encoder_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mword_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mword_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mword_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwordEncoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \"\"\"\n\u001b[1;32m   2419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "handler.fit_nlp_model('action_binary_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit_generator in module keras.engine.training:\n",
      "\n",
      "fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      "    Trains the model on data generated batch-by-batch by a Python generator\n",
      "    (or an instance of `Sequence`).\n",
      "    \n",
      "    The generator is run in parallel to the model, for efficiency.\n",
      "    For instance, this allows you to do real-time data augmentation\n",
      "    on images on CPU in parallel to training your model on GPU.\n",
      "    \n",
      "    The use of `keras.utils.Sequence` guarantees the ordering\n",
      "    and guarantees the single use of every input per epoch when\n",
      "    using `use_multiprocessing=True`.\n",
      "    \n",
      "    # Arguments\n",
      "        generator: A generator or an instance of `Sequence`\n",
      "            (`keras.utils.Sequence`) object in order to avoid\n",
      "            duplicate data when using multiprocessing.\n",
      "            The output of the generator must be either\n",
      "            - a tuple `(inputs, targets)`\n",
      "            - a tuple `(inputs, targets, sample_weights)`.\n",
      "            This tuple (a single output of the generator) makes a single\n",
      "            batch. Therefore, all arrays in this tuple must have the same\n",
      "            length (equal to the size of this batch). Different batches may\n",
      "            have different sizes. For example, the last batch of the epoch\n",
      "            is commonly smaller than the others, if the size of the dataset\n",
      "            is not divisible by the batch size.\n",
      "            The generator is expected to loop over its data\n",
      "            indefinitely. An epoch finishes when `steps_per_epoch`\n",
      "            batches have been seen by the model.\n",
      "        steps_per_epoch: Integer.\n",
      "            Total number of steps (batches of samples)\n",
      "            to yield from `generator` before declaring one epoch\n",
      "            finished and starting the next epoch. It should typically\n",
      "            be equal to the number of samples of your dataset\n",
      "            divided by the batch size.\n",
      "            Optional for `Sequence`: if unspecified, will use\n",
      "            the `len(generator)` as a number of steps.\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire data provided,\n",
      "            as defined by `steps_per_epoch`.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See [callbacks](/callbacks).\n",
      "        validation_data: This can be either\n",
      "            - a generator or a `Sequence` object for the validation data\n",
      "            - tuple `(x_val, y_val)`\n",
      "            - tuple `(x_val, y_val, val_sample_weights)`\n",
      "            on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data.\n",
      "        validation_steps: Only relevant if `validation_data`\n",
      "            is a generator. Total number of steps (batches of samples)\n",
      "            to yield from `validation_data` generator before stopping\n",
      "            at the end of every epoch. It should typically\n",
      "            be equal to the number of samples of your\n",
      "            validation dataset divided by the batch size.\n",
      "            Optional for `Sequence`: if unspecified, will use\n",
      "            the `len(validation_data)` as a number of steps.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only). This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples\n",
      "            from an under-represented class.\n",
      "        max_queue_size: Integer. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Maximum number of processes to spin up\n",
      "            when using process-based threading.\n",
      "            If unspecified, `workers` will default to 1. If 0, will\n",
      "            execute the generator on the main thread.\n",
      "        use_multiprocessing: Boolean.\n",
      "            If `True`, use process-based threading.\n",
      "            If unspecified, `use_multiprocessing` will default to `False`.\n",
      "            Note that because this implementation\n",
      "            relies on multiprocessing,\n",
      "            you should not pass non-picklable arguments to the generator\n",
      "            as they can't be passed easily to children processes.\n",
      "        shuffle: Boolean. Whether to shuffle the order of the batches at\n",
      "            the beginning of each epoch. Only used with instances\n",
      "            of `Sequence` (`keras.utils.Sequence`).\n",
      "            Has no effect when `steps_per_epoch` is not `None`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "    \n",
      "    # Returns\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    # Raises\n",
      "        ValueError: In case the generator yields data in an invalid format.\n",
      "    \n",
      "    # Example\n",
      "    \n",
      "    ```python\n",
      "    def generate_arrays_from_file(path):\n",
      "        while True:\n",
      "            with open(path) as f:\n",
      "                for line in f:\n",
      "                    # create numpy arrays of input data\n",
      "                    # and labels, from each line in the file\n",
      "                    x1, x2, y = process_line(line)\n",
      "                    yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
      "    \n",
      "    model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
      "                        steps_per_epoch=10000, epochs=10)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(keras.models.Model.fit_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "\n",
      " \t ::: 1 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 50, 'regulizer_value': 0.0015, 'dropout_value': 0.015, 'learning_rate': 0.001}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        (None, 64, 151, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 19328)             0         \n",
      "_________________________________________________________________\n",
      "dense_375 (Dense)            (None, 50)                966450    \n",
      "_________________________________________________________________\n",
      "dropout_325 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_376 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_326 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_377 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_327 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_378 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 971,652\n",
      "Trainable params: 971,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "[<utils_train.model_utils.PredictDataCallback object at 0x7f1cdfbe3048>, <utils_train.model_utils.PredictDataCallback object at 0x7f1cdfbe39b0>, <keras.callbacks.EarlyStopping object at 0x7f1cdfbe3e80>]\n",
      "Train on 20072 samples, validate on 5018 samples\n",
      "Epoch 1/200\n",
      "20072/20072 [==============================] - 12s 585us/step - loss: 0.2588 - val_loss: 0.1660\n",
      "Epoch 2/200\n",
      "20072/20072 [==============================] - 3s 154us/step - loss: 0.1370 - val_loss: 0.1384\n",
      "Epoch 3/200\n",
      "20072/20072 [==============================] - 3s 143us/step - loss: 0.1265 - val_loss: 0.1339\n",
      "Epoch 4/200\n",
      "20072/20072 [==============================] - 3s 140us/step - loss: 0.1239 - val_loss: 0.1324\n",
      "Epoch 5/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1225 - val_loss: 0.1308\n",
      "Epoch 6/200\n",
      "20072/20072 [==============================] - 3s 142us/step - loss: 0.1217 - val_loss: 0.1298\n",
      "Epoch 7/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1207 - val_loss: 0.1291\n",
      "Epoch 8/200\n",
      "20072/20072 [==============================] - 3s 140us/step - loss: 0.1203 - val_loss: 0.1288\n",
      "Epoch 9/200\n",
      "20072/20072 [==============================] - 3s 142us/step - loss: 0.1200 - val_loss: 0.1286\n",
      "Epoch 10/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1199 - val_loss: 0.1284\n",
      "Epoch 11/200\n",
      "20072/20072 [==============================] - 3s 143us/step - loss: 0.1198 - val_loss: 0.1282\n",
      "Epoch 12/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1196 - val_loss: 0.1281\n",
      "Epoch 13/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1196 - val_loss: 0.1281\n",
      "Epoch 14/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1195 - val_loss: 0.1280\n",
      "Epoch 15/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1194 - val_loss: 0.1280\n",
      "Epoch 16/200\n",
      "20072/20072 [==============================] - 3s 146us/step - loss: 0.1193 - val_loss: 0.1279\n",
      "Epoch 17/200\n",
      "20072/20072 [==============================] - 3s 142us/step - loss: 0.1193 - val_loss: 0.1279\n",
      "Epoch 18/200\n",
      "20072/20072 [==============================] - 3s 141us/step - loss: 0.1193 - val_loss: 0.1278\n",
      "Epoch 19/200\n",
      "20072/20072 [==============================] - 3s 144us/step - loss: 0.1192 - val_loss: 0.1278\n",
      "Epoch 20/200\n",
      "20072/20072 [==============================] - 3s 144us/step - loss: 0.1192 - val_loss: 0.1278\n",
      "Epoch 21/200\n",
      "20072/20072 [==============================] - 3s 146us/step - loss: 0.1191 - val_loss: 0.1277\n",
      "Epoch 22/200\n",
      "20072/20072 [==============================] - 3s 146us/step - loss: 0.1192 - val_loss: 0.1277\n",
      "Epoch 23/200\n",
      "20072/20072 [==============================] - 3s 145us/step - loss: 0.1191 - val_loss: 0.1277\n",
      "Epoch 00023: early stopping\n",
      "Result: 0.5\n",
      "\n",
      " \t ::: 2 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 10, 'dense_units': 12, 'regulizer_value': 0.15189689846309112, 'dropout_value': 0.2686533062685247, 'learning_rate': 8.836464508307472e-05}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_52 (InputLayer)        (None, 64, 151, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 19328)             0         \n",
      "_________________________________________________________________\n",
      "dense_379 (Dense)            (None, 12)                231948    \n",
      "_________________________________________________________________\n",
      "dropout_328 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_380 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_329 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_381 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_330 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_382 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_331 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_383 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_332 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_384 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_333 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_385 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_334 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_386 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_335 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_387 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_336 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_388 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_337 (Dropout)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_389 (Dense)            (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 233,378\n",
      "Trainable params: 233,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "[<utils_train.model_utils.PredictDataCallback object at 0x7f1cdf0b6b00>, <utils_train.model_utils.PredictDataCallback object at 0x7f1cdf01ffd0>, <keras.callbacks.EarlyStopping object at 0x7f1cdf03eb70>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20072 samples, validate on 5018 samples\n",
      "Epoch 1/200\n",
      "20072/20072 [==============================] - 13s 638us/step - loss: 16.2009 - val_loss: 15.0451\n",
      "Epoch 2/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 14.2880 - val_loss: 13.6084\n",
      "Epoch 3/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 12.9366 - val_loss: 12.3275\n",
      "Epoch 4/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 11.7176 - val_loss: 11.1673\n",
      "Epoch 5/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 10.6118 - val_loss: 10.1134\n",
      "Epoch 6/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 9.6057 - val_loss: 9.1542\n",
      "Epoch 7/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 8.6895 - val_loss: 8.2782\n",
      "Epoch 8/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 7.8524 - val_loss: 7.4753\n",
      "Epoch 9/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 7.0870 - val_loss: 6.7436\n",
      "Epoch 10/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 6.3896 - val_loss: 6.0776\n",
      "Epoch 11/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 5.7528 - val_loss: 5.4705\n",
      "Epoch 12/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 5.1744 - val_loss: 4.9171\n",
      "Epoch 13/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 4.6458 - val_loss: 4.4128\n",
      "Epoch 14/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 4.1652 - val_loss: 3.9536\n",
      "Epoch 15/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 3.7270 - val_loss: 3.5361\n",
      "Epoch 16/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 3.3292 - val_loss: 3.1566\n",
      "Epoch 17/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 2.9677 - val_loss: 2.8124\n",
      "Epoch 18/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 2.6403 - val_loss: 2.5006\n",
      "Epoch 19/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 2.3440 - val_loss: 2.2187\n",
      "Epoch 20/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 2.0767 - val_loss: 1.9646\n",
      "Epoch 21/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 1.8357 - val_loss: 1.7358\n",
      "Epoch 22/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 1.6196 - val_loss: 1.5306\n",
      "Epoch 23/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 1.4251 - val_loss: 1.3470\n",
      "Epoch 24/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 1.2514 - val_loss: 1.1833\n",
      "Epoch 25/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 1.0974 - val_loss: 1.0379\n",
      "Epoch 26/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.9605 - val_loss: 0.9094\n",
      "Epoch 27/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.8402 - val_loss: 0.7962\n",
      "Epoch 28/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.7342 - val_loss: 0.6969\n",
      "Epoch 29/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.6418 - val_loss: 0.6103\n",
      "Epoch 30/200\n",
      "20072/20072 [==============================] - 3s 166us/step - loss: 0.5605 - val_loss: 0.5353\n",
      "Epoch 31/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 0.4923 - val_loss: 0.4706\n",
      "Epoch 32/200\n",
      "20072/20072 [==============================] - 4s 189us/step - loss: 0.4314 - val_loss: 0.4152\n",
      "Epoch 33/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.3806 - val_loss: 0.3680\n",
      "Epoch 34/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.3369 - val_loss: 0.3282\n",
      "Epoch 35/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.3005 - val_loss: 0.2947\n",
      "Epoch 36/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.2702 - val_loss: 0.2668\n",
      "Epoch 37/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.2450 - val_loss: 0.2438\n",
      "Epoch 38/200\n",
      "20072/20072 [==============================] - 4s 187us/step - loss: 0.2239 - val_loss: 0.2250\n",
      "Epoch 39/200\n",
      "20072/20072 [==============================] - 4s 186us/step - loss: 0.2074 - val_loss: 0.2096\n",
      "Epoch 40/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1937 - val_loss: 0.1972\n",
      "Epoch 41/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1827 - val_loss: 0.1873\n",
      "Epoch 42/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1739 - val_loss: 0.1793\n",
      "Epoch 43/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.1663 - val_loss: 0.1730\n",
      "Epoch 44/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1610 - val_loss: 0.1680\n",
      "Epoch 45/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1567 - val_loss: 0.1640\n",
      "Epoch 46/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 0.1532 - val_loss: 0.1608\n",
      "Epoch 47/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1502 - val_loss: 0.1581\n",
      "Epoch 48/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.1476 - val_loss: 0.1560\n",
      "Epoch 49/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1461 - val_loss: 0.1541\n",
      "Epoch 50/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1444 - val_loss: 0.1525\n",
      "Epoch 51/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1428 - val_loss: 0.1510\n",
      "Epoch 52/200\n",
      "20072/20072 [==============================] - 4s 179us/step - loss: 0.1412 - val_loss: 0.1497\n",
      "Epoch 53/200\n",
      "20072/20072 [==============================] - 4s 185us/step - loss: 0.1400 - val_loss: 0.1485\n",
      "Epoch 54/200\n",
      "20072/20072 [==============================] - 4s 179us/step - loss: 0.1390 - val_loss: 0.1473\n",
      "Epoch 55/200\n",
      "20072/20072 [==============================] - 4s 183us/step - loss: 0.1380 - val_loss: 0.1462\n",
      "Epoch 56/200\n",
      "20072/20072 [==============================] - 4s 185us/step - loss: 0.1368 - val_loss: 0.1451\n",
      "Epoch 57/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1354 - val_loss: 0.1441\n",
      "Epoch 58/200\n",
      "20072/20072 [==============================] - 4s 184us/step - loss: 0.1346 - val_loss: 0.1432\n",
      "Epoch 59/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1341 - val_loss: 0.1423\n",
      "Epoch 60/200\n",
      "20072/20072 [==============================] - 4s 181us/step - loss: 0.1331 - val_loss: 0.1414\n",
      "Epoch 61/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.1322 - val_loss: 0.1406\n",
      "Epoch 62/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.1314 - val_loss: 0.1398\n",
      "Epoch 63/200\n",
      "20072/20072 [==============================] - 4s 182us/step - loss: 0.1306 - val_loss: 0.1390\n",
      "Epoch 64/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1300 - val_loss: 0.1383\n",
      "Epoch 65/200\n",
      "20072/20072 [==============================] - 4s 174us/step - loss: 0.1291 - val_loss: 0.1376\n",
      "Epoch 66/200\n",
      "20072/20072 [==============================] - 4s 187us/step - loss: 0.1286 - val_loss: 0.1370\n",
      "Epoch 67/200\n",
      "20072/20072 [==============================] - 4s 186us/step - loss: 0.1279 - val_loss: 0.1363\n",
      "Epoch 68/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1273 - val_loss: 0.1358\n",
      "Epoch 69/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1266 - val_loss: 0.1352\n",
      "Epoch 70/200\n",
      "20072/20072 [==============================] - 4s 189us/step - loss: 0.1261 - val_loss: 0.1347\n",
      "Epoch 71/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1257 - val_loss: 0.1342\n",
      "Epoch 72/200\n",
      "20072/20072 [==============================] - 4s 182us/step - loss: 0.1252 - val_loss: 0.1337\n",
      "Epoch 73/200\n",
      "20072/20072 [==============================] - 4s 179us/step - loss: 0.1247 - val_loss: 0.1332\n",
      "Epoch 74/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 0.1242 - val_loss: 0.1328\n",
      "Epoch 75/200\n",
      "20072/20072 [==============================] - 4s 180us/step - loss: 0.1239 - val_loss: 0.1324\n",
      "Epoch 76/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1235 - val_loss: 0.1320\n",
      "Epoch 77/200\n",
      "20072/20072 [==============================] - 4s 174us/step - loss: 0.1231 - val_loss: 0.1316\n",
      "Epoch 78/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1227 - val_loss: 0.1313\n",
      "Epoch 79/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1223 - val_loss: 0.1310\n",
      "Epoch 80/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1221 - val_loss: 0.1307\n",
      "Epoch 81/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1218 - val_loss: 0.1304\n",
      "Epoch 82/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1215 - val_loss: 0.1301\n",
      "Epoch 83/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1213 - val_loss: 0.1298\n",
      "Epoch 84/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1210 - val_loss: 0.1296\n",
      "Epoch 85/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1207 - val_loss: 0.1294\n",
      "Epoch 86/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1205 - val_loss: 0.1292\n",
      "Epoch 87/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1203 - val_loss: 0.1290\n",
      "Epoch 88/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1202 - val_loss: 0.1288\n",
      "Epoch 89/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1200 - val_loss: 0.1287\n",
      "Epoch 90/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1198 - val_loss: 0.1285\n",
      "Epoch 91/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1197 - val_loss: 0.1284\n",
      "Epoch 92/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1195 - val_loss: 0.1282\n",
      "Epoch 93/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1194 - val_loss: 0.1281\n",
      "Epoch 94/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1193 - val_loss: 0.1280\n",
      "Epoch 95/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1192 - val_loss: 0.1279\n",
      "Epoch 96/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1191 - val_loss: 0.1278\n",
      "Epoch 97/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 0.1190 - val_loss: 0.1278\n",
      "Epoch 98/200\n",
      "20072/20072 [==============================] - 4s 177us/step - loss: 0.1190 - val_loss: 0.1277\n",
      "Epoch 99/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 0.1189 - val_loss: 0.1276\n",
      "Epoch 100/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1188 - val_loss: 0.1276\n",
      "Epoch 101/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1188 - val_loss: 0.1275\n",
      "Epoch 102/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1187 - val_loss: 0.1275\n",
      "Epoch 103/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1187 - val_loss: 0.1274\n",
      "Epoch 104/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 0.1187 - val_loss: 0.1274\n",
      "Epoch 105/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1186 - val_loss: 0.1274\n",
      "Epoch 106/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1274\n",
      "Epoch 107/200\n",
      "20072/20072 [==============================] - 3s 168us/step - loss: 0.1186 - val_loss: 0.1274\n",
      "Epoch 108/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 109/200\n",
      "20072/20072 [==============================] - 4s 175us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 110/200\n",
      "20072/20072 [==============================] - 4s 178us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 111/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 112/200\n",
      "20072/20072 [==============================] - 3s 174us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 113/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 114/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 115/200\n",
      "20072/20072 [==============================] - 3s 172us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 116/200\n",
      "20072/20072 [==============================] - 3s 171us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 117/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 118/200\n",
      "20072/20072 [==============================] - 4s 176us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 119/200\n",
      "20072/20072 [==============================] - 3s 170us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 120/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 121/200\n",
      "20072/20072 [==============================] - 3s 173us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 122/200\n",
      "20072/20072 [==============================] - 3s 167us/step - loss: 0.1186 - val_loss: 0.1273\n",
      "Epoch 00122: early stopping\n",
      "Result: 0.5\n",
      "\n",
      " \t ::: 3 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 73, 'regulizer_value': 0.5158612809419505, 'dropout_value': 0.3225979543593234, 'learning_rate': 0.00032554867843791457}\n",
      "Set early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_53 (InputLayer)        (None, 64, 151, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 19328)             0         \n",
      "_________________________________________________________________\n",
      "dense_390 (Dense)            (None, 73)                1411017   \n",
      "_________________________________________________________________\n",
      "dropout_338 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_391 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_339 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_392 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_340 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_393 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_341 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_394 (Dense)            (None, 73)                5402      \n",
      "_________________________________________________________________\n",
      "dropout_342 (Dropout)        (None, 73)                0         \n",
      "_________________________________________________________________\n",
      "dense_395 (Dense)            (None, 2)                 148       \n",
      "=================================================================\n",
      "Total params: 1,432,773\n",
      "Trainable params: 1,432,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(20072, 64, 151, 2)\n",
      "(5018, 64, 151, 2)\n",
      "[<utils_train.model_utils.PredictDataCallback object at 0x7f1cdddcc240>, <utils_train.model_utils.PredictDataCallback object at 0x7f1cdddccac8>, <keras.callbacks.EarlyStopping object at 0x7f1cdddccb00>]\n",
      "Train on 20072 samples, validate on 5018 samples\n",
      "Epoch 1/200\n",
      "20072/20072 [==============================] - 13s 627us/step - loss: 102.8321 - val_loss: 61.0412\n",
      "Epoch 2/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 39.6956 - val_loss: 23.6965\n",
      "Epoch 3/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 15.1019 - val_loss: 8.7539\n",
      "Epoch 4/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 5.4479 - val_loss: 3.0754\n",
      "Epoch 5/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 1.8900 - val_loss: 1.0784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.6856 - val_loss: 0.4395\n",
      "Epoch 7/200\n",
      "20072/20072 [==============================] - 3s 162us/step - loss: 0.3154 - val_loss: 0.2530\n",
      "Epoch 8/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.2107 - val_loss: 0.2009\n",
      "Epoch 9/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1802 - val_loss: 0.1838\n",
      "Epoch 10/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1688 - val_loss: 0.1752\n",
      "Epoch 11/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1619 - val_loss: 0.1691\n",
      "Epoch 12/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1566 - val_loss: 0.1639\n",
      "Epoch 13/200\n",
      "20072/20072 [==============================] - 3s 162us/step - loss: 0.1520 - val_loss: 0.1596\n",
      "Epoch 14/200\n",
      "20072/20072 [==============================] - 3s 161us/step - loss: 0.1481 - val_loss: 0.1558\n",
      "Epoch 15/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1448 - val_loss: 0.1525\n",
      "Epoch 16/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1417 - val_loss: 0.1495\n",
      "Epoch 17/200\n",
      "20072/20072 [==============================] - 3s 169us/step - loss: 0.1392 - val_loss: 0.1470\n",
      "Epoch 18/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1368 - val_loss: 0.1447\n",
      "Epoch 19/200\n",
      "20072/20072 [==============================] - 3s 165us/step - loss: 0.1347 - val_loss: 0.1427\n",
      "Epoch 20/200\n",
      "20072/20072 [==============================] - 3s 163us/step - loss: 0.1329 - val_loss: 0.1410\n",
      "Epoch 21/200\n",
      "20072/20072 [==============================] - 3s 164us/step - loss: 0.1310 - val_loss: 0.1392\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-ecaab41b5a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'action_binary_encoded'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-135-288214bb3174>\u001b[0m in \u001b[0;36mfit_base_model\u001b[0;34m(self, label, batch_size, test_split, val_split, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_optimal_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mfind_optimal_parameters\u001b[0;34m(self, X_train, y_train, X_test, y_test, num_calls, evaluation_function, max_epochs, batch_size, early_stopping_callback, seed, verbose, summary_txt_path)\u001b[0m\n\u001b[1;32m    149\u001b[0m         search_result = gp_minimize( func = fitness, dimensions = self.dimensions,\n\u001b[1;32m    150\u001b[0m                                      \u001b[0macq_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Expected Improvement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                                      n_calls = num_calls, x0 = prior_values )\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         s = create_skopt_results_string( search_result, prior_names, \n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(**p)\u001b[0m\n\u001b[1;32m    138\u001b[0m             self.train( X_train, y_train, X_test, y_test, max_epochs=max_epochs, batch_size=batch_size,  \n\u001b[1;32m    139\u001b[0m                         \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                         seed=seed, verbose=verbose )\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/base_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, X_val, y_val, max_epochs, batch_size, seed, verbose, early_stopping_callback, early_stopping)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         history = self.model.fit( X_train, y_train, validation_data = (X_val, y_val), \\\n\u001b[0;32m--> 104\u001b[0;31m                                  epochs = max_epochs, batch_size = batch_size, callbacks = keras_callbacks)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIErrorLogAnalysis/training/utils_train/model_utils.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_word\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_word\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'loss', 'predictions', 'labels', 'val_predictions', 'val_labels'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "##############\n",
    "# Functions to yield batches for the training\n",
    "##############\n",
    "\n",
    "\n",
    "def train_generator(self, batch_size):\n",
    "\n",
    "    train_gen = InputBatchGenerator(self.X_train, self.y_train, batch_size, self.codes, self.sites, self.dim_msg)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            for B in train_gen.generate_msg_count_batches():\n",
    "                yield B\n",
    "        except StopIteration:\n",
    "            logging.warning(\"start over generator loop\")          \n",
    "\n",
    "def test_generator(self, batch_size):\n",
    "\n",
    "    test_gen = InputBatchGenerator(self.X_test, self.y_test, batch_size, self.codes, self.sites, self.dim_msg)\n",
    "    for B in test_gen.generate_msg_count_batches():\n",
    "        yield B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
