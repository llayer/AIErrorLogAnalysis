{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings based on small error messages\n",
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new = spark.read.csv('hdfs:///cms/users/llayer/df_errors_11102017_01042019.csv', header=True)\n",
    "df1 = spark.read.csv('hdfs:///cms/users/llayer/df_reduced_codes.csv', header=True)\n",
    "df2 = spark.read.csv('hdfs:///cms/users/llayer/df_reduced_codes2.csv', header=True)\n",
    "#df2 = spark.read.csv('hdfs:///cms/users/llayer/df__exitcodes2.csv', header=True)\n",
    "#df2 = spark.read.csv('hdfs:///cms/users/llayer/df_errors_small2.csv', header=True)\n",
    "#df_old = spark.read.csv('hdfs:///cms/users/llayer/df_errors_01052017_09102017.csv', header=True)\n",
    "#df_old2 = spark.read.csv('hdfs:///cms/users/llayer/df_errors_01012017_01052017.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hash vectorizer + IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,6,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,6,...|\n",
      "|(20,[0,1,3,5,7,8,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,5,7,9,...|\n",
      "|(20,[0,1,3,4,5,6,...|\n",
      "|(20,[0,1,3,4,5,6,...|\n",
      "|(20,[0,3,4,5,6,7,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,2,3,4,5,7,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the tokens\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"error_msg\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(df)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(tokenized)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "print rescaledData.select('features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "\n",
    "model = cv.fit(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+--------------------+----------+------+------+--------------------+--------------------+\n",
      "|           task_name|               site|error|           error_msg|side_state|action|memory|               words|            features|\n",
      "+--------------------+-------------------+-----+--------------------+----------+------+------+--------------------+--------------------+\n",
      "|/fabozzi_Run2016D...|    T1_US_FNAL_Disk|   85|Adding last 25 li...|  bad_site|  acdc|  null|[adding, last, 25...|(3,[0,2],[141.0,7...|\n",
      "|/fabozzi_Run2017B...|         T1_RU_JINR|   92|Adding last 25 li...| good_site|  acdc|  null|[adding, last, 25...|(3,[0,2],[66.0,7.0])|\n",
      "|/fabozzi_Run2017F...|T2_UK_London_Brunel|99109|Error in StageOut...| good_site|  acdc|  null|[error, in, stage...|(3,[0,2],[639.0,2...|\n",
      "|/fabozzi_Run2017F...|          T2_US_MIT| 8004|An exception of c...| good_site|  acdc|180000|[an, exception, o...|     (3,[0],[139.0])|\n",
      "|/fabozzi_Run2017H...|     T2_US_Nebraska|   92|Adding last 25 li...| good_site|  acdc|  null|[adding, last, 25...|(3,[0,2],[78.0,7.0])|\n",
      "|/mcremone_ACDC0_t...|    T2_UK_London_IC|   92|Adding last 25 li...|  bad_site|  acdc|  null|[adding, last, 25...|(3,[0,2],[77.0,7.0])|\n",
      "|/mcremone_task_HI...|     T2_CH_CERN_HLT|50660|Error in CMSSW st...| good_site|  acdc| 18000|[error, in, cmssw...|           (3,[],[])|\n",
      "|/mcremone_task_HI...|         T2_CH_CERN|   92|Adding last 25 li...| good_site|  acdc|  null|[adding, last, 25...|(3,[0,2],[85.0,7.0])|\n",
      "|/pdmvserv_task_B2...|         T1_IT_CNAF|50660|Error in CMSSW st...| good_site|  acdc|  8000|[error, in, cmssw...|           (3,[],[])|\n",
      "|/pdmvserv_task_B2...|         T2_US_UCSD|50660|Error in CMSSW st...| good_site|  acdc| 20000|[error, in, cmssw...|           (3,[],[])|\n",
      "|/pdmvserv_task_B2...|    T2_UK_London_IC|50660|Error in CMSSW st...| good_site|  acdc| 20000|[error, in, cmssw...|           (3,[],[])|\n",
      "|/pdmvserv_task_B2...|     T2_US_Nebraska|50664|Error in CMSSW st...| good_site|  acdc|  null|[error, in, cmssw...|           (3,[],[])|\n",
      "|/pdmvserv_task_B2...|T2_UK_London_Brunel|   85|Adding last 25 li...| good_site|  acdc|  null|[adding, last, 25...|(3,[0,2],[164.0,7...|\n",
      "|/pdmvserv_task_B2...|      T1_FR_CCIN2P3|   92|Adding last 25 li...| good_site|  acdc|  null|[adding, last, 25...|(3,[0,2],[89.0,7.0])|\n",
      "|/pdmvserv_task_B2...|    T2_US_Wisconsin|   73|Adding last 25 li...| good_site| clone|  null|[adding, last, 25...|(3,[0,2],[23.0,7.0])|\n",
      "|/pdmvserv_task_B2...|          T1_UK_RAL|   53|Adding last 25 li...| good_site|  acdc|  null|[adding, last, 25...|(3,[0,2],[55.0,7.0])|\n",
      "|/pdmvserv_task_B2...|         T2_BE_IIHE|99303|Could not find jo...| good_site|  acdc|  null|[could, not, find...|(3,[0,1],[184.0,4...|\n",
      "|/pdmvserv_task_B2...|         T2_BE_IIHE|99303|Could not find jo...| good_site|  acdc|  null|[could, not, find...|(3,[0,1],[184.0,4...|\n",
      "|/pdmvserv_task_B2...|    T2_UK_London_IC|   92|Adding last 25 li...| good_site|  acdc|  null|[adding, last, 25...|(3,[0,2],[77.0,7.0])|\n",
      "|/pdmvserv_task_BP...|  T2_UK_SGrid_RALPP|   84|Adding last 25 li...| good_site| clone|  null|[adding, last, 25...|(3,[0,2],[139.0,7...|\n",
      "+--------------------+-------------------+-----+--------------------+----------+------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(tokenized)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 idf|               words|\n",
      "+--------------------+--------------------+\n",
      "|(3,[0,2],[36.4053...|[adding, last, 25...|\n",
      "|(3,[0,2],[17.0408...|[adding, last, 25...|\n",
      "|(3,[0,2],[164.986...|[error, in, stage...|\n",
      "|(3,[0],[35.888978...|[an, exception, o...|\n",
      "|(3,[0,2],[20.1391...|[adding, last, 25...|\n",
      "|(3,[0,2],[19.8809...|[adding, last, 25...|\n",
      "|           (3,[],[])|[error, in, cmssw...|\n",
      "|(3,[0,2],[21.9464...|[adding, last, 25...|\n",
      "|           (3,[],[])|[error, in, cmssw...|\n",
      "|           (3,[],[])|[error, in, cmssw...|\n",
      "|           (3,[],[])|[error, in, cmssw...|\n",
      "|           (3,[],[])|[error, in, cmssw...|\n",
      "|(3,[0,2],[42.3438...|[adding, last, 25...|\n",
      "|(3,[0,2],[22.9792...|[adding, last, 25...|\n",
      "|(3,[0,2],[5.93846...|[adding, last, 25...|\n",
      "|(3,[0,2],[14.2006...|[adding, last, 25...|\n",
      "|(3,[0,1],[47.5077...|[could, not, find...|\n",
      "|(3,[0,1],[47.5077...|[could, not, find...|\n",
      "|(3,[0,2],[19.8809...|[adding, last, 25...|\n",
      "|(3,[0,2],[35.8889...|[adding, last, 25...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(result)\n",
    "rescaledData = idfModel.transform(result)\n",
    "\n",
    "rescaledData.select('idf', 'words').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"error_msg\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(df)\n",
    "word2vec = Word2Vec(inputCol=\"words\", outputCol=\"w2v\", vectorSize = 10)\n",
    "model = word2vec.fit(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+\n",
      "|           task_name|               site|                 w2v|\n",
      "+--------------------+-------------------+--------------------+\n",
      "|/fabozzi_Run2016D...|    T1_US_FNAL_Disk|[-0.6767393925434...|\n",
      "|/fabozzi_Run2017B...|         T1_RU_JINR|[-0.6808537950240...|\n",
      "|/fabozzi_Run2017F...|T2_UK_London_Brunel|[-0.1858461705453...|\n",
      "|/fabozzi_Run2017F...|          T2_US_MIT|[-0.4759413566384...|\n",
      "|/fabozzi_Run2017H...|     T2_US_Nebraska|[-0.7338168016692...|\n",
      "|/mcremone_ACDC0_t...|    T2_UK_London_IC|[-0.6695511883629...|\n",
      "|/mcremone_task_HI...|     T2_CH_CERN_HLT|[0.03431056173784...|\n",
      "|/mcremone_task_HI...|         T2_CH_CERN|[-0.6576490365118...|\n",
      "|/pdmvserv_task_B2...|         T1_IT_CNAF|[-0.0120665904666...|\n",
      "|/pdmvserv_task_B2...|         T2_US_UCSD|[-0.1310454917450...|\n",
      "|/pdmvserv_task_B2...|    T2_UK_London_IC|[-0.1272198777231...|\n",
      "|/pdmvserv_task_B2...|     T2_US_Nebraska|[-0.1811169521124...|\n",
      "|/pdmvserv_task_B2...|T2_UK_London_Brunel|[-0.6458878479538...|\n",
      "|/pdmvserv_task_B2...|      T1_FR_CCIN2P3|[-0.6606843486992...|\n",
      "|/pdmvserv_task_B2...|    T2_US_Wisconsin|[-0.9319566702423...|\n",
      "|/pdmvserv_task_B2...|          T1_UK_RAL|[-0.8665516190057...|\n",
      "|/pdmvserv_task_B2...|         T2_BE_IIHE|[-0.0798195473646...|\n",
      "|/pdmvserv_task_B2...|         T2_BE_IIHE|[-0.1300440456680...|\n",
      "|/pdmvserv_task_B2...|    T2_UK_London_IC|[-0.6611040220357...|\n",
      "|/pdmvserv_task_BP...|  T2_UK_SGrid_RALPP|[-0.6317876752459...|\n",
      "+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(tokenized).coalesce(200).persist()\n",
    "print result.select(['task_name', 'site', 'w2v']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = result.select(['task_name', 'error', 'site','w2v']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.to_csv('/eos/user/l/llayer/AIErrorHandling/df_word2vec_exitcodes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "sparkconnect": {
   "bundled_options": [
    "CMSSpark"
   ],
   "list_of_options": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
