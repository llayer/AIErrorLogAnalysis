{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word embeddings from small error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'utils.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data from HDFS with the WMArchive entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timerange = [20180706, 20180906]\n",
    "#timerange = [20171011, 20190401]\n",
    "#timerange = [20190207, 20190307]\n",
    "timerange = [20180704, 20181004]\n",
    "#timerange = [20170101, 20171009]\n",
    "dirs = ut.getDirs( timerange )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs:///cms/wmarchive/avro/fwjr/2018/07/04', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/05', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/06', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/07', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/08', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/09', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/10', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/11', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/12', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/13', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/14', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/15', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/16', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/17', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/18', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/19', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/20', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/21', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/22', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/23', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/24', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/25', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/26', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/27', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/28', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/29', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/30', 'hdfs:///cms/wmarchive/avro/fwjr/2018/07/31', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/01', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/02', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/03', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/06', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/07', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/08', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/09', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/10', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/11', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/12', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/13', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/14', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/15', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/16', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/20', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/22', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/23', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/24', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/25', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/26', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/27', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/28', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/29', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/30', 'hdfs:///cms/wmarchive/avro/fwjr/2018/08/31', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/01', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/02', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/03', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/04', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/05', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/06', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/07', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/08', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/09', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/10', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/11', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/12', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/13', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/14', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/15', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/16', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/17', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/18', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/19', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/20', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/21', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/22', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/23', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/24', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/25', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/26', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/27', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/28', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/29', 'hdfs:///cms/wmarchive/avro/fwjr/2018/09/30', 'hdfs:///cms/wmarchive/avro/fwjr/2018/10/01', 'hdfs:///cms/wmarchive/avro/fwjr/2018/10/02', 'hdfs:///cms/wmarchive/avro/fwjr/2018/10/03', 'hdfs:///cms/wmarchive/avro/fwjr/2018/10/04']\n"
     ]
    }
   ],
   "source": [
    "print dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_file = 'hdfs:///cms/wmarchive/avro/schema.avsc'\n",
    "#schema_file = 'hdfs:///cms/wmarchive/avro/schemas/current.avsc.20161215'\n",
    "rdd = sc.textFile(schema_file, 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input avro schema, the rdd is a list of lines (sc.textFile similar to readlines)\n",
    "avsc = reduce(lambda x, y: x + y, rdd) # merge all entries from rdd list\n",
    "schema = ''.join(avsc.split()) # remove spaces in avsc map\n",
    "conf = {\"avro.schema.input.key\": schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define newAPIHadoopFile parameters, java classes\n",
    "aformat=\"org.apache.avro.mapreduce.AvroKeyInputFormat\"\n",
    "akey=\"org.apache.avro.mapred.AvroKey\"\n",
    "awrite=\"org.apache.hadoop.io.NullWritable\"\n",
    "aconv=\"org.apache.spark.examples.pythonconverters.AvroWrapperToJavaConverter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = dirs\n",
    "# load data from HDFS\n",
    "if  isinstance(data_path, list):\n",
    "    avro_rdd = sc.union([sc.newAPIHadoopFile(f, aformat, akey, awrite, aconv, conf=conf) for f in data_path])\n",
    "else:\n",
    "    avro_rdd = sc.newAPIHadoopFile(data_path, aformat, akey, awrite, aconv, conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter out the failing tasks and create a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the tasks - keep only failing \n",
    "def getFailing(row):\n",
    "    rec = row[0]\n",
    "    meta = rec.get('meta_data', {})\n",
    "    if meta.get('jobstate', '') != 'jobfailed':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# create task, site, error key  \n",
    "def avro_rdd_KV(row):\n",
    "    rec = row[0]\n",
    "    task = rec[\"task\"]\n",
    "    steps = rec.get('steps', [])\n",
    "    status = []\n",
    "    sites = []\n",
    "    error_msg = []\n",
    "    \n",
    "    for step in steps:\n",
    "        errors = step['errors']\n",
    "        #details = []\n",
    "        #exitCodes = []\n",
    "        for error in errors:\n",
    "            #details.append(error['details'])\n",
    "            #exitCodes.append(error['exitCode'])\n",
    "            exitCode = error['exitCode']\n",
    "            msg = error['details'].replace(\"\\n\", \" \").replace('\\r', ' ')\n",
    "            sites.append( step.get('site','') )\n",
    "            status.append( exitCode )\n",
    "            error_msg.append( msg )\n",
    "        #details = list(set(details))\n",
    "        #flat_msg = ''\n",
    "        #for d in details:\n",
    "        #    flat_msg += d\n",
    "        #flat_msg = flat_msg.replace(\"\\n\", \" \").replace('\\r', ' ')\n",
    "        #error_msg.append(flat_msg)\n",
    "        #exitCodes = list(set(exitCodes))\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(exitCodes) < 1:\n",
    "            exitCode = -1\n",
    "        else:\n",
    "            exitCode = exitCodes[0]\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        exitCode = -9999\n",
    "        if not len(exitCodes) > 1:\n",
    "            exitCode = exitCodes[0]\n",
    "        status.append( exitCode )\n",
    "        \"\"\"\n",
    "        #status.append( exitCodes )\n",
    "        #status.append( step.get('exitCode','') )\n",
    "        #sites.append( step.get('site','') )\n",
    "    #return (task, rec)\n",
    "    \n",
    "    return [(task, site, error, msg) for error, msg, site in zip(status, error_msg, sites) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "failing_workflows = avro_rdd.filter(lambda x : getFailing(x)).flatMap(lambda x : avro_rdd_KV(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "failing_workflows_df_test = failing_workflows.toDF([\"task_name\", \"site\", \"error\", \"error_msg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+--------------------+\n",
      "|           task_name|site|error|           error_msg|\n",
      "+--------------------+----+-----+--------------------+\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_BT...|null|99303|Could not find jo...|\n",
      "+--------------------+----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failing_workflows_df_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_problem(row):\n",
    "    #task_name = row[0]\n",
    "    rec = row[0]\n",
    "    task_name = rec[\"task\"]\n",
    "    #task_name = row[0]\n",
    "    #error = row[2]\n",
    "    #test_task = '/pdmvserv_task_HIG-RunIIFall17wmLHEGS-03477__v1_T_190208_210318_4829/HIG-RunIIFall17wmLHEGS-03477_0'\n",
    "    #test_error = 99109\n",
    "    #test_task = '/pdmvserv_task_B2G-RunIISummer15wmLHEGS-01375__v1_T_180703_203928_6625/B2G-RunIISummer15wmLHEGS-01375_0'\n",
    "    #test_task = '/pdmvserv_task_HIG-RunIIFall17wmLHEGS-00630__v1_T_180216_120647_5907/HIG-RunIIFall17wmLHEGS-00630_0/HIG-RunIIFall17DRPremix-00737_0'\n",
    "    #test_task = '/vlimant_ACDC0_task_HIG-RunIIFall17wmLHEGS-01415__v1_T_180706_002124_986/HIG-RunIIFall17DRPremix-02001_1/HIG-RunIIFall17DRPremix-02001_1MergeAODSIMoutput/HIG-RunIIFall17MiniAODv2-01299_0'\n",
    "    #test_error = 85\n",
    "    #test_task = '/pdmvserv_task_HIG-RunIIFall17wmLHEGS-00689__v1_T_180307_201555_2433/HIG-RunIIFall17wmLHEGS-00689_0/HIG-RunIIFall17DRPremix-00886_0'\n",
    "    test_task = '/pdmvserv_task_HIG-RunIIFall17wmLHEGS-02145__v1_T_180705_162228_8813/HIG-RunIIFall17wmLHEGS-02145_0/HIG-RunIIFall17DRPremix-02708_0'\n",
    "    if (task_name == test_task): # and (error == test_error):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = avro_rdd.filter(lambda x : getFailing(x)).filter(lambda x : filter_for_problem(x))\n",
    "#failures = avro_rdd.filter(lambda x : getFailing(x)).flatMap(lambda x : avro_rdd_KV(x)).filter(lambda x : filter_for_problem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures.saveAsTextFile(\"hdfs:///cms/users/llayer/debug5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = failures.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1_UK_RAL 85\n",
      "T1_UK_RAL 85\n",
      "T1_UK_RAL 85\n",
      "T1_UK_RAL 85\n",
      "T1_UK_RAL 85\n",
      "T1_UK_RAL 85\n",
      "T1_FR_CCIN2P3 85\n",
      "T1_FR_CCIN2P3 85\n",
      "T1_UK_RAL 85\n",
      "T1_UK_RAL 85\n",
      "T1_UK_RAL 85\n",
      "T1_UK_RAL 85\n"
     ]
    }
   ],
   "source": [
    "for prob in probs:\n",
    "    print prob[1], prob[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_key(row):\n",
    "    return (row[0], row[1], row[2]), row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'/pdmvserv_task_SMP-RunIISummer16DR80Premix-00011__v1_T_161227_083845_4347/SMP-RunIISummer16DR80Premix-00011_0', u'T2_UK_London_IC', 99999), u'Could not find report file for step stageOut1!')]\n"
     ]
    }
   ],
   "source": [
    "print failing_workflows.map(lambda x : map_to_key(x)).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceToLongest(row):\n",
    "    key = row[0]\n",
    "    message_list = list(row[1])\n",
    "    longest_msg = ''\n",
    "    for msg in message_list:\n",
    "        if len(msg) > len(longest_msg):\n",
    "            longest_msg = msg\n",
    "    return key[0], key[1], key[2], longest_msg\n",
    "\n",
    "failing_workflows_reduce = failing_workflows.map(lambda x : map_to_key(x)).groupByKey().map(lambda x : reduceToLongest(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "failing_workflows_df = failing_workflows_reduce.toDF([\"task_name\", \"site\", \"error\", \"error_msg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-----+--------------------+\n",
      "|           task_name|           site|error|           error_msg|\n",
      "+--------------------+---------------+-----+--------------------+\n",
      "|/pdmvserv_task_BP...|     T2_DE_DESY|   80|  Adding last 25 ...|\n",
      "|/fabozzi_Run2016H...|   T2_US_Purdue|99109|Error in StageOut...|\n",
      "|/pdmvserv_task_B2...|           null|99303|Could not find jo...|\n",
      "|/pdmvserv_task_HI...|T2_FR_GRIF_IRFU|  134|  Adding last 25 ...|\n",
      "|/fabozzi_Run2016B...|     T1_RU_JINR|99999|Adding extra erro...|\n",
      "|/pdmvserv_task_SU...|      T1_UK_RAL|  139|  Adding last 25 ...|\n",
      "|/pdmvserv_task_EX...|T2_US_Wisconsin| 8001|Exit 8001: CMSExe...|\n",
      "|/prebello_Run2016...|   T2_US_Purdue| 8022|An exception of c...|\n",
      "|/prozober_ACDC0_t...|     T2_CH_CERN|99999|Adding extra erro...|\n",
      "|/pdmvserv_task_HI...|     T2_DE_RWTH| 8028|An exception of c...|\n",
      "|/pdmvserv_task_EG...|     T2_US_UCSD|   85|  Adding last 25 ...|\n",
      "|/pdmvserv_task_B2...|      T1_UK_RAL| 8001|Exit 8001: CMSExe...|\n",
      "|/pdmvserv_task_EX...|  T2_US_Florida|99999|Adding extra erro...|\n",
      "|/pdmvserv_task_TS...|   T2_US_Purdue|  139|  Adding last 25 ...|\n",
      "|/pdmvserv_task_EX...| T2_CH_CERN_HLT|50115|Error reading XML...|\n",
      "|/pdmvserv_task_B2...| T2_US_Nebraska|99999|Adding extra erro...|\n",
      "|/pdmvserv_task_HI...|T2_FR_GRIF_IRFU|  139|  Adding last 25 ...|\n",
      "|/pdmvserv_task_TO...|T1_US_FNAL_Disk|  134|  Adding last 25 ...|\n",
      "|/pdmvserv_task_TO...|      T2_US_MIT| 8028|An exception of c...|\n",
      "|/pdmvserv_task_EX...|      T1_ES_PIC|   85|  Adding last 25 ...|\n",
      "+--------------------+---------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failing_workflows_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4159"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failing_workflows_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "failing_workflows_df_rep = failing_workflows_df.repartition(500)\n",
    "print failing_workflows_df_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load actionhist and convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext, StorageLevel\n",
    "sql = SQLContext(sc)\n",
    "labeled_failing_tasks = (sql.read\n",
    "     .format(\"com.databricks.spark.csv\")\n",
    "     .option(\"header\", \"true\")\n",
    "     .load(\"hdfs:///cms/users/llayer/actionhist.csv\"))\n",
    "\n",
    "#rdd_failing_tasks = rdd_failing_tasks.rdd.map(tuple)\n",
    "#print rdd_failing_tasks.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(task_name=u'/vlimant_ACDC0_task_HIG-RunIIFall17wmLHEGS-01415__v1_T_180706_002124_986/HIG-RunIIFall17DRPremix-02001_1/HIG-RunIIFall17DRPremix-02001_1MergeAODSIMoutput/HIG-RunIIFall17MiniAODv2-01299_0', side_state=u'good_site', error=85, site=u'T1_UK_RAL', action=u'acdc', memory=None)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "labeled_failing_tasks = labeled_failing_tasks.withColumn(\"error\", labeled_failing_tasks[\"error\"].cast(IntegerType()))\n",
    "print labeled_failing_tasks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Join both frames and save to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = failing_workflows_df.join(labeled_failing_tasks, ['task_name','site', 'error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('com.databricks.spark.csv').save('hdfs:///cms/users/llayer/df_reduced_codes2.csv',header = 'true')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "sparkconnect": {
   "bundled_options": [
    "CMSSpark"
   ],
   "list_of_options": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
